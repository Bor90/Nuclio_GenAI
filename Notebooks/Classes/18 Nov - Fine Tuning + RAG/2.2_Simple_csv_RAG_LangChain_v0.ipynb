{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "644nvokVLKXx"
      },
      "source": [
        "# Simple RAG (Retrieval-Augmented Generation) System for CSV Files\n",
        "\n",
        "## Overview\n",
        "\n",
        "This code implements a basic Retrieval-Augmented Generation (RAG) system for processing and querying CSV documents. The system encodes the document content into a vector store, which can then be queried to retrieve relevant information.\n",
        "\n",
        "# CSV File Structure and Use Case\n",
        "The CSV file contains dummy customer data, comprising various attributes like first name, last name, company, etc. This dataset will be utilized for a RAG use case, facilitating the creation of a customer information Q&A system.\n",
        "\n",
        "## Key Components\n",
        "\n",
        "1. Loading and spliting csv files.\n",
        "2. Vector store creation using [FAISS](https://engineering.fb.com/2017/03/29/data-infrastructure/faiss-a-library-for-efficient-similarity-search/) and OpenAI embeddings\n",
        "3. Retriever setup for querying the processed documents\n",
        "4. Creating a question and answer over the csv data.\n",
        "\n",
        "## Method Details\n",
        "\n",
        "### Document Preprocessing\n",
        "\n",
        "1. The csv is loaded using langchain Csvloader\n",
        "2. The data is split into chunks.\n",
        "\n",
        "\n",
        "### Vector Store Creation\n",
        "\n",
        "1. OpenAI embeddings are used to create vector representations of the text chunks.\n",
        "2. A FAISS vector store is created from these embeddings for efficient similarity search.\n",
        "\n",
        "### Retriever Setup\n",
        "\n",
        "1. A retriever is configured to fetch the most relevant chunks for a given query.\n",
        "\n",
        "## Benefits of this Approach\n",
        "\n",
        "1. Scalability: Can handle large documents by processing them in chunks.\n",
        "2. Flexibility: Easy to adjust parameters like chunk size and number of retrieved results.\n",
        "3. Efficiency: Utilizes FAISS for fast similarity search in high-dimensional spaces.\n",
        "4. Integration with Advanced NLP: Uses OpenAI embeddings for state-of-the-art text representation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "new_langchain_intro"
      },
      "source": [
        "## Introduction to LangChain\n",
        "\n",
        "This notebook, while demonstrating a RAG system, also serves as a practical introduction to [LangChain](https://www.langchain.com/). LangChain is a powerful open-source framework designed to simplify the creation of applications using large language models (LLMs). It provides a standard interface for chains, lots of integrations with other tools, and end-to-end chains for common applications.\n",
        "\n",
        "LangChain can be seen as a higher-level library that orchestrates the different components of a RAG system (and other LLM applications) in a more streamlined and modular way. Instead of writing boilerplate code to connect your data source, embedding model, vector store, and LLM, LangChain provides convenient abstractions to do so.\n",
        "\n",
        "In this notebook, we will be using several LangChain components, which will be explained in more detail in the following sections."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJWiaI6BLKX4"
      },
      "source": [
        "# Package Installation and Imports\n",
        "\n",
        "The cell below installs all necessary packages required to run this notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i9qNjYl0LKX5"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install faiss-cpu langchain langchain-community langchain-openai pandas python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Mbk4nUkMRAL"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vpKGEumBLKX7"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
        "from pathlib import Path\n",
        "from langchain_openai import ChatOpenAI,OpenAIEmbeddings\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables from a .env file\n",
        "env_path = \"/content/drive/MyDrive/.env\"\n",
        "load_dotenv(env_path)\n",
        "\n",
        "# Set the OpenAI API key environment variable\n",
        "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add Langchain create_agent_chain (so after you use it in 3.2 and 4.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NM3q6UW8LKX7"
      },
      "source": [
        "# CSV File Structure and Use Case\n",
        "The CSV file contains dummy customer data, comprising various attributes like first name, last name, company, etc. This dataset will be utilized for a RAG use case, facilitating the creation of a customer information Q&A system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5mVu3J1cLKX8",
        "outputId": "5fb680cc-9c72-4ab3-f35d-e8f578aadfde"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2026-01-22 18:04:47--  https://raw.githubusercontent.com/NirDiamant/RAG_TECHNIQUES/main/data/Understanding_Climate_Change.pdf\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 206372 (202K) [application/octet-stream]\n",
            "Saving to: ‘data/Understanding_Climate_Change.pdf’\n",
            "\n",
            "data/Understanding_ 100%[===================>] 201.54K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2026-01-22 18:04:47 (7.89 MB/s) - ‘data/Understanding_Climate_Change.pdf’ saved [206372/206372]\n",
            "\n",
            "--2026-01-22 18:04:47--  https://raw.githubusercontent.com/NirDiamant/RAG_TECHNIQUES/main/data/customers-100.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 17160 (17K) [text/plain]\n",
            "Saving to: ‘data/customers-100.csv’\n",
            "\n",
            "data/customers-100. 100%[===================>]  16.76K  --.-KB/s    in 0s      \n",
            "\n",
            "2026-01-22 18:04:47 (96.5 MB/s) - ‘data/customers-100.csv’ saved [17160/17160]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Download required data files\n",
        "import os\n",
        "os.makedirs('data', exist_ok=True)\n",
        "\n",
        "# Download the PDF document used in this notebook\n",
        "!wget -O data/Understanding_Climate_Change.pdf https://raw.githubusercontent.com/NirDiamant/RAG_TECHNIQUES/main/data/Understanding_Climate_Change.pdf\n",
        "!wget -O data/customers-100.csv https://raw.githubusercontent.com/NirDiamant/RAG_TECHNIQUES/main/data/customers-100.csv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PQDQ5CHTLKX-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "file_path = ('data/customers-100.csv') # insert the path of the csv file\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "#preview the csv file\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "new_csvloader_explanation"
      },
      "source": [
        "### Loading Data with LangChain's `CSVLoader`\n",
        "\n",
        "The first step in our RAG pipeline is to load the data from our CSV file. LangChain provides a variety of document loaders for different file formats, and in this case, we use the `CSVLoader`. This loader reads the CSV file and creates a `Document` object for each row. Each document contains the page content (the row data) and metadata."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bDzt_tR8LKYA"
      },
      "outputs": [],
      "source": [
        "loader = CSVLoader(file_path=file_path)\n",
        "docs = loader.load_and_split()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "new_embeddings_explanation"
      },
      "source": [
        "### Embeddings with LangChain and OpenAI\n",
        "\n",
        "After loading the data, we need to create vector embeddings for our documents. LangChain provides a seamless integration with various embedding models, including OpenAI's. The `OpenAIEmbeddings` class is a wrapper around the OpenAI API that allows us to easily generate embeddings for our text data. These embeddings are high-dimensional vectors that capture the semantic meaning of the text, which is crucial for the retrieval step in our RAG system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdQLSVfJLKYB"
      },
      "outputs": [],
      "source": [
        "embeddings = OpenAIEmbeddings()\n",
        "index = faiss.IndexFlatL2(len(OpenAIEmbeddings().embed_query(\" \")))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "new_vectorstore_explanation"
      },
      "source": [
        "### Vector Stores in LangChain: FAISS\n",
        "\n",
        "Once we have the embeddings, we need a place to store them and efficiently search for similar vectors. This is where vector stores come in. LangChain supports a wide range of vector stores, and in this notebook, we use `FAISS` (Facebook AI Similarity Search), a library for efficient similarity search and clustering of dense vectors.\n",
        "\n",
        "LangChain's `FAISS` class provides a convenient wrapper around the FAISS library, allowing us to create a vector store from our documents and embeddings with just a few lines of code. This vector store will be used by our retriever to find the most relevant documents for a given query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0F44q4NRReA-"
      },
      "outputs": [],
      "source": [
        "import faiss\n",
        "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "vector_store = FAISS(\n",
        "    embedding_function=OpenAIEmbeddings(),\n",
        "    index=index,\n",
        "    docstore=InMemoryDocstore(),\n",
        "    index_to_docstore_id={}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhGiCcjzLKYC"
      },
      "source": [
        "Add the splitted csv data to the vector store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08qoqPgGLKYD"
      },
      "outputs": [],
      "source": [
        "vector_store.add_documents(documents=docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "new_retrievalchain_explanation"
      },
      "source": [
        "### LangChain Chains: `create_retrieval_chain`\n",
        "\n",
        "The core of LangChain lies in its concept of 'chains'. Chains allow us to combine multiple components together to create a single, coherent application. In our case, we want to create a retrieval-based question-answering chain.\n",
        "\n",
        "The `create_retrieval_chain` function is a helper function that simplifies the process of creating such a chain. It takes a retriever and a language model as input and creates a chain that first retrieves relevant documents from the vector store and then passes them to the language model to generate an answer. This is the essence of the Retrieval-Augmented Generation (RAG) pattern."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1AHSB-AULKYD"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain.chains import create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "\n",
        "retriever = __.as_retriever()\n",
        "\n",
        "# Set up system prompt\n",
        "system_prompt = (\n",
        "    \"You are an assistant for question-answering tasks. \"\n",
        "    \"Use the following pieces of retrieved context to answer \"\n",
        "    \"the question. If you don't know the answer, say that you \"\n",
        "    \"don't know. Use three sentences maximum and keep the \"\n",
        "    \"answer concise.\"\n",
        "    \"\\n\\n\"\n",
        "    \"{context}\"\n",
        ")\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", __), ## This is the context\n",
        "    (\"human\", \"{input}\"), ## This will be our question\n",
        "\n",
        "])\n",
        "\n",
        "# Create the question-answer chain\n",
        "question_answer_chain = create_stuff_documents_chain(__, __) ## Defines the structure of the prompt that will generate the answer\n",
        "rag_chain = create_retrieval_chain(__, __) ## Builds the chain of retrieve + generate answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytp01r3oLKYD"
      },
      "source": [
        "Query the rag bot with a question based on the CSV data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ia6C6lg4LKYD"
      },
      "outputs": [],
      "source": [
        "answer= __({\"__\": \"which company does sheryl Baxter work for?\"})\n",
        "answer['answer']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oP5RPYe5Chw"
      },
      "source": [
        "## **LAB3: RAG Q&A Bot**\n",
        "<u>**Download the following files to create your own RAG system:**</u>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nmG2GK5A5U6z"
      },
      "outputs": [],
      "source": [
        "!wget -O app.py https://raw.githubusercontent.com/Bor90/Nuclio_GenAI/refs/heads/main/Labs/RAG_QandA/app.py\n",
        "!wget -O requirements.txt https://raw.githubusercontent.com/Bor90/Nuclio_GenAI/refs/heads/main/Labs/RAG_QandA/requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "This simple RAG system provides a solid foundation for building more complex information retrieval and question-answering systems. By encoding document content into a searchable vector store, it enables efficient retrieval of relevant information in response to queries. This approach is particularly useful for applications requiring quick access to specific information within a csv file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxCFhREsLKYE"
      },
      "source": [
        "![](https://europe-west1-rag-techniques-views-tracker.cloudfunctions.net/rag-techniques-tracker?notebook=all-rag-techniques--simple-csv-rag)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
