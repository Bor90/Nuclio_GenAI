{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# Reasoning Agent: HTML Code Generator with Self-Improvement\n",
    "\n",
    "This notebook demonstrates a **reasoning agent** that:\n",
    "1. Generates HTML code based on a user prompt\n",
    "2. Evaluates and improves the code iteratively\n",
    "\n",
    "We'll implement two versions:\n",
    "- **Version 1**: LLM-as-Judge (the LLM evaluates its own output)\n",
    "- **Version 2**: Reflection with External Feedback (using HTML validation)\n",
    "\n",
    "We'll use **Hugging Face's free Inference API** with open-source models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "theory"
   },
   "source": [
    "---\n",
    "\n",
    "## üìö Theory: Understanding Agentic AI\n",
    "\n",
    "Before diving into the implementation, let's understand the theoretical foundations of agentic AI and reasoning systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "theory_what"
   },
   "source": [
    "### What is Agentic AI?\n",
    "\n",
    "**Agentic AI** refers to AI systems that can **plan, act, evaluate, and improve** autonomously in pursuit of specific goals. Unlike traditional AI that follows fixed instructions or responds to patterns, agentic systems use **reasoning loops** to make context-aware decisions in real time.\n",
    "\n",
    "At its core, an agentic system combines:\n",
    "- A **Large Language Model (LLM)** as the reasoning engine\n",
    "- **External tools** that extend capabilities (search, code execution, validation)\n",
    "- **Feedback loops** that enable learning and self-improvement\n",
    "\n",
    "This combination allows AI to handle open-ended, multifaceted problems that require adaptive workflows and context-aware decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "theory_loop"
   },
   "source": [
    "### The Reasoning Loop: Think-Act-Observe\n",
    "\n",
    "Agentic systems operate through a continuous cycle that mirrors human problem-solving:\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ   THINK     ‚îÇ  1. Task Decomposition: Break down the goal\n",
    "‚îÇ  (Reason)   ‚îÇ  2. Planning: Decide on approach\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "       ‚îÇ\n",
    "       ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ    ACT      ‚îÇ  3. Delegation: Assign to tools/agents\n",
    "‚îÇ  (Execute)  ‚îÇ  4. Action: Generate output or call tools\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "       ‚îÇ\n",
    "       ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  OBSERVE    ‚îÇ  5. Evaluation: Review results\n",
    "‚îÇ (Reflect)   ‚îÇ  6. Adaptation: Refine approach\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "       ‚îÇ\n",
    "       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ Loop back to THINK\n",
    "```\n",
    "\n",
    "This iterative process enables:\n",
    "- **Decision-making** based on context\n",
    "- **Learning** from results\n",
    "- **Long-term planning** across multiple steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "theory_patterns"
   },
   "source": [
    "### Key Agentic Design Patterns\n",
    "\n",
    "#### 1. **Reflection** üîÑ\n",
    "\n",
    "Creating feedback loops where LLMs review and improve their outputs.\n",
    "\n",
    "**Two Types**:\n",
    "- **Self-Reflection**: LLM critiques its own output (LLM-as-Judge)\n",
    "- **External Feedback**: Using tools to provide objective validation\n",
    "\n",
    "**Research Evidence**:\n",
    "- **Self-Refine** (Madaan et al., 2023): ~20% improvement across diverse tasks\n",
    "- **Reflexion** (Shinn et al., 2023): 91% accuracy on HumanEval (vs GPT-4's 80%)\n",
    "- **CRITIC** (Gou et al., 2024): 10-30% improvement using external tools\n",
    "\n",
    "**When to Apply Reflection**:\n",
    "- ‚úÖ Validating request feasibility\n",
    "- ‚úÖ Checking initial plans\n",
    "- ‚úÖ After each execution step\n",
    "- ‚úÖ Verifying final outputs\n",
    "\n",
    "**Trade-offs**:\n",
    "- ‚ûï Improved accuracy and quality\n",
    "- ‚ûñ Increased latency (multiple LLM calls)\n",
    "- ‚ûñ Higher costs\n",
    "\n",
    "#### 2. **Tool Use** üõ†Ô∏è\n",
    "\n",
    "Extending LLM capabilities with external tools:\n",
    "- Web search for real-time information\n",
    "- Code execution for calculations\n",
    "- Database queries for data access\n",
    "- Validators for correctness checking\n",
    "\n",
    "#### 3. **Planning** üìã\n",
    "\n",
    "Breaking complex goals into actionable steps:\n",
    "- Multi-step reasoning\n",
    "- Conditional logic and branching\n",
    "- Dynamic replanning based on results\n",
    "\n",
    "#### 4. **Multi-Agent Collaboration** üë•\n",
    "\n",
    "Multiple specialized agents working together:\n",
    "- Division of labor by expertise\n",
    "- Parallel processing of subtasks\n",
    "- Coordination and synthesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "theory_react"
   },
   "source": [
    "### The ReAct Framework\n",
    "\n",
    "**ReAct** (Reasoning + Acting) by Yao et al. (2022) is a foundational framework that combines:\n",
    "\n",
    "- **Reasoning**: Explicit thought traces (reflection + planning)\n",
    "- **Acting**: Task-relevant actions in the environment\n",
    "\n",
    "The framework creates a loop where:\n",
    "1. Reasoning guides action selection\n",
    "2. Actions produce observations\n",
    "3. Observations inform further reasoning\n",
    "\n",
    "**Popular Implementations**:\n",
    "- **DSPy** (Databricks): `ReAct` class\n",
    "- **LangGraph**: `create_react_agent` function\n",
    "- **smolagents** (HuggingFace): ReAct-based code agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "theory_comparison"
   },
   "source": [
    "### Traditional AI vs Agentic AI\n",
    "\n",
    "| Aspect | Traditional AI | Agentic AI |\n",
    "|--------|---------------|------------|\n",
    "| **Behavior** | Fixed instructions | Dynamic decision-making |\n",
    "| **Feedback** | One-shot response | Iterative refinement |\n",
    "| **Tools** | Limited/none | Extensive tool use |\n",
    "| **Planning** | Pre-programmed | Adaptive planning |\n",
    "| **Learning** | Static | Self-improvement |\n",
    "| **Context** | Pattern matching | Context-aware reasoning |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "theory_our_approach"
   },
   "source": [
    "### Our Implementation Approach\n",
    "\n",
    "In this notebook, we'll implement **two versions** of a reasoning agent:\n",
    "\n",
    "#### **Version 1: LLM-as-Judge (Self-Reflection)**\n",
    "```\n",
    "User Prompt ‚Üí Generate HTML ‚Üí Self-Evaluate ‚Üí Improve ‚Üí Repeat\n",
    "```\n",
    "- The LLM generates code\n",
    "- The same LLM judges its own output\n",
    "- Iteratively improves based on self-critique\n",
    "- **Pros**: Simple, no external dependencies\n",
    "- **Cons**: May have blind spots in self-evaluation\n",
    "\n",
    "#### **Version 2: Reflection with External Feedback**\n",
    "```\n",
    "User Prompt ‚Üí Generate HTML ‚Üí External Validator ‚Üí Reflect on Errors ‚Üí Fix ‚Üí Repeat\n",
    "```\n",
    "- The LLM generates code\n",
    "- External HTML parser validates syntax\n",
    "- LLM reflects on objective validation errors\n",
    "- Iteratively fixes issues\n",
    "- **Pros**: Objective validation, catches concrete errors\n",
    "- **Cons**: Requires external tools, more complex\n",
    "\n",
    "Both approaches demonstrate the power of **reflection** in improving AI output quality through iterative refinement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "theory_references"
   },
   "source": [
    "### Key Research Papers\n",
    "\n",
    "1. **Yao et al. (2022)**: [\"ReAct: Synergizing Reasoning and Acting in Language Models\"](https://arxiv.org/abs/2210.03629)\n",
    "2. **Madaan et al. (2023)**: [\"Self-Refine: Iterative Refinement with Self-Feedback\"](https://arxiv.org/abs/2303.17651)\n",
    "3. **Shinn et al. (2023)**: [\"Reflexion: Language Agents with Verbal Reinforcement Learning\"](https://arxiv.org/abs/2303.11366)\n",
    "4. **Gou et al. (2024)**: [\"CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing\"](https://arxiv.org/abs/2305.11738)\n",
    "\n",
    "Now let's see these concepts in action! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## Setup\n",
    "\n",
    "First, install the required packages and set up authentication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install"
   },
   "outputs": [],
   "source": [
    "!pip install -q huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import InferenceClient\n",
    "from typing import Dict, List, Tuple\n",
    "import json\n",
    "from html.parser import HTMLParser\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "token"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pUeaY3ebYmmK"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "env_path = \"/content/drive/MyDrive/.env\"\n",
    "load_dotenv(env_path)\n",
    "\n",
    "HF_TOKEN = os.getenv('HF_TOKEN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "model_setup"
   },
   "source": [
    "### Model Configuration\n",
    "\n",
    "We'll use **Qwen/Qwen2.5-72B-Instruct** - a powerful open-source model available via Hugging Face Inference API.\n",
    "\n",
    "Alternative models you can try:\n",
    "- `meta-llama/Llama-3.1-70B-Instruct`\n",
    "- `mistralai/Mixtral-8x7B-Instruct-v0.1`\n",
    "- `microsoft/Phi-3-medium-4k-instruct`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7fCERzHKY9Br"
   },
   "outputs": [],
   "source": [
    "# Initialize the Hugging Face Inference Client\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-72B-Instruct\"\n",
    "client = InferenceClient(token=HF_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "llm_messages_intro"
   },
   "source": [
    "---\n",
    "\n",
    "## üí¨ Understanding LLM Messages: Roles and Content\n",
    "\n",
    "Before we dive into building our reasoning agent, let's understand how we communicate with Large Language Models (LLMs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "llm_messages_explain"
   },
   "source": [
    "### How Do We Talk to LLMs?\n",
    "\n",
    "When you interact with an LLM (like ChatGPT, Claude, or open-source models), you're not just sending plain text. Instead, you send **structured messages** that help the LLM understand the context and respond appropriately.\n",
    "\n",
    "Each message has two key components:\n",
    "\n",
    "#### 1. **Role** - Who is speaking?\n",
    "\n",
    "There are three main roles:\n",
    "\n",
    "| Role | Description | Purpose |\n",
    "|------|-------------|----------|\n",
    "| **`system`** | Sets the behavior and context | \"You are a helpful assistant\", \"You are an expert coder\" |\n",
    "| **`user`** | The human asking questions | Your prompts and requests |\n",
    "| **`assistant`** | The LLM's responses | Previous answers from the AI |\n",
    "\n",
    "#### 2. **Content** - What is being said?\n",
    "\n",
    "The actual text of the message - the instructions, questions, or responses.\n",
    "\n",
    "### Message Structure\n",
    "\n",
    "Messages are formatted as a list of dictionaries:\n",
    "\n",
    "```python\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful coding assistant.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Write a Python function to calculate factorial.\"\n",
    "    }\n",
    "]\n",
    "```\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "Understanding message roles is crucial because:\n",
    "\n",
    "1. **System messages** set the \"personality\" and instructions for the LLM\n",
    "2. **Conversation history** is maintained through user/assistant message pairs\n",
    "3. **Context** from previous messages influences future responses\n",
    "4. **Agentic systems** use this structure to create feedback loops\n",
    "\n",
    "Let's see this in action! üëá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llm(messages: List[Dict[str, str]], max_tokens: int = 2000, temperature: float = 0.7) -> str:\n",
    "    \"\"\"\n",
    "    Call the LLM with a list of messages.\n",
    "\n",
    "    Args:\n",
    "        messages: List of message dicts with 'role' and 'content'\n",
    "        max_tokens: Maximum tokens to generate\n",
    "        temperature: Sampling temperature (0.0 to 1.0)\n",
    "\n",
    "    Returns:\n",
    "        Generated text response\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = client.chat_completion(\n",
    "            __,\n",
    "            model=MODEL_NAME,\n",
    "            __,\n",
    "            __\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"Error calling LLM: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the connection\n",
    "test_response = call_llm([{\"role\": \"user\", \"content\": __}], max_tokens=50)\n",
    "print(\"Model test:\", test_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "llm_messages_demo"
   },
   "source": [
    "### üéØ Interactive Demo: Message Roles in Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Simple message with system role\n",
    "print(\"Example 1: Basic Message Structure\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "messages_example1 = [\n",
    "    ###########################\n",
    "    # The agent is a \"friendly teacher explaining concepts simply\",\n",
    "    # and you ask \"What is Python?\"\n",
    "    # INSERT YOU CODE HERE\n",
    "    {\n",
    "        \"role\": \n",
    "        \"content\": \n",
    "    },\n",
    "    {\n",
    "        \"role\": \n",
    "        \"content\": \n",
    "    }\n",
    "    ###########################\n",
    "]\n",
    "\n",
    "print(\"Messages sent to LLM:\")\n",
    "for msg in messages_example1:\n",
    "    print(f\"\\n[{msg['role'].upper()}]\")\n",
    "    print(f\"{msg['content']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "demo_basic_message"
   },
   "outputs": [],
   "source": [
    "print(\"Calling LLM...\\n\")\n",
    "\n",
    "response1 = # INSERT YOU CODE HERE\n",
    "print(response1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Impact of different system messages\n",
    "print(\"Example 2: How System Messages Change Behavior\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "user_question = \"Explain what a variable is in programming.\"\n",
    "\n",
    "###########################\n",
    "# Ask the same question to two different agents.\n",
    "# INSERT YOU CODE HERE\n",
    "messages_friendly = # Friendly teacher\n",
    "messages_technical = # Technical expert\n",
    "###########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# Print both answers\n",
    "# INSERT YOU CODE HERE\n",
    "###########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "demo_conversation"
   },
   "outputs": [],
   "source": [
    "# Example 3: Multi-turn conversation\n",
    "print(\"Example 3: Multi-Turn Conversation\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "messages_example3 = [\n",
    "    {\n",
    "        \"role\": \"__\",\n",
    "        \"content\": \"You are a concise coding assistant.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"__\",\n",
    "        \"content\": \"Write a function to add two numbers.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"__\",\n",
    "        \"content\": \"def add(a, b):\\n    return a + b\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"__\",\n",
    "        \"content\": \"Now add type hints to it.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"Conversation history:\")\n",
    "for i, msg in enumerate(messages_example3, 1):\n",
    "    print(f\"\\n{i}. [{msg['role'].upper()}]\")\n",
    "    print(f\"   {msg['content'][:100]}...\" if len(msg['content']) > 100 else f\"   {msg['content']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "demo_system_role"
   },
   "outputs": [],
   "source": [
    "print(\"Calling LLM with conversation history...\\n\")\n",
    "\n",
    "###########################\n",
    "# INSERT YOU CODE HERE\n",
    "###########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "version1"
   },
   "source": [
    "---\n",
    "\n",
    "## Version 1: LLM-as-Judge\n",
    "\n",
    "In this version, the LLM generates HTML code, then acts as a judge to evaluate its own output and suggest improvements. The agent iterates through multiple rounds of generation and self-critique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v1_generator"
   },
   "outputs": [],
   "source": [
    "# Function 1: generate HTML code based on a prompt\n",
    "\n",
    "def generate_html(prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate HTML code based on a prompt.\n",
    "\n",
    "    Args:\n",
    "        prompt: Description of the HTML to generate\n",
    "\n",
    "    Returns:\n",
    "        Generated HTML code\n",
    "    \"\"\"\n",
    "    ###########################\n",
    "    # INSERT YOU CODE HERE\n",
    "    # Messages instructions:\n",
    "    # - Use a system message to set the role as a \"expert HTML developer\"\n",
    "    # - Tell the agent to respond only with HTML code\n",
    "    # - Use the user prompt to specify the HTML content needed\n",
    "    # - The prompt should be passed as a user message (ask me about this!)\n",
    "    # Response instructions:\n",
    "    # - Limit max tokens to 2000\n",
    "    # - Set a high temperature to encourage creativity\n",
    "    messages = \n",
    "    response = \n",
    "    ###########################\n",
    "\n",
    "    # Extract HTML code from response (remove markdown code blocks if present)\n",
    "    html_code = response.strip()\n",
    "    if \"```html\" in html_code:\n",
    "        html_code = html_code.split(\"```html\")[1].split(\"```\")[0].strip()\n",
    "    elif \"```\" in html_code:\n",
    "        html_code = html_code.split(\"```\")[1].split(\"```\")[0].strip()\n",
    "\n",
    "    return html_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the generate_html function (this will not be our real task)\n",
    "test_prompt = \"A simple homepage for a bakery with a header, product list, and contact info\"\n",
    "generated_html = generate_html(test_prompt)\n",
    "print(\"Generated HTML:\\n\")\n",
    "print(generated_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v1_judge"
   },
   "outputs": [],
   "source": [
    "# Function 2: judge HTML code quality based on a prompt\n",
    "\n",
    "def judge_html(html_code: str, original_prompt: str) -> Tuple[float, str]:\n",
    "    \"\"\"\n",
    "    Use LLM to judge the quality of generated HTML.\n",
    "\n",
    "    Args:\n",
    "        html_code: The HTML code to evaluate\n",
    "        original_prompt: The original user prompt\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (score, feedback)\n",
    "        - score: Quality score from 0.0 to 10.0\n",
    "        - feedback: Detailed feedback and improvement suggestions\n",
    "    \"\"\"\n",
    "    ###########################\n",
    "    # INSERT YOU CODE HERE\n",
    "    # Messages instructions (you can use ChatGPT to help you write this!):\n",
    "    # - Write a system message that sets the role as an \"expert web developer and code reviewer\"\n",
    "    # - Provide clear instructions to score the HTML from 0 to 10 based on criteria like correctness, style, and adherence to the prompt\n",
    "    # - Ask for detailed feedback on improvements\n",
    "    # - Request the response to include a \"SCORE:\" line and a \"FEEDBACK:\" section\n",
    "    # - Include both the original prompt and the generated HTML in user messages\n",
    "    # Response instructions:\n",
    "    # - Limit max tokens to 1000\n",
    "    # - Use a moderate temperature for deterministic output\n",
    "    messages = \n",
    "    response = \n",
    "    ###########################\n",
    "\n",
    "    # Parse score and feedback\n",
    "    score = 5.0  # Default score\n",
    "    feedback = response\n",
    "\n",
    "    if \"SCORE:\" in response:\n",
    "        try:\n",
    "            score_text = response.split(\"SCORE:\")[1].split(\"\\n\")[0].strip()\n",
    "            score = float(re.findall(r'\\d+\\.?\\d*', score_text)[0])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    if \"FEEDBACK:\" in response:\n",
    "        feedback = response.split(\"FEEDBACK:\")[1].strip()\n",
    "\n",
    "    return score, feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the judge_html function\n",
    "# You must get a score between 0 and 1, and a feedback string\n",
    "test_score, test_feedback = judge_html(generated_html, test_prompt)\n",
    "print(f\"Judge Score: {test_score}/10\")\n",
    "print(\"Judge Feedback:\")\n",
    "print(test_feedback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v1_improve"
   },
   "outputs": [],
   "source": [
    "# Function 3: improve HTML code based on feedback\n",
    "\n",
    "def improve_html(html_code: str, feedback: str, original_prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Improve HTML code based on feedback.\n",
    "\n",
    "    Args:\n",
    "        html_code: Current HTML code\n",
    "        feedback: Feedback from the judge\n",
    "        original_prompt: Original user prompt\n",
    "\n",
    "    Returns:\n",
    "        Improved HTML code\n",
    "    \"\"\"\n",
    "    ###########################\n",
    "    # INSERT YOU CODE HERE\n",
    "    # Messages instructions (you can use ChatGPT to help you write this!):\n",
    "    # - Set the role as an \"expert HTML developer\" and code improver\n",
    "    # - Instruct the agent to improve the provided HTML code based on the feedback\n",
    "    # - Request only the improved HTML code in the response\n",
    "    # - Include the original prompt, current HTML, and feedback in user messages\n",
    "    # Response instructions:\n",
    "    # - Limit max tokens to 2000\n",
    "    # - Use a higher temperature for creativity    \n",
    "    messages = \n",
    "    response = \n",
    "    ###########################\n",
    "\n",
    "    # Extract HTML code\n",
    "    html_code = response.strip()\n",
    "    if \"```html\" in html_code:\n",
    "        html_code = html_code.split(\"```html\")[1].split(\"```\")[0].strip()\n",
    "    elif \"```\" in html_code:\n",
    "        html_code = html_code.split(\"```\")[1].split(\"```\")[0].strip()\n",
    "\n",
    "    return html_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PnukZoW_D0Zp"
   },
   "source": [
    "Let's tie it all together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v1_agent"
   },
   "outputs": [],
   "source": [
    "def reasoning_agent_v1(prompt: str, max_iterations: int = 3, target_score: float = 8.0) -> Dict:\n",
    "    \"\"\"\n",
    "    Reasoning agent that generates and improves HTML using LLM-as-Judge.\n",
    "\n",
    "    Args:\n",
    "        prompt: Description of the HTML to generate\n",
    "        max_iterations: Maximum number of improvement iterations\n",
    "        target_score: Target quality score to achieve\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with final HTML, score, and iteration history\n",
    "    \"\"\"\n",
    "    print(f\"ü§ñ Reasoning Agent V1: LLM-as-Judge\")\n",
    "    print(f\"üìù Task: {prompt}\\n\")\n",
    "\n",
    "    history = []\n",
    "\n",
    "    # Initial generation\n",
    "    print(\"[Iteration 1] Generating initial HTML...\")\n",
    "    html_code = # INSERT YOU CODE HERE\n",
    "\n",
    "    # Evaluate\n",
    "    print(\"[Iteration 1] Evaluating quality...\")\n",
    "    score, feedback = # INSERT YOU CODE HERE\n",
    "    print(f\"[Iteration 1] Score: {score}/10\")\n",
    "    print(f\"[Iteration 1] Feedback: {feedback[:200]}...\\n\")\n",
    "\n",
    "    history.append({\n",
    "        \"iteration\": 1,\n",
    "        \"html\": html_code,\n",
    "        \"score\": score,\n",
    "        \"feedback\": feedback\n",
    "    })\n",
    "\n",
    "    # Iterative improvement\n",
    "    for i in range(2, max_iterations + 1):\n",
    "        if score >= target_score:\n",
    "            print(f\"‚úÖ Target score achieved! Stopping at iteration {i-1}\\n\")\n",
    "            break\n",
    "\n",
    "        print(f\"[Iteration {i}] Improving HTML based on feedback...\")\n",
    "        html_code = # INSERT YOU CODE HERE\n",
    "\n",
    "        print(f\"[Iteration {i}] Evaluating improved version...\")\n",
    "        score, feedback =  # INSERT YOU CODE HERE\n",
    "        print(f\"[Iteration {i}] Score: {score}/10\")\n",
    "        print(f\"[Iteration {i}] Feedback: {feedback[:200]}...\\n\")\n",
    "\n",
    "        history.append({\n",
    "            \"iteration\": i,\n",
    "            \"html\": html_code,\n",
    "            \"score\": score,\n",
    "            \"feedback\": feedback\n",
    "        })\n",
    "\n",
    "    print(f\"üéØ Final Score: {score}/10\\n\")\n",
    "\n",
    "    return {\n",
    "        \"final_html\": html_code,\n",
    "        \"final_score\": score,\n",
    "        \"history\": history\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v1_demo"
   },
   "source": [
    "### Demo: Version 1 (LLM-as-Judge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the reasoning agent\n",
    "# Use the prompt: \"A modern landing page for a coffee shop with a hero section, menu preview, and contact form\"\n",
    "result_v1 = # INSERT YOU CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v1_run"
   },
   "outputs": [],
   "source": [
    "# Display final HTML\n",
    "print(\"=\"*80)\n",
    "print(\"FINAL HTML CODE:\")\n",
    "print(\"=\"*80)\n",
    "print(result_v1[\"final_html\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v1_visualize"
   },
   "outputs": [],
   "source": [
    "# Visualize the HTML in Colab\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "display(HTML(result_v1[\"final_html\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "version2"
   },
   "source": [
    "---\n",
    "\n",
    "## Version 2: Reflection with External Feedback\n",
    "\n",
    "In this version, we add **external validation** using:\n",
    "1. HTML syntax validation (checking for parsing errors)\n",
    "2. Structure validation (checking for required elements)\n",
    "3. LLM reflection based on external feedback\n",
    "\n",
    "This demonstrates how external tools can provide objective feedback to guide the reasoning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v2_validator"
   },
   "outputs": [],
   "source": [
    "class HTMLValidator(HTMLParser):\n",
    "    \"\"\"\n",
    "    Custom HTML parser to validate HTML structure and collect errors.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.errors = []\n",
    "        self.tags = []\n",
    "        self.tag_stack = []\n",
    "\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        self.tags.append(tag)\n",
    "        if tag not in ['img', 'br', 'hr', 'input', 'meta', 'link']:\n",
    "            self.tag_stack.append(tag)\n",
    "\n",
    "    def handle_endtag(self, tag):\n",
    "        if tag in ['img', 'br', 'hr', 'input', 'meta', 'link']:\n",
    "            return\n",
    "        if not self.tag_stack:\n",
    "            self.errors.append(f\"Unexpected closing tag: </{tag}>\")\n",
    "        elif self.tag_stack[-1] != tag:\n",
    "            self.errors.append(f\"Mismatched tags: expected </{self.tag_stack[-1]}>, got </{tag}>\")\n",
    "        else:\n",
    "            self.tag_stack.pop()\n",
    "\n",
    "    def error(self, message):\n",
    "        self.errors.append(f\"Parse error: {message}\")\n",
    "\n",
    "def validate_html(html_code: str, required_elements: List[str] = None) -> Tuple[bool, List[str]]:\n",
    "    \"\"\"\n",
    "    Validate HTML code for syntax errors and required elements.\n",
    "\n",
    "    Args:\n",
    "        html_code: HTML code to validate\n",
    "        required_elements: List of required HTML tags (e.g., ['html', 'body', 'head'])\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (is_valid, list_of_issues)\n",
    "    \"\"\"\n",
    "    validator = HTMLValidator()\n",
    "    issues = []\n",
    "\n",
    "    try:\n",
    "        validator.feed(html_code)\n",
    "    except Exception as e:\n",
    "        issues.append(f\"Critical parsing error: {str(e)}\")\n",
    "        return False, issues\n",
    "\n",
    "    # Check for parsing errors\n",
    "    if validator.errors:\n",
    "        issues.extend(validator.errors)\n",
    "\n",
    "    # Check for unclosed tags\n",
    "    if validator.tag_stack:\n",
    "        issues.append(f\"Unclosed tags: {', '.join(validator.tag_stack)}\")\n",
    "\n",
    "    # Check for required elements\n",
    "    if required_elements:\n",
    "        missing = [elem for elem in required_elements if elem not in validator.tags]\n",
    "        if missing:\n",
    "            issues.append(f\"Missing required elements: {', '.join(missing)}\")\n",
    "\n",
    "    # Basic structure checks\n",
    "    if 'html' in validator.tags:\n",
    "        if 'head' not in validator.tags:\n",
    "            issues.append(\"Missing <head> element\")\n",
    "        if 'body' not in validator.tags:\n",
    "            issues.append(\"Missing <body> element\")\n",
    "\n",
    "    is_valid = len(issues) == 0\n",
    "    return is_valid, issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v2_reflect"
   },
   "outputs": [],
   "source": [
    "def reflect_and_improve(html_code: str, validation_issues: List[str], original_prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Use LLM to reflect on external validation feedback and improve code.\n",
    "\n",
    "    Args:\n",
    "        html_code: Current HTML code\n",
    "        validation_issues: List of issues from external validator\n",
    "        original_prompt: Original user prompt\n",
    "\n",
    "    Returns:\n",
    "        Improved HTML code\n",
    "    \"\"\"\n",
    "    issues_text = \"\\n- \" + \"\\n- \".join(validation_issues)\n",
    "\n",
    "    ###########################\n",
    "    # INSERT YOU CODE HERE\n",
    "    # Messages instructions:\n",
    "    # - Set the role as an \"expert HTML developer\" who improves code based on validation feedback\n",
    "    # - Instruct the agent to reflect on the validation issues and generate corrected HTML code\n",
    "    # - Request only the corrected HTML code in the response\n",
    "    # - Include the original prompt, current HTML, and validation issues in user messages\n",
    "    # Response instructions:\n",
    "    # - Limit max tokens to 2000\n",
    "    # - Use a higher temperature for creativity\n",
    "    messages = \n",
    "    response = \n",
    "    ###########################\n",
    "\n",
    "    # Extract HTML code\n",
    "    html_code = response.strip()\n",
    "    if \"```html\" in html_code:\n",
    "        html_code = html_code.split(\"```html\")[1].split(\"```\")[0].strip()\n",
    "    elif \"```\" in html_code:\n",
    "        html_code = html_code.split(\"```\")[1].split(\"```\")[0].strip()\n",
    "\n",
    "    return html_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v2_agent"
   },
   "outputs": [],
   "source": [
    "def reasoning_agent_v2(prompt: str, max_iterations: int = 3, required_elements: List[str] = None) -> Dict:\n",
    "    \"\"\"\n",
    "    Reasoning agent with reflection based on external validation feedback.\n",
    "\n",
    "    Args:\n",
    "        prompt: Description of the HTML to generate\n",
    "        max_iterations: Maximum number of improvement iterations\n",
    "        required_elements: List of required HTML elements\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with final HTML, validation status, and iteration history\n",
    "    \"\"\"\n",
    "    print(f\"ü§ñ Reasoning Agent V2: Reflection with External Feedback\")\n",
    "    print(f\"üìù Task: {prompt}\\n\")\n",
    "\n",
    "    history = []\n",
    "\n",
    "    # Initial generation\n",
    "    print(\"[Iteration 1] Generating initial HTML...\")\n",
    "    html_code = # INSERT YOU CODE HERE\n",
    "\n",
    "    # External validation\n",
    "    print(\"[Iteration 1] Running external validation...\")\n",
    "    is_valid, issues = # INSERT YOU CODE HERE\n",
    "\n",
    "    if is_valid:\n",
    "        print(\"[Iteration 1] ‚úÖ Validation passed!\")\n",
    "    else:\n",
    "        print(f\"[Iteration 1] ‚ùå Validation failed with {len(issues)} issue(s)\")\n",
    "        for issue in issues:\n",
    "            print(f\"  - {issue}\")\n",
    "    print()\n",
    "\n",
    "    # Also get LLM judge score\n",
    "    score, feedback = # INSERT YOU CODE HERE\n",
    "    print(f\"[Iteration 1] LLM Judge Score: {score}/10\\n\")\n",
    "\n",
    "    history.append({\n",
    "        \"iteration\": 1,\n",
    "        \"html\": html_code,\n",
    "        \"is_valid\": is_valid,\n",
    "        \"issues\": issues,\n",
    "        \"score\": score,\n",
    "        \"feedback\": feedback\n",
    "    })\n",
    "\n",
    "    # Iterative improvement based on external feedback\n",
    "    for i in range(2, max_iterations + 1):\n",
    "        if is_valid and score >= 8.0:\n",
    "            print(f\"‚úÖ Code is valid and high quality! Stopping at iteration {i-1}\\n\")\n",
    "            break\n",
    "\n",
    "        if not is_valid:\n",
    "            # Fix validation issues first\n",
    "            print(f\"[Iteration {i}] Reflecting on validation issues and improving...\")\n",
    "            html_code = # INSERT YOU CODE HERE\n",
    "        else:\n",
    "            # Improve based on LLM feedback\n",
    "            print(f\"[Iteration {i}] Improving based on LLM feedback...\")\n",
    "            html_code = # INSERT YOU CODE HERE\n",
    "\n",
    "        # Validate again\n",
    "        print(f\"[Iteration {i}] Running external validation...\")\n",
    "        is_valid, issues = # INSERT YOU CODE HERE\n",
    "\n",
    "        if is_valid:\n",
    "            print(f\"[Iteration {i}] ‚úÖ Validation passed!\")\n",
    "        else:\n",
    "            print(f\"[Iteration {i}] ‚ùå Validation failed with {len(issues)} issue(s)\")\n",
    "            for issue in issues:\n",
    "                print(f\"  - {issue}\")\n",
    "        print()\n",
    "\n",
    "        # Get LLM score\n",
    "        score, feedback = # INSERT YOU CODE HERE\n",
    "        print(f\"[Iteration {i}] LLM Judge Score: {score}/10\\n\")\n",
    "\n",
    "        history.append({\n",
    "            \"iteration\": i,\n",
    "            \"html\": html_code,\n",
    "            \"is_valid\": is_valid,\n",
    "            \"issues\": issues,\n",
    "            \"score\": score,\n",
    "            \"feedback\": feedback\n",
    "        })\n",
    "\n",
    "    print(f\"üéØ Final Status: {'‚úÖ Valid' if is_valid else '‚ùå Invalid'}, Score: {score}/10\\n\")\n",
    "\n",
    "    return {\n",
    "        \"final_html\": html_code,\n",
    "        \"is_valid\": is_valid,\n",
    "        \"final_score\": score,\n",
    "        \"history\": history\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v2_demo"
   },
   "source": [
    "### Demo: Version 2 (Reflection with External Feedback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the reasoning agent with external validation\n",
    "# Use the prompt: \"A modern landing page for a coffee shop with a hero section, menu preview, and contact form\"\n",
    "# The required elements are ['html', 'head', 'body', 'title']\n",
    "result_v2 = # INSERT YOU CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v2_run"
   },
   "outputs": [],
   "source": [
    "# Display final HTML\n",
    "print(\"=\"*80)\n",
    "print(\"FINAL HTML CODE:\")\n",
    "print(\"=\"*80)\n",
    "print(result_v2[\"final_html\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v2_visualize"
   },
   "outputs": [],
   "source": [
    "# Visualize the HTML in Colab\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "display(HTML(result_v2[\"final_html\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí≠ Discussion questions\n",
    "\n",
    "1. **When is self-reflection (LLM-as-Judge) better than external validation?**\n",
    "   - Think about subjective vs. objective tasks\n",
    "\n",
    "2. **What are the trade-offs of adding more reflection iterations?**\n",
    "   - Consider: quality, cost, latency, diminishing returns\n",
    "\n",
    "3. **How would you combine multiple types of feedback?**\n",
    "   - Example: Syntax validation + style checking + user preferences\n",
    "\n",
    "4. **What tasks are NOT suitable for reasoning agents?**\n",
    "   - When is a simple one-shot generation better?\n",
    "\n",
    "5. **How can we prevent infinite loops in reasoning agents?**\n",
    "   - What stopping criteria make sense?\n",
    "\n",
    "6. **Real-world applications**: Where have you seen reasoning agents in action?\n",
    "   - GitHub Copilot, ChatGPT Code Interpreter, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "This notebook demonstrated two approaches to building reasoning agents:\n",
    "\n",
    "### **Version 1: LLM-as-Judge**\n",
    "- The LLM evaluates its own output\n",
    "- Useful for subjective quality assessment\n",
    "- Simpler implementation\n",
    "- May have blind spots in self-evaluation\n",
    "\n",
    "### **Version 2: Reflection with External Feedback**\n",
    "- Combines LLM reasoning with objective external tools\n",
    "- More reliable for catching concrete errors\n",
    "- Demonstrates how to integrate external validation\n",
    "- Better separation of concerns (correctness vs. quality)\n",
    "\n",
    "### **General Principles**\n",
    "1. **Iterative refinement**: Both agents improve through multiple iterations\n",
    "2. **Feedback loops**: Critical for self-improvement\n",
    "3. **External validation**: Adds objectivity and reliability\n",
    "4. **Open-source models**: Powerful reasoning is possible without proprietary APIs\n",
    "\n",
    "### **Extensions You Can Try**\n",
    "- Add performance metrics (page size, load time)\n",
    "- Implement multi-agent collaboration (one generates, another reviews)\n",
    "- Add user feedback as another external signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
