{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGjfYQBcyWve"
      },
      "source": [
        "# Conversational Memory with LCEL\n",
        "\n",
        "Conversational memory is how chatbots can respond to our queries in a chat-like manner. It enables a coherent conversation, and without it, every query would be treated as an entirely independent input without considering past interactions.\n",
        "\n",
        "The memory allows an _\"agent\"_ to remember previous interactions with the user. By default, agents are *stateless* — meaning each incoming query is processed independently of other interactions. The only thing that exists for a stateless agent is the current input, nothing else.\n",
        "\n",
        "There are many applications where remembering previous interactions is very important, such as chatbots. Conversational memory allows us to do that.\n",
        "\n",
        "In this notebook we'll explore conversational memory using modern LangChain Expression Language (LCEL) and the recommended `RunnableWithMessageHistory` class.\n",
        "\n",
        "We'll start by importing all of the libraries that we'll be using in this example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ETg8fr8-yWvf",
        "outputId": "348925c6-c66b-415a-ce4f-469b020670fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.3/65.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m458.9/458.9 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.0/363.0 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m948.6/948.6 kB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m74.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langgraph-prebuilt 1.0.6 requires langchain-core>=1.0.0, but you have langchain-core 0.3.83 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -qU \\\n",
        "  langchain==0.3.25 \\\n",
        "  langchain-community==0.3.25 \\\n",
        "  langchain-openai==0.3.22 \\\n",
        "  tiktoken==0.9.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSvjQpbKyWvf"
      },
      "source": [
        "To run this notebook, we will need to use an OpenAI LLM. Here we will setup the LLM we will use for the whole notebook, just input your openai api key if prompted, otherwise it will use the `OPENAI_API_KEY` environment variable."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5V6ilUMpHb9",
        "outputId": "912086ac-9c67-44fb-8791-d4428c96b04c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "nnquGYaQyWvf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables from a .env file\n",
        "env_path = \"/content/drive/MyDrive/.env\"\n",
        "# env_path = \"C:\\\\Users\\\\vborg\\\\Git\\\\.env\"\n",
        "load_dotenv(env_path)\n",
        "\n",
        "# Set the OpenAI API key environment variable\n",
        "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "wFhsehZEyWvf"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    temperature=0,\n",
        "    model_name='gpt-4.1-mini'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPQgxde4yWvf"
      },
      "source": [
        "Later we will make use of a `count_tokens` utility function. This will allow us to count the number of tokens we are using for each call. We define it as so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "YG0RXg5PyWvf"
      },
      "outputs": [],
      "source": [
        "from langchain.callbacks import get_openai_callback\n",
        "\n",
        "def count_tokens(pipeline, query, config=None):\n",
        "    with get_openai_callback() as cb:\n",
        "        # Handle both dict and string inputs\n",
        "        if isinstance(query, str):\n",
        "            query = {\"query\": query}\n",
        "\n",
        "        # Use provided config `or default\n",
        "        if config is None:\n",
        "            config = {\"configurable\": {\"session_id\": \"default\"}}\n",
        "\n",
        "        result = pipeline.invoke(query, config=config)\n",
        "        print(f'Spent a total of {cb.total_tokens} tokens')\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPk7c5IgyWvf"
      },
      "source": [
        "Now let's dive into **Conversational Memory** using LCEL.\n",
        "\n",
        "## What is memory?\n",
        "\n",
        "**Definition**: Memory is an agent's capacity of remembering previous interactions with the user (think chatbots)\n",
        "\n",
        "The official definition of memory is the following:\n",
        "\n",
        "> By default, Chains and Agents are stateless, meaning that they treat each incoming query independently. In some applications (chatbots being a GREAT example) it is highly important to remember previous interactions, both at a short term but also at a long term level. The concept of \"Memory\" exists to do exactly that.\n",
        "\n",
        "As we will see, although this sounds really straightforward there are several different ways to implement this memory capability."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZgnUOmSyWvf"
      },
      "source": [
        "## Building Conversational Chains with LCEL\n",
        "\n",
        "Before we delve into the different memory types, let's understand how to build conversational chains using LCEL. The key components are:\n",
        "\n",
        "1. **Prompt Template** - Defines the conversation structure with placeholders for history and input\n",
        "2. **LLM** - The language model that generates responses\n",
        "3. **Output Parser** - Converts the LLM output to the desired format (optional)\n",
        "4. **RunnableWithMessageHistory** - Manages conversation history\n",
        "\n",
        "Let's create our base conversational chain:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TEpNx9oNyWvg",
        "outputId": "f61c1bdc-5fe2-42db-9c44-51158631d42e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n"
          ]
        }
      ],
      "source": [
        "from langchain.prompts import (\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        "    MessagesPlaceholder\n",
        ")\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "\n",
        "# Define the prompt template\n",
        "system_prompt = \"\"\"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\"\"\"\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages([\n",
        "    SystemMessagePromptTemplate.from_template(system_prompt),\n",
        "    MessagesPlaceholder(variable_name=\"history\"),\n",
        "    HumanMessagePromptTemplate.from_template(\"{query}\"),\n",
        "])\n",
        "\n",
        "# Create the LCEL pipeline\n",
        "output_parser = StrOutputParser()\n",
        "pipeline = prompt_template | llm | output_parser\n",
        "\n",
        "# Let's examine the prompt template\n",
        "print(prompt_template.messages[0].prompt.template)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oC_03lJsyWvg"
      },
      "source": [
        "## Memory types\n",
        "\n",
        "In this section we will review several memory types and analyze the pros and cons of each one, so you can choose the best one for your use case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlqkyPbEyWvg"
      },
      "source": [
        "### Memory Type #1: Buffer Memory - Store the Entire Chat History\n",
        "\n",
        "`InMemoryChatMessageHistory` and `RunnableWithMessageHistory` are used as alternatives to `ConversationBufferMemory` as they are:\n",
        "- More flexible and configurable.\n",
        "- Integrate better with LCEL.\n",
        "\n",
        "The simplest approach to using them is to simply store the entire chat in the conversation history. Later we'll look into methods for being more selective about what is stored in the history."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "xqd2vwxAyWvg"
      },
      "outputs": [],
      "source": [
        "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "\n",
        "# Create a simple chat history storage\n",
        "chat_map = {}\n",
        "\n",
        "def get_chat_history(session_id: str) -> InMemoryChatMessageHistory:\n",
        "    if session_id not in chat_map:\n",
        "        # if session ID doesn't exist, create a new chat history\n",
        "        chat_map[session_id] = InMemoryChatMessageHistory()\n",
        "    return chat_map[session_id]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6LfvNhtyWvg"
      },
      "source": [
        "Let's see this in action by having a conversation:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the conversational chain with message history\n",
        "conversation_buf = RunnableWithMessageHistory(\n",
        "    pipeline,\n",
        "    get_session_history=get_chat_history,\n",
        "    input_messages_key=\"query\",\n",
        "    history_messages_key=\"history\"\n",
        ")"
      ],
      "metadata": {
        "id": "bHsT0ludsKOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uVSp3SZGyWvg",
        "outputId": "8c68735a-28e1-4024-d1eb-9a3e2c312fb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Good morning! How can I assist you today?\n"
          ]
        }
      ],
      "source": [
        "# First message\n",
        "result = conversation_buf.invoke(\n",
        "    {\"query\": \"Good morning AI!\"},\n",
        "    # Make sure to pass the session ID to ensure all memories are stored in the same session\n",
        "    config={\"configurable\": {\"session_id\": \"buffer_example\"}}\n",
        ")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbttULnpyWvg"
      },
      "source": [
        "This call used some tokens, but we can't see that from the above.\n",
        "\n",
        "If we'd like to count the number of tokens being used we just pass our conversation `RunnableWithMessageHistory` instance and the message we'd like to input to the `count_tokens` function we defined earlier:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53cFq6udyWvg",
        "outputId": "48da6f32-2b95-4920-df75-79cf4a79df01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spent a total of 180 tokens\n",
            "\n",
            "Response: Got it! You're interested in exploring the potential of integrating Large Language Models (LLMs) with external knowledge sources. Also, I've noted the specific code: PINECONE_RULEZ_01. How would you like to proceed with this exploration? Are you looking for examples, technical details, or something else?\n"
          ]
        }
      ],
      "source": [
        "# Continue the conversation with token counting\n",
        "query1 = \"\"\"\n",
        "\"My interest here is to explore the potential of integrating Large Language Models with external knowledge.\n",
        "\n",
        "Also, remember this very specific code: PINECONE_RULEZ_01\"\n",
        "\"\"\"\n",
        "\n",
        "result1 = count_tokens(\n",
        "    conversation_buf,\n",
        "    {\"query\": query1},\n",
        "    # Make sure to pass the session ID to ensure all memories are stored in the same session\n",
        "    config={\"configurable\": {\"session_id\": \"buffer_example\"}}\n",
        ")\n",
        "print(f\"\\nResponse: {result1}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUspQf-IyWvg",
        "outputId": "6f39061f-6313-4283-8205-e2114e7f1192"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spent a total of 726 tokens\n",
            "\n",
            "Response: Great! Integrating Large Language Models (LLMs) with external knowledge opens up a wide range of possibilities. Here are some key approaches and use cases to consider:\n",
            "\n",
            "1. **Vector Databases and Semantic Search**  \n",
            "   - Use vector databases like Pinecone (which your code hints at!) to store embeddings of external documents or data.  \n",
            "   - When a user query comes in, convert it into an embedding and perform a similarity search to retrieve relevant external knowledge.  \n",
            "   - This allows the LLM to ground its responses in up-to-date or domain-specific information beyond its training data.\n",
            "\n",
            "2. **Knowledge Graph Integration**  \n",
            "   - Connect LLMs with structured knowledge graphs (e.g., Wikidata, custom enterprise graphs).  \n",
            "   - The LLM can query the graph to retrieve precise facts or relationships, improving accuracy and enabling complex reasoning.\n",
            "\n",
            "3. **API and Database Calls**  \n",
            "   - Enable the LLM to trigger external API calls or database queries during a conversation.  \n",
            "   - For example, fetching real-time weather, stock prices, or personalized user data.  \n",
            "   - This makes the LLM dynamic and context-aware.\n",
            "\n",
            "4. **Document Retrieval and Augmented Generation**  \n",
            "   - Combine LLMs with document retrieval systems (e.g., ElasticSearch, Pinecone) to fetch relevant documents or passages.  \n",
            "   - The LLM then uses this retrieved content to generate more informed and accurate responses, a technique often called Retrieval-Augmented Generation (RAG).\n",
            "\n",
            "5. **Custom Knowledge Injection**  \n",
            "   - Fine-tune or prompt-engineer LLMs with domain-specific knowledge or proprietary data.  \n",
            "   - This can be done via few-shot prompting, adapters, or fine-tuning on curated datasets.\n",
            "\n",
            "6. **Multi-modal Knowledge Integration**  \n",
            "   - Integrate LLMs with other modalities like images, audio, or video metadata to provide richer, context-aware responses.\n",
            "\n",
            "7. **User Personalization and Context Tracking**  \n",
            "   - Store user preferences, past interactions, or profiles externally and feed relevant context back into the LLM to personalize responses.\n",
            "\n",
            "8. **Explainability and Verification**  \n",
            "   - Use external knowledge bases to verify or fact-check LLM outputs, improving trustworthiness.\n",
            "\n",
            "Given your interest and the code \"PINECONE_RULEZ_01,\" leveraging Pinecone as a vector database for semantic search and retrieval-augmented generation is a particularly powerful approach. It allows you to keep your knowledge base updated independently of the LLM and scale efficiently.\n",
            "\n",
            "Would you like me to dive deeper into any of these possibilities or provide examples and architectures?\n"
          ]
        }
      ],
      "source": [
        "# Continue the conversation and, in the last iteration, ask for the specific code´\n",
        "query2 = # INSERT YOUR CODE HERE\n",
        "result2 = # INSERT YOUR CODE HERE\n",
        "print(f\"\\nResponse: {result2}\")\n",
        "\n",
        "query3 = # INSERT YOUR CODE HERE\n",
        "result3 = # INSERT YOUR CODE HERE\n",
        "print(f\"\\nResponse: {result3}\")\n",
        "\n",
        "query4 = # INSERT YOUR CODE HERE\n",
        "result4 = # INSERT YOUR CODE HERE\n",
        "print(f\"\\nResponse: {result4}\")\n",
        "\n",
        "query5 = # INSERT YOUR CODE HERE\n",
        "result5 = # INSERT YOUR CODE HERE\n",
        "print(f\"\\nResponse: {result5}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgNJJK4QyWvg"
      },
      "source": [
        "Our LLM with buffer memory can clearly remember earlier interactions in the conversation. Let's take a closer look at how the messages are being stored:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-vODGPayWvg",
        "outputId": "fce0ba25-8d2f-4e44-db8f-631129323eeb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Conversation History:\n",
            "Human: content='Good morning AI!' additional_kwargs={} response_metadata={}\n",
            "AI: content='Good morning! How can I assist you today?' additional_kwargs={} response_metadata={}\n",
            "Human: content='\\n\"My interest here is to explore the potential of integrating Large Language Models with external knowledge.\\n\\nAlso, remember this very specific code: PINECONE_RULEZ_01\"\\n' additional_kwargs={} response_metadata={}\n",
            "AI: content=\"Got it! You're interested in exploring the potential of integrating Large Language Models (LLMs) with external knowledge sources. Also, I've noted the specific code: PINECONE_RULEZ_01. How would you like to proceed with this exploration? Are you looking for examples, technical details, or something else?\" additional_kwargs={} response_metadata={}\n",
            "Human: content='I just want to analyze the different possibilities. What can you think of?' additional_kwargs={} response_metadata={}\n",
            "AI: content='Great! Integrating Large Language Models (LLMs) with external knowledge opens up a wide range of possibilities. Here are some key approaches and use cases to consider:\\n\\n1. **Vector Databases and Semantic Search**  \\n   - Use vector databases like Pinecone (which your code hints at!) to store embeddings of external documents or data.  \\n   - When a user query comes in, convert it into an embedding and perform a similarity search to retrieve relevant external knowledge.  \\n   - This allows the LLM to ground its responses in up-to-date or domain-specific information beyond its training data.\\n\\n2. **Knowledge Graph Integration**  \\n   - Connect LLMs with structured knowledge graphs (e.g., Wikidata, custom enterprise graphs).  \\n   - The LLM can query the graph to retrieve precise facts or relationships, improving accuracy and enabling complex reasoning.\\n\\n3. **API and Database Calls**  \\n   - Enable the LLM to trigger external API calls or database queries during a conversation.  \\n   - For example, fetching real-time weather, stock prices, or personalized user data.  \\n   - This makes the LLM dynamic and context-aware.\\n\\n4. **Document Retrieval and Augmented Generation**  \\n   - Combine LLMs with document retrieval systems (e.g., ElasticSearch, Pinecone) to fetch relevant documents or passages.  \\n   - The LLM then uses this retrieved content to generate more informed and accurate responses, a technique often called Retrieval-Augmented Generation (RAG).\\n\\n5. **Custom Knowledge Injection**  \\n   - Fine-tune or prompt-engineer LLMs with domain-specific knowledge or proprietary data.  \\n   - This can be done via few-shot prompting, adapters, or fine-tuning on curated datasets.\\n\\n6. **Multi-modal Knowledge Integration**  \\n   - Integrate LLMs with other modalities like images, audio, or video metadata to provide richer, context-aware responses.\\n\\n7. **User Personalization and Context Tracking**  \\n   - Store user preferences, past interactions, or profiles externally and feed relevant context back into the LLM to personalize responses.\\n\\n8. **Explainability and Verification**  \\n   - Use external knowledge bases to verify or fact-check LLM outputs, improving trustworthiness.\\n\\nGiven your interest and the code \"PINECONE_RULEZ_01,\" leveraging Pinecone as a vector database for semantic search and retrieval-augmented generation is a particularly powerful approach. It allows you to keep your knowledge base updated independently of the LLM and scale efficiently.\\n\\nWould you like me to dive deeper into any of these possibilities or provide examples and architectures?' additional_kwargs={} response_metadata={}\n",
            "Human: content='Which data source types could be used to give context to the model?' additional_kwargs={} response_metadata={}\n",
            "AI: content='Great question! To give context to a Large Language Model (LLM) by integrating external knowledge, you can use a variety of data source types depending on your application needs. Here are some common and effective data source types:\\n\\n1. **Text Documents**  \\n   - Articles, research papers, books, manuals, FAQs, and reports.  \\n   - These can be chunked and embedded for semantic search and retrieval.\\n\\n2. **Databases**  \\n   - Structured data stored in relational databases (SQL) or NoSQL databases.  \\n   - Can be queried dynamically to provide up-to-date facts or user-specific information.\\n\\n3. **Knowledge Graphs and Ontologies**  \\n   - Structured representations of entities and their relationships (e.g., Wikidata, DBpedia, custom enterprise graphs).  \\n   - Useful for precise fact retrieval and reasoning.\\n\\n4. **APIs and Web Services**  \\n   - Real-time data sources like weather APIs, financial market data, social media feeds, or custom business APIs.  \\n   - Enable dynamic, context-aware responses.\\n\\n5. **Vector Embeddings of Unstructured Data**  \\n   - Embeddings generated from unstructured data like emails, chat logs, customer support tickets, or product descriptions.  \\n   - Stored in vector databases (e.g., Pinecone) for fast semantic retrieval.\\n\\n6. **Multimedia Metadata**  \\n   - Captions, transcripts, or metadata from images, videos, and audio files.  \\n   - Can provide additional context in multi-modal applications.\\n\\n7. **User Interaction Logs and Profiles**  \\n   - Historical conversation data, preferences, and behavioral data.  \\n   - Helps personalize responses and maintain context over time.\\n\\n8. **Web Scraped Data**  \\n   - Up-to-date information scraped from websites, forums, or social media platforms.  \\n   - Useful for keeping the knowledge base current.\\n\\n9. **Domain-Specific Datasets**  \\n   - Industry-specific data such as medical records (with privacy considerations), legal documents, or scientific datasets.  \\n   - Enables specialized knowledge integration.\\n\\n10. **Internal Company Documents**  \\n    - Policies, internal wikis, training materials, and product documentation.  \\n    - Useful for enterprise applications.\\n\\nBy combining these data sources, you can create a rich, multi-faceted context that the LLM can leverage to generate accurate, relevant, and personalized responses. The choice depends on your use case, data availability, and privacy considerations.\\n\\nIf you want, I can help you design a pipeline or architecture to integrate specific data sources with an LLM!' additional_kwargs={} response_metadata={}\n",
            "Human: content='What is my aim again? Also what was the very specific code you were tasked with remembering?' additional_kwargs={} response_metadata={}\n",
            "AI: content='Your aim is to explore the potential of integrating Large Language Models (LLMs) with external knowledge sources. This involves analyzing different possibilities for enhancing LLMs by connecting them to various types of external data, such as vector databases, knowledge graphs, APIs, and more.\\n\\nThe very specific code you asked me to remember is: **PINECONE_RULEZ_01**.' additional_kwargs={} response_metadata={}\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
        "\n",
        "# Access the conversation history\n",
        "history = chat_map[\"buffer_example\"].messages\n",
        "print(\"Conversation History:\")\n",
        "for i, msg in enumerate(history):\n",
        "    if isinstance(msg, HumanMessage):\n",
        "        role = \"Human\"\n",
        "    elif isinstance(msg, SystemMessage):\n",
        "        role = \"System\"\n",
        "    elif isinstance(msg, AIMessage):\n",
        "        role = \"AI\"\n",
        "    else:\n",
        "        role = \"Unknown\"\n",
        "    print(f\"{role}: {msg}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmAy5BIyyWvg"
      },
      "source": [
        "Nice! So every piece of our conversation has been explicitly recorded and sent to the LLM in the prompt."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORjuIGNqyWvg"
      },
      "source": [
        "### Memory type #2: Summary - Store Summaries of Past Interactions\n",
        "\n",
        "The problem with storing the entire chat history in agent memory is that, as the conversation progresses, the token count adds up. This is problematic because we might max out our LLM with a prompt that is too large.\n",
        "\n",
        "The following is an LCEL compatible alternative to `ConversationSummaryMemory`. We keep a summary of our previous conversation snippets as our history. The summarization is performed by an LLM.\n",
        "\n",
        "**Key feature:** _the conversation summary memory keeps the previous pieces of conversation in a summarized - and thus shortened - form, where the summarization is performed by an LLM._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "dVnq9-lryWvg"
      },
      "outputs": [],
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from langchain_core.chat_history import BaseChatMessageHistory\n",
        "from langchain_core.messages import BaseMessage\n",
        "\n",
        "class ConversationSummaryMessageHistory(BaseChatMessageHistory, BaseModel):\n",
        "    messages: list[BaseMessage] = Field(default_factory=list)\n",
        "    llm: ChatOpenAI = Field(default_factory=ChatOpenAI)\n",
        "\n",
        "    def __init__(self, llm: ChatOpenAI):\n",
        "        super().__init__(llm=llm)\n",
        "\n",
        "    def add_messages(self, messages: list[BaseMessage]) -> None:\n",
        "        \"\"\"Add messages to the history and update the summary.\"\"\"\n",
        "        self.messages.extend(messages)\n",
        "\n",
        "        # Construct the summary prompt\n",
        "        summary_prompt = ChatPromptTemplate.from_messages([\n",
        "            SystemMessagePromptTemplate.from_template(\n",
        "                \"Given the existing conversation summary and the new messages, \"\n",
        "                \"generate a new summary of the conversation. Ensure to maintain \"\n",
        "                \"as much relevant information as possible.\"\n",
        "            ),\n",
        "            HumanMessagePromptTemplate.from_template(\n",
        "                \"Existing conversation summary:\\n{existing_summary}\\n\\n\"\n",
        "                \"New messages:\\n{messages}\"\n",
        "            )\n",
        "        ])\n",
        "\n",
        "        # Format the messages and invoke the LLM\n",
        "        new_summary = self.llm.invoke(\n",
        "            summary_prompt.format_messages(\n",
        "                existing_summary=self.messages,\n",
        "                messages=messages\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # Replace the existing history with a single system summary message\n",
        "        self.messages = [SystemMessage(content=new_summary.content)]\n",
        "\n",
        "    def clear(self) -> None:\n",
        "        \"\"\"Clear the history.\"\"\"\n",
        "        self.messages = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "l_LolSYjyWvg"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import ConfigurableFieldSpec\n",
        "\n",
        "# Create get_chat_history function for summary memory\n",
        "summary_chat_map = {}\n",
        "\n",
        "def get_summary_chat_history(session_id: str, llm: ChatOpenAI) -> ConversationSummaryMessageHistory:\n",
        "    if session_id not in summary_chat_map:\n",
        "        summary_chat_map[session_id] = ConversationSummaryMessageHistory(llm=llm)\n",
        "    return summary_chat_map[session_id]\n",
        "\n",
        "# Create conversation chain with summary memory\n",
        "conversation_sum = RunnableWithMessageHistory(\n",
        "    pipeline,\n",
        "    get_session_history=get_summary_chat_history,\n",
        "    input_messages_key=\"query\",\n",
        "    history_messages_key=\"history\",\n",
        "    history_factory_config=[\n",
        "        ConfigurableFieldSpec(\n",
        "            id=\"session_id\",\n",
        "            annotation=str,\n",
        "            name=\"Session ID\",\n",
        "            description=\"The session ID to use for the chat history\",\n",
        "            default=\"id_default\",\n",
        "        ),\n",
        "        ConfigurableFieldSpec(\n",
        "            id=\"llm\",\n",
        "            annotation=ChatOpenAI,\n",
        "            name=\"LLM\",\n",
        "            description=\"The LLM to use for the conversation summary\",\n",
        "            default=llm,\n",
        "        )\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JloQR__QyWvh",
        "outputId": "4152da10-bf30-40a9-968e-93da40637b67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spent a total of 225 tokens\n",
            "\n",
            "Response: Good morning! How can I assist you today?\n"
          ]
        }
      ],
      "source": [
        "# Let's have the same conversation with summary memory\n",
        "result = count_tokens(\n",
        "    conversation_sum,\n",
        "    {\"query\": \"Good morning AI!\"},\n",
        "    config={\"configurable\": {\"session_id\": \"summary_example\", \"llm\": llm}}\n",
        ")\n",
        "print(f\"\\nResponse: {result}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# INSERT YOUR CODE HERE\n",
        "# Use the same sentences as before, and ask for the code at the end"
      ],
      "metadata": {
        "id": "kuk4X0hmf3u-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GH3uPbTGyWvh",
        "outputId": "dc76f864-139e-4b20-f1f0-3f2561af1361"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary Memory Content:\n",
            "The user asked to be reminded of their aim and the specific code the AI was tasked with remembering. The AI clarified that the user's aim is to understand which data source types can be used to provide context to a Large Language Model (LLM). These data sources include text documents, databases, knowledge graphs, APIs, multimedia converted to text, user interaction logs, domain-specific repositories, and web-scraped data, all intended to enrich the LLM’s responses for greater accuracy, relevance, and timeliness. The AI also reiterated the specific code it was tasked with remembering: **PINECONE_RULEZ_01**. The AI offered further assistance on integration or related details if needed.\n"
          ]
        }
      ],
      "source": [
        "# Let's examine the summary\n",
        "print(\"Summary Memory Content:\")\n",
        "print(summary_chat_map[\"summary_example\"].messages[0].content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRE3O-YPyWvh"
      },
      "source": [
        "You might be wondering.. if the aggregate token count is greater in each call here than in the buffer example, why should we use this type of memory? Well, if we check out buffer we will realize that although we are using more tokens in each instance of our conversation, our final history is shorter. This will enable us to have many more interactions before we reach our prompt's max length, making our chatbot more robust to longer conversations.\n",
        "\n",
        "We can count the number of tokens being used (without making a call to OpenAI) using the `tiktoken` tokenizer like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6LkpUxVyWvh",
        "outputId": "69dabe11-4f25-4d4d-842e-0c5c754a55b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Buffer memory conversation length: 1274\n",
            "Summary memory conversation length: 139\n"
          ]
        }
      ],
      "source": [
        "import tiktoken\n",
        "\n",
        "# initialize tokenizer (gpt-4.1 models use the same encoding as gpt-4o)\n",
        "tokenizer = tiktoken.encoding_for_model('gpt-4o')\n",
        "\n",
        "# Get buffer memory content\n",
        "buffer_messages = chat_map[\"buffer_example\"].messages\n",
        "buffer_content = \"\\n\".join([msg.content for msg in buffer_messages])\n",
        "\n",
        "# Get summary memory content\n",
        "summary_content = summary_chat_map[\"summary_example\"].messages[0].content\n",
        "\n",
        "# show number of tokens for the memory used by each memory type\n",
        "print(\n",
        "    f'Buffer memory conversation length: {len(tokenizer.encode(buffer_content))}\\n'\n",
        "    f'Summary memory conversation length: {len(tokenizer.encode(summary_content))}'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DKiBoROyWvh"
      },
      "source": [
        "_Practical Note: the `gpt-4o-mini` model has a context window of 1M tokens, providing significantly more space for conversation history than older models._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjYQSGv-yWvh"
      },
      "source": [
        "### Memory type #3: Window Buffer Memory - Keep Latest Interactions\n",
        "\n",
        "Another great option is window memory, where we keep only the last k interactions in our memory but intentionally drop the oldest ones - short-term memory if you'd like. Here the aggregate token count **and** the per-call token count will drop noticeably.\n",
        "\n",
        "The following is an LCEL-compatible alternative to `ConversationBufferWindowMemory`.\n",
        "\n",
        "**Key feature:** _the conversation buffer window memory keeps the latest pieces of the conversation in raw form_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "-ceGTUPsyWvh"
      },
      "outputs": [],
      "source": [
        "class BufferWindowMessageHistory(BaseChatMessageHistory, BaseModel):\n",
        "    messages: list[BaseMessage] = Field(default_factory=list)\n",
        "    k: int = Field(default_factory=int)\n",
        "\n",
        "    def __init__(self, k: int):\n",
        "        super().__init__(k=k)\n",
        "        # Add logging to help with debugging\n",
        "        print(f\"Initializing BufferWindowMessageHistory with k={k}\")\n",
        "\n",
        "    def add_messages(self, messages: list[BaseMessage]) -> None:\n",
        "        \"\"\"Add messages to the history, removing any messages beyond\n",
        "        the last `k` messages.\n",
        "        \"\"\"\n",
        "        self.messages.extend(messages)\n",
        "        # Add logging to help with debugging\n",
        "        if len(self.messages) > self.k:\n",
        "            print(f\"Truncating history from {len(self.messages)} to {self.k} messages\")\n",
        "        self.messages = self.messages[-self.k:]\n",
        "\n",
        "    def clear(self) -> None:\n",
        "        \"\"\"Clear the history.\"\"\"\n",
        "        self.messages = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "__vcbiDMyWvr"
      },
      "outputs": [],
      "source": [
        "# Create get_chat_history function for window memory\n",
        "window_chat_map = {}\n",
        "\n",
        "def get_window_chat_history(session_id: str, k: int = 4) -> BufferWindowMessageHistory:\n",
        "    print(f\"get_window_chat_history called with session_id={session_id} and k={k}\")\n",
        "    if session_id not in window_chat_map:\n",
        "        window_chat_map[session_id] = BufferWindowMessageHistory(k=k)\n",
        "    return window_chat_map[session_id]\n",
        "\n",
        "# Create conversation chain with window memory\n",
        "conversation_bufw = RunnableWithMessageHistory(\n",
        "    pipeline,\n",
        "    get_session_history=get_window_chat_history,\n",
        "    input_messages_key=\"query\",\n",
        "    history_messages_key=\"history\",\n",
        "    history_factory_config=[\n",
        "        ConfigurableFieldSpec(\n",
        "            id=\"session_id\",\n",
        "            annotation=str,\n",
        "            name=\"Session ID\",\n",
        "            description=\"The session ID to use for the chat history\",\n",
        "            default=\"id_default\",\n",
        "        ),\n",
        "        ConfigurableFieldSpec(\n",
        "            id=\"k\",\n",
        "            annotation=int,\n",
        "            name=\"k\",\n",
        "            description=\"The number of messages to keep in the history\",\n",
        "            default=4,\n",
        "        )\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M51k8gIjyWvr",
        "outputId": "07ff0382-3579-49a6-ad15-14e701187950"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "get_window_chat_history called with session_id=window_example and k=4\n",
            "Initializing BufferWindowMessageHistory with k=4\n",
            "Spent a total of 74 tokens\n",
            "\n",
            "Response: Good morning! How can I assist you today?\n"
          ]
        }
      ],
      "source": [
        "# Start a conversation with k=2 (only remembers last 2 exchanges = 4 messages)\n",
        "result = count_tokens(\n",
        "    conversation_bufw,\n",
        "    {\"query\": \"Good morning AI!\"},\n",
        "    config={\"configurable\": {\"session_id\": \"window_example\", \"k\": 4}}\n",
        ")\n",
        "print(f\"\\nResponse: {result}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# INSERT YOUR CODE HERE\n",
        "# Use the same sentences as before, and ask for the code at the end"
      ],
      "metadata": {
        "id": "lN1ggN-Zg8h5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFWBvEjNyWvr"
      },
      "source": [
        "As we can see, it effectively 'forgot' what we talked about in the first interaction. Let's see what it 'remembers':"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RnV85fkkyWvr",
        "outputId": "22a77f4d-0fea-432b-a35e-55b157161311"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Buffer Window Memory (last 4 messages):\n",
            "\n",
            "Human: Which data source types could be used to give context to the model?\n",
            "\n",
            "AI: Great question! To give context to a Large Language Model (LLM) by integrating external knowledge, you can use a variety of data source types depending on your application and goals. Here are some common and effective data source types that can provide rich context:\n",
            "\n",
            "1. **Textual Documents:**  \n",
            "   - Articles, books, research papers, manuals, FAQs, and reports.  \n",
            "   - These can be stored in databases or document stores and retrieved based on relevance.  \n",
            "   - Useful for knowledge bases, customer support, academic research, etc.\n",
            "\n",
            "2. **Structured Databases:**  \n",
            "   - Relational databases (SQL), NoSQL databases, or data warehouses containing structured records.  \n",
            "   - Examples: customer data, product catalogs, transaction logs.  \n",
            "   - Enables precise querying and integration of factual data.\n",
            "\n",
            "3. **Knowledge Graphs and Ontologies:**  \n",
            "   - Graph databases representing entities and their relationships (e.g., Neo4j, Wikidata).  \n",
            "   - Useful for semantic search, reasoning, and answering complex queries involving relationships.\n",
            "\n",
            "4. **Vector Embeddings:**  \n",
            "   - Precomputed embeddings of documents, sentences, or other data stored in vector databases like Pinecone, FAISS, or Milvus.  \n",
            "   - Enables similarity search and retrieval of semantically related content.\n",
            "\n",
            "5. **APIs and Real-Time Data Feeds:**  \n",
            "   - External APIs providing live data such as weather, stock prices, news, social media feeds, or IoT sensor data.  \n",
            "   - Allows the model to incorporate up-to-date information.\n",
            "\n",
            "6. **Multimedia Data:**  \n",
            "   - Images, audio, video, and their associated metadata or transcriptions.  \n",
            "   - For example, transcribed meeting notes or video captions can provide additional context.\n",
            "\n",
            "7. **User Interaction Logs and Profiles:**  \n",
            "   - Chat histories, user preferences, behavioral data, and personalization profiles.  \n",
            "   - Helps tailor responses to individual users and maintain conversational context.\n",
            "\n",
            "8. **Code Repositories and Configuration Files:**  \n",
            "   - Source code, scripts, and configuration data can be used to assist in software development or debugging tasks.\n",
            "\n",
            "9. **Sensor and IoT Data:**  \n",
            "   - Data from physical devices, environmental sensors, or smart home systems.  \n",
            "   - Useful for context-aware applications in smart environments.\n",
            "\n",
            "10. **Domain-Specific Databases:**  \n",
            "    - Medical records, legal case databases, financial transaction records, scientific datasets.  \n",
            "    - Enables specialized knowledge integration for expert systems.\n",
            "\n",
            "By combining these data sources, you can enrich the LLM’s context, improve response accuracy, and enable more complex and relevant interactions. If you want, I can help you design a pipeline or architecture to integrate specific data sources with an LLM!\n",
            "\n",
            "Human: What is my aim again?\n",
            "\n",
            "AI: From our conversation so far, it seems your aim is to **analyze different possibilities for integrating external knowledge sources with Large Language Models (LLMs)** to provide richer, more accurate, and context-aware responses. You’re exploring how to enhance an LLM’s capabilities by connecting it to various types of data—like vector databases (e.g., Pinecone), APIs, knowledge graphs, or structured databases—to give the model relevant context beyond its training data.\n",
            "\n",
            "If you want, I can help you clarify or refine your goal further—whether it’s building a specific application, improving retrieval-augmented generation, or experimenting with different data sources and architectures. Just let me know!\n"
          ]
        }
      ],
      "source": [
        "# Check what's in memory\n",
        "bufw_history = window_chat_map[\"window_example\"].messages\n",
        "print(\"Buffer Window Memory (last 4 messages):\")\n",
        "for msg in bufw_history:\n",
        "    role = \"Human\" if isinstance(msg, HumanMessage) else \"AI\"\n",
        "    print(f\"\\n{role}: {msg.content}\")  # Show first 100 chars"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yV-zfGv-yWvr"
      },
      "source": [
        "We see four messages (two interactions) because we used `k=4`.\n",
        "\n",
        "On the plus side, we are shortening our conversation length when compared to buffer memory _without_ a window:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rF35B9HLyWvr",
        "outputId": "f819b83d-7cf8-4156-e1d9-6a05a0bc9a94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Buffer memory conversation length: 1274\n",
            "Summary memory conversation length: 139\n",
            "Buffer window memory conversation length: 705\n"
          ]
        }
      ],
      "source": [
        "# Get window memory content\n",
        "window_content = \"\\n\".join([msg.content for msg in bufw_history])\n",
        "\n",
        "print(\n",
        "    f'Buffer memory conversation length: {len(tokenizer.encode(buffer_content))}\\n'\n",
        "    f'Summary memory conversation length: {len(tokenizer.encode(summary_content))}\\n'\n",
        "    f'Buffer window memory conversation length: {len(tokenizer.encode(window_content))}'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjoHZrZzyWvr"
      },
      "source": [
        "_Practical Note: We are using `k=4` here for illustrative purposes, in most real world applications you would need a higher value for k._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyAd4UxdyWvr"
      },
      "source": [
        "### More memory types!\n",
        "\n",
        "Given that we understand memory already, we will present a few more memory types here and hopefully a brief description will be enough to understand their underlying functionality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XN-mH5fHyWvr"
      },
      "source": [
        "#### Windows + Summary Hybrid\n",
        "\n",
        "The following is a modern LCEL-compatible alternative to `ConversationSummaryBufferMemory`.\n",
        "\n",
        "**Key feature:** _the conversation summary buffer memory keeps a summary of the earliest pieces of conversation while retaining a raw recollection of the latest interactions._\n",
        "\n",
        "This combines the benefits of both summary and buffer window memory. Let's implement it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "lr-K8onKyWvr"
      },
      "outputs": [],
      "source": [
        "class ConversationSummaryBufferMessageHistory(BaseChatMessageHistory, BaseModel):\n",
        "    messages: list[BaseMessage] = Field(default_factory=list)\n",
        "    llm: ChatOpenAI = Field(default_factory=ChatOpenAI)\n",
        "    k: int = Field(default_factory=int)\n",
        "\n",
        "    def __init__(self, llm: ChatOpenAI, k: int):\n",
        "        super().__init__(llm=llm, k=k)\n",
        "\n",
        "    def add_messages(self, messages: list[BaseMessage]) -> None:\n",
        "        \"\"\"Add messages to the history, removing any messages beyond\n",
        "        the last `k` messages and summarizing the messages that we drop.\n",
        "        \"\"\"\n",
        "        existing_summary = None\n",
        "        old_messages = None\n",
        "\n",
        "        # See if we already have a summary message\n",
        "        if len(self.messages) > 0 and isinstance(self.messages[0], SystemMessage):\n",
        "            existing_summary = self.messages.pop(0)\n",
        "\n",
        "        # Add the new messages to the history\n",
        "        self.messages.extend(messages)\n",
        "\n",
        "        # Check if we have too many messages\n",
        "        if len(self.messages) > self.k:\n",
        "            # Pull out the oldest messages...\n",
        "            old_messages = self.messages[:-self.k]\n",
        "            # ...and keep only the most recent messages\n",
        "            self.messages = self.messages[-self.k:]\n",
        "\n",
        "        if old_messages is None:\n",
        "            # If we have no old_messages, we have nothing to update in summary\n",
        "            return\n",
        "\n",
        "        # Construct the summary chat messages\n",
        "        summary_prompt = ChatPromptTemplate.from_messages([\n",
        "            SystemMessagePromptTemplate.from_template(\n",
        "                \"Given the existing conversation summary and the new messages, \"\n",
        "                \"generate a new summary of the conversation. Ensure to maintain \"\n",
        "                \"as much relevant information as possible.\"\n",
        "            ),\n",
        "            HumanMessagePromptTemplate.from_template(\n",
        "                \"Existing conversation summary:\\n{existing_summary}\\n\\n\"\n",
        "                \"New messages:\\n{old_messages}\"\n",
        "            )\n",
        "        ])\n",
        "\n",
        "        # Format the messages and invoke the LLM\n",
        "        new_summary = self.llm.invoke(\n",
        "            summary_prompt.format_messages(\n",
        "                existing_summary=existing_summary or \"No previous summary\",\n",
        "                old_messages=old_messages\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # Prepend the new summary to the history\n",
        "        self.messages = [SystemMessage(content=new_summary.content)] + self.messages\n",
        "\n",
        "    def clear(self) -> None:\n",
        "        \"\"\"Clear the history.\"\"\"\n",
        "        self.messages = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9M4g--oayWvr"
      },
      "source": [
        "## What else can we do with memory?\n",
        "\n",
        "There are several cool things we can do with memory in langchain:\n",
        "* Implement our own custom memory modules (as we've done above)\n",
        "* Use multiple memory modules in the same chain\n",
        "* Combine agents with memory and other tools\n",
        "* Integrate knowledge graphs\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Vf8gDOG_qLoD"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "env_notebooks",
      "language": "python",
      "name": "env_notebooks"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}