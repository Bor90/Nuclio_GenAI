# Quick Start Guide: Text Generation Evaluator

## What Is It?

A Python library that automatically evaluates the quality of text generated by different LLM decoding strategies (Greedy, Beam Search, Sampling, etc.).

## Location

- **Library file**: `text_evaluator.py` (in the same directory as your notebook)
- **Documentation**: `TEXT_EVALUATOR_README.md`

## Quick Usage Examples

### Example 1: Evaluate All Strategies Automatically

```python
from text_evaluator import compare_texts, print_comparison

# All strategy outputs are compared and scores displayed
results = {
    'Greedy': greedy_output_text,
    'Beam Search': beam_search_output_text,
    'Top-p Sampling': top_p_output_text,
}

comparison = compare_texts(results)
print_comparison(comparison)  # Shows winners and scores
```

**Output**: A comparison table showing which strategy is best for each metric.

### Example 2: Evaluate Single Text

```python
from text_evaluator import evaluate_text, print_results

text = "Your generated text here"
metrics = evaluate_text(text)
print_results(metrics, "My Strategy")
```

**Output**: Detailed metrics including diversity, repetition, coherence, fluency, and overall score.

### Example 3: Analyze Your Custom Experiments

```python
# After running your own generation experiments
my_results = {
    'low_temperature': your_text_1,
    'high_temperature': your_text_2,
}

comparison = compare_texts(my_results)
print_comparison(comparison)
```

## The Four Main Metrics Explained

| Metric | Range | Meaning | Target |
|--------|-------|---------|--------|
| **Diversity** | 0-1 | Vocabulary richness | Higher = More varied words |
| **Repetition** | 0-1 | Repeated n-grams | Lower = Less repetition |
| **Coherence** | 0-1 | Sentence consistency | Higher = Better structure |
| **Fluency** | 0-1 | Natural language patterns | Higher = More natural |

## Interpreting Results

### High Scores (0.8-1.0)
- ✅ Excellent quality for creative writing
- ✅ Very few repetitions
- ✅ Natural, flowing text

### Medium Scores (0.5-0.8)
- ⚠️ Acceptable for most tasks
- ⚠️ Some repetition or inconsistency
- ⚠️ Generally readable

### Low Scores (0.0-0.5)
- ❌ Poor quality
- ❌ Significant issues (repetition, incoherence)
- ❌ May need different strategy

## Strategy Selection Guide

Use the evaluation results to choose the best strategy:

- **Need factual, concise text?** → Greedy or Beam Search
- **Need creative, varied text?** → Top-p or Top-k Sampling
- **Need balanced quality?** → Beam Search
- **Want maximum diversity?** → Top-p Sampling with high temperature

## Run the Evaluator Cell

In the notebook, find the cell titled:
> "Load the text evaluator library"

**This cell automatically:**
1. Loads the evaluator library
2. Evaluates all 6 generation strategies
3. Shows comparative analysis
4. Displays winners for each metric

Just run this cell to see results!

## Tips

1. **Compare on the same prompt** - Fair comparisons need the same starting text
2. **Run multiple times** - Sampling methods are random, so results vary
3. **Check actual text** - Metrics are indicators, not absolute truth
4. **Experiment boldly** - Try different parameters and see how scores change

## Troubleshooting

**Error: `ModuleNotFoundError: No module named 'nltk'`**
→ Install: `pip install nltk`

**Error: `nltk data not found`**
→ Normal! The library downloads it automatically on first use.

**No output or strange results**
→ Make sure you're running the evaluator cell AFTER generating the texts.

## More Information

See `TEXT_EVALUATOR_README.md` for:
- Detailed metric descriptions
- Advanced usage
- Complete function reference
- Metrics interpretation guide
