# Text Generation Evaluator Library

A comprehensive evaluation library for assessing the quality of text generated by different decoding strategies in Large Language Models.

## Features

The `text_evaluator` module provides multiple evaluation metrics for generated text:

### Evaluation Metrics

1. **Length Metrics**
   - Word count
   - Sentence count
   - Average words per sentence
   - Character count

2. **Diversity Score** (0-1, higher is better)
   - Type-Token Ratio (vocabulary richness)
   - Measures how varied the vocabulary is

3. **Repetition Score** (0-1, lower is better)
   - Measures repeated bigrams and trigrams
   - Indicates if the model gets stuck in loops

4. **Coherence Score** (0-1, higher is better)
   - Evaluates sentence structure consistency
   - Measures whether sentences have similar lengths and structure

5. **Fluency Score** (0-1, higher is better)
   - Evaluates stopword distribution
   - Measures if the text follows natural language patterns

## Installation

The module requires NLTK:
```bash
pip install nltk
```

## Usage

### Basic Usage - Evaluate Single Text

```python
from text_evaluator import evaluate_text, print_results

# Evaluate generated text
metrics = evaluate_text("Your generated text here")
print_results(metrics, "My Strategy")
```

### Compare Multiple Strategies

```python
from text_evaluator import compare_texts, print_comparison

results = {
    'Greedy': 'The cat sat on the mat. The cat sat on the mat.',
    'Beam Search': 'The cat sat comfortably on the mat.',
    'Top-p Sampling': 'A feline rested peacefully on the rug.',
}

comparison = compare_texts(results)
print_comparison(comparison)
```

### Advanced - Using TextEvaluator Class

```python
from text_evaluator import TextEvaluator

evaluator = TextEvaluator()

# Evaluate single text
metrics = evaluator.evaluate(generated_text)
evaluator.print_evaluation(metrics, "My Strategy")

# Compare multiple texts
results = {...}
comparison = evaluator.compare_strategies(results)
evaluator.print_comparison(comparison)
```

## Metrics Interpretation

### Diversity Score
- **0.3-0.5**: Low diversity (repetitive vocabulary)
- **0.5-0.7**: Medium diversity (reasonable vocabulary variation)
- **0.7+**: High diversity (rich vocabulary)

### Repetition Score
- **0.0-0.1**: Very good (minimal repetition)
- **0.1-0.3**: Good (some repetition)
- **0.3-0.5**: Fair (noticeable repetition)
- **0.5+**: Poor (significant repetition)

### Coherence Score
- **0.8-1.0**: High coherence (consistent sentence structure)
- **0.6-0.8**: Medium coherence (some inconsistency)
- **0.0-0.6**: Low coherence (very inconsistent)

### Fluency Score
- **0.9-1.0**: Excellent fluency (natural language patterns)
- **0.7-0.9**: Good fluency (mostly natural)
- **0.5-0.7**: Fair fluency (some unnatural patterns)
- **0.0-0.5**: Poor fluency (very unnatural)

## Strategies Comparison Guide

The evaluator helps students understand when to use each decoding strategy:

| Strategy | Best For | Expected Scores |
|----------|----------|-----------------|
| **Greedy** | Short, factual text | High repetition, low diversity |
| **Beam Search** | Balanced quality/speed | Medium diversity, low repetition |
| **Top-k Sampling** | Creative output | High diversity, variable coherence |
| **Top-p (Nucleus) Sampling** | Context-aware diversity | High diversity, good coherence |
| **Temperature** | Fine-grained control | Depends on temperature value |

## Example Output

```
============================================================
STRATEGY COMPARISON SUMMARY
============================================================

üìä Individual Scores by Strategy:
------------------------------------------------------------

GREEDY:
  Overall Score: 0.4523
  Diversity: 0.4200
  Repetition: 0.6500
  Coherence: 0.5200
  Fluency: 0.3100

BEAM SEARCH:
  Overall Score: 0.6234
  Diversity: 0.5800
  Repetition: 0.2100
  Coherence: 0.7200
  Fluency: 0.6700

üèÜ Winners by Category:
------------------------------------------------------------
  Best Diversity: top_p
  Lowest Repetition: beam_search
  Best Coherence: beam_search
  Best Fluency: top_k

============================================================
```

## Functions Reference

### evaluate_text(text, original_prompt=None)
Evaluates a single text and returns metrics dictionary.

### compare_texts(results)
Compares multiple texts and returns comparative analysis.

### print_results(metrics, strategy_name=None)
Pretty prints evaluation metrics for a single text.

### print_comparison(comparison_results)
Pretty prints comparison summary of multiple strategies.

## Tips for Best Results

1. **Use consistent prompts** when comparing strategies
2. **Run multiple generations** to account for randomness in sampling methods
3. **Examine both metrics and actual text** - metrics are indicators, not absolute truth
4. **Experiment with parameters** to understand their impact on output quality
5. **Compare against baselines** (greedy and beam search) to understand improvements

## License

MIT
