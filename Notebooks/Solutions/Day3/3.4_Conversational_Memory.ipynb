{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGjfYQBcyWve"
      },
      "source": [
        "# Conversational Memory with LCEL\n",
        "\n",
        "Conversational memory is how chatbots can respond to our queries in a chat-like manner. It enables a coherent conversation, and without it, every query would be treated as an entirely independent input without considering past interactions.\n",
        "\n",
        "The memory allows an _\"agent\"_ to remember previous interactions with the user. By default, agents are *stateless* — meaning each incoming query is processed independently of other interactions. The only thing that exists for a stateless agent is the current input, nothing else.\n",
        "\n",
        "There are many applications where remembering previous interactions is very important, such as chatbots. Conversational memory allows us to do that.\n",
        "\n",
        "In this notebook we'll explore conversational memory using modern LangChain Expression Language (LCEL) and the recommended `RunnableWithMessageHistory` class.\n",
        "\n",
        "We'll start by importing all of the libraries that we'll be using in this example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ETg8fr8-yWvf",
        "outputId": "4fb47497-29e7-43b8-c4bb-8de433a26caf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.3/65.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m458.9/458.9 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.0/363.0 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m948.6/948.6 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langgraph-prebuilt 1.0.6 requires langchain-core>=1.0.0, but you have langchain-core 0.3.83 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -qU \\\n",
        "  langchain==0.3.25 \\\n",
        "  langchain-community==0.3.25 \\\n",
        "  langchain-openai==0.3.22 \\\n",
        "  tiktoken==0.9.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSvjQpbKyWvf"
      },
      "source": [
        "To run this notebook, we will need to use an OpenAI LLM. Here we will setup the LLM we will use for the whole notebook, just input your openai api key if prompted, otherwise it will use the `OPENAI_API_KEY` environment variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnquGYaQyWvf",
        "outputId": "13dde5e1-7fcd-434d-a784-9e998edda748"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your OpenAI API key: ··········\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\") \\\n",
        "    or getpass(\"Enter your OpenAI API key: \")\n",
        "\n",
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wFhsehZEyWvf"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    temperature=0,\n",
        "    openai_api_key=OPENAI_API_KEY,\n",
        "    model_name='gpt-4.1-mini'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPQgxde4yWvf"
      },
      "source": [
        "Later we will make use of a `count_tokens` utility function. This will allow us to count the number of tokens we are using for each call. We define it as so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "YG0RXg5PyWvf"
      },
      "outputs": [],
      "source": [
        "from langchain.callbacks import get_openai_callback\n",
        "\n",
        "def count_tokens(pipeline, query, config=None):\n",
        "    with get_openai_callback() as cb:\n",
        "        # Handle both dict and string inputs\n",
        "        if isinstance(query, str):\n",
        "            query = {\"query\": query}\n",
        "\n",
        "        # Use provided config `or default\n",
        "        if config is None:\n",
        "            config = {\"configurable\": {\"session_id\": \"default\"}}\n",
        "\n",
        "        result = pipeline.invoke(query, config=config)\n",
        "        print(f'Spent a total of {cb.total_tokens} tokens')\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPk7c5IgyWvf"
      },
      "source": [
        "Now let's dive into **Conversational Memory** using LCEL.\n",
        "\n",
        "## What is memory?\n",
        "\n",
        "**Definition**: Memory is an agent's capacity of remembering previous interactions with the user (think chatbots)\n",
        "\n",
        "The official definition of memory is the following:\n",
        "\n",
        "> By default, Chains and Agents are stateless, meaning that they treat each incoming query independently. In some applications (chatbots being a GREAT example) it is highly important to remember previous interactions, both at a short term but also at a long term level. The concept of \"Memory\" exists to do exactly that.\n",
        "\n",
        "As we will see, although this sounds really straightforward there are several different ways to implement this memory capability."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZgnUOmSyWvf"
      },
      "source": [
        "## Building Conversational Chains with LCEL\n",
        "\n",
        "Before we delve into the different memory types, let's understand how to build conversational chains using LCEL. The key components are:\n",
        "\n",
        "1. **Prompt Template** - Defines the conversation structure with placeholders for history and input\n",
        "2. **LLM** - The language model that generates responses\n",
        "3. **Output Parser** - Converts the LLM output to the desired format (optional)\n",
        "4. **RunnableWithMessageHistory** - Manages conversation history\n",
        "\n",
        "Let's create our base conversational chain:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TEpNx9oNyWvg",
        "outputId": "0e771ce3-aa41-42a0-d25e-23e233f5375a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n"
          ]
        }
      ],
      "source": [
        "from langchain.prompts import (\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        "    MessagesPlaceholder\n",
        ")\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "\n",
        "# Define the prompt template\n",
        "system_prompt = \"\"\"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\"\"\"\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages([\n",
        "    SystemMessagePromptTemplate.from_template(system_prompt),\n",
        "    MessagesPlaceholder(variable_name=\"history\"),\n",
        "    HumanMessagePromptTemplate.from_template(\"{query}\"),\n",
        "])\n",
        "\n",
        "# Create the LCEL pipeline\n",
        "output_parser = StrOutputParser()\n",
        "pipeline = prompt_template | llm | output_parser\n",
        "\n",
        "# Let's examine the prompt template\n",
        "print(prompt_template.messages[0].prompt.template)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oC_03lJsyWvg"
      },
      "source": [
        "## Memory types\n",
        "\n",
        "In this section we will review several memory types and analyze the pros and cons of each one, so you can choose the best one for your use case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlqkyPbEyWvg"
      },
      "source": [
        "### Memory Type #1: Buffer Memory - Store the Entire Chat History\n",
        "\n",
        "`InMemoryChatMessageHistory` and `RunnableWithMessageHistory` are used as alternatives to `ConversationBufferMemory` as they are:\n",
        "- More flexible and configurable.\n",
        "- Integrate better with LCEL.\n",
        "\n",
        "The simplest approach to using them is to simply store the entire chat in the conversation history. Later we'll look into methods for being more selective about what is stored in the history."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "xqd2vwxAyWvg"
      },
      "outputs": [],
      "source": [
        "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "\n",
        "# Create a simple chat history storage\n",
        "chat_map = {}\n",
        "\n",
        "def get_chat_history(session_id: str) -> InMemoryChatMessageHistory:\n",
        "    if session_id not in chat_map:\n",
        "        # if session ID doesn't exist, create a new chat history\n",
        "        chat_map[session_id] = InMemoryChatMessageHistory()\n",
        "    return chat_map[session_id]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6LfvNhtyWvg"
      },
      "source": [
        "Let's see this in action by having a conversation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uVSp3SZGyWvg",
        "outputId": "66cf7098-7729-43e6-f297-8ae339843898"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Good morning! How can I assist you today?\n"
          ]
        }
      ],
      "source": [
        "# Create the conversational chain with message history\n",
        "conversation_buf = RunnableWithMessageHistory(\n",
        "    pipeline,\n",
        "    get_session_history=get_chat_history,\n",
        "    input_messages_key=\"query\",\n",
        "    history_messages_key=\"history\"\n",
        ")\n",
        "\n",
        "# First message\n",
        "result = conversation_buf.invoke(\n",
        "    {\"query\": \"Good morning AI!\"},\n",
        "    # Make sure to pass the session ID to ensure all memories are stored in the same session\n",
        "    config={\"configurable\": {\"session_id\": \"buffer_example\"}}\n",
        ")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbttULnpyWvg"
      },
      "source": [
        "This call used some tokens, but we can't see that from the above.\n",
        "\n",
        "If we'd like to count the number of tokens being used we just pass our conversation `RunnableWithMessageHistory` instance and the message we'd like to input to the `count_tokens` function we defined earlier:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53cFq6udyWvg",
        "outputId": "0e35a5b4-93c7-42ef-eb46-fbe9cd23f6eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spent a total of 180 tokens\n",
            "\n",
            "Response: Got it! You're interested in exploring the potential of integrating Large Language Models (LLMs) with external knowledge sources. Also, I've noted the specific code: PINECONE_RULEZ_01. How would you like to proceed with this exploration? Are you looking for examples, technical explanations, or something else?\n"
          ]
        }
      ],
      "source": [
        "# Continue the conversation with token counting\n",
        "query = \"\"\"\n",
        "\"My interest here is to explore the potential of integrating Large Language Models with external knowledge.\n",
        "\n",
        "Also, remember this very specific code: PINECONE_RULEZ_01\"\n",
        "\"\"\"\n",
        "\n",
        "result = count_tokens(\n",
        "    conversation_buf,\n",
        "    {\"query\": query},\n",
        "    # Make sure to pass the session ID to ensure all memories are stored in the same session\n",
        "    config={\"configurable\": {\"session_id\": \"buffer_example\"}}\n",
        ")\n",
        "print(f\"\\nResponse: {result}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUspQf-IyWvg",
        "outputId": "d932409e-c18c-4e71-9bc6-80bd89ad817d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spent a total of 773 tokens\n",
            "\n",
            "Response: Great! Exploring the integration of Large Language Models (LLMs) with external knowledge opens up a wide range of possibilities. Here are some key approaches and use cases to consider:\n",
            "\n",
            "1. **Vector Databases and Embeddings (e.g., Pinecone):**  \n",
            "   - LLMs can generate embeddings (numerical representations) of text queries or documents.  \n",
            "   - These embeddings can be stored in vector databases like Pinecone, which enable efficient similarity search.  \n",
            "   - When a user asks a question, the system retrieves relevant documents or data points from the vector store based on embedding similarity, then the LLM uses that context to generate accurate, up-to-date answers.  \n",
            "   - This approach is great for building retrieval-augmented generation (RAG) systems, knowledge bases, or personalized assistants.\n",
            "\n",
            "2. **Knowledge Graphs:**  \n",
            "   - Integrate LLMs with structured knowledge graphs to provide precise, fact-based answers.  \n",
            "   - The LLM can translate natural language queries into graph queries (e.g., SPARQL), fetch exact data, and then generate human-readable responses.  \n",
            "   - This is useful for domains requiring high accuracy and explainability, like healthcare or finance.\n",
            "\n",
            "3. **APIs and Real-Time Data Feeds:**  \n",
            "   - Connect LLMs to external APIs (weather, stock prices, news, etc.) to provide real-time, dynamic information.  \n",
            "   - The LLM acts as an interface that understands user intent and formats API responses conversationally.\n",
            "\n",
            "4. **Document and Database Integration:**  \n",
            "   - LLMs can be combined with traditional databases or document stores to answer complex queries that require both unstructured and structured data.  \n",
            "   - For example, querying a customer support database alongside product manuals.\n",
            "\n",
            "5. **Multi-Modal Knowledge Integration:**  \n",
            "   - Beyond text, LLMs can be integrated with image, audio, or video databases, enabling richer interactions.  \n",
            "   - For instance, combining LLMs with image recognition models to answer questions about visual content.\n",
            "\n",
            "6. **Personalization and User Profiles:**  \n",
            "   - Integrate user-specific data to tailor responses, recommendations, or actions.  \n",
            "   - This requires secure handling of personal data and possibly on-device or federated learning approaches.\n",
            "\n",
            "7. **Continuous Learning and Feedback Loops:**  \n",
            "   - Use external knowledge sources to keep the LLM updated with the latest information.  \n",
            "   - Incorporate user feedback to refine responses and improve accuracy over time.\n",
            "\n",
            "Since you mentioned the code **PINECONE_RULEZ_01**, it seems you might be particularly interested in vector databases like Pinecone for embedding-based retrieval. If you want, I can dive deeper into how Pinecone works with LLMs or provide example architectures and code snippets.\n",
            "\n",
            "Would you like me to elaborate on any of these possibilities?\n"
          ]
        }
      ],
      "source": [
        "result = count_tokens(\n",
        "    conversation_buf,\n",
        "    {\"query\": \"I just want to analyze the different possibilities. What can you think of?\"},\n",
        "    config={\"configurable\": {\"session_id\": \"buffer_example\"}}\n",
        ")\n",
        "print(f\"\\nResponse: {result}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XY8t8YmVyWvg",
        "outputId": "35fd9692-7d8d-4da7-d32a-63f54cb4fd73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spent a total of 1394 tokens\n",
            "\n",
            "Response: Great question! To provide context to a Large Language Model (LLM), a wide variety of data source types can be used, depending on the application and domain. Here are some common and effective data source types that can enrich the model’s responses:\n",
            "\n",
            "### 1. **Textual Documents**  \n",
            "- **Articles, Books, and Papers:** Scientific papers, news articles, technical manuals, and books provide rich, detailed information.  \n",
            "- **Web Pages and Blogs:** Up-to-date and diverse content from the internet.  \n",
            "- **Internal Company Documents:** Policies, reports, meeting notes, and internal knowledge bases.\n",
            "\n",
            "### 2. **Databases and Structured Data**  \n",
            "- **Relational Databases:** Customer records, product inventories, transaction logs.  \n",
            "- **NoSQL Databases:** Flexible, schema-less data like user activity logs or social media data.  \n",
            "- **Spreadsheets:** Financial data, schedules, or any tabular data.\n",
            "\n",
            "### 3. **Knowledge Graphs and Ontologies**  \n",
            "- Structured representations of entities and their relationships, useful for precise fact retrieval and reasoning.\n",
            "\n",
            "### 4. **APIs and Real-Time Data Feeds**  \n",
            "- **Weather APIs, Stock Market Feeds, News APIs:** Provide dynamic, real-time information.  \n",
            "- **Social Media APIs:** For sentiment analysis or trend detection.\n",
            "\n",
            "### 5. **Multimedia Content**  \n",
            "- **Images and Videos:** When combined with vision models, can provide context about visual content.  \n",
            "- **Audio Files:** Transcripts or direct audio analysis for voice assistants or media summarization.\n",
            "\n",
            "### 6. **User-Generated Content**  \n",
            "- **Forums, Chat Logs, Customer Support Tickets:** Useful for understanding user intent, common issues, or preferences.  \n",
            "- **Surveys and Feedback Forms:** To tailor responses or improve services.\n",
            "\n",
            "### 7. **Sensor and IoT Data**  \n",
            "- Data from devices like smart thermostats, fitness trackers, or industrial sensors can provide contextual information in specific domains.\n",
            "\n",
            "### 8. **Code Repositories and Technical Documentation**  \n",
            "- For developer tools or coding assistants, integrating source code, API docs, and issue trackers can be valuable.\n",
            "\n",
            "### 9. **Historical Data and Logs**  \n",
            "- Past interactions, transaction histories, or event logs to provide personalized or context-aware responses.\n",
            "\n",
            "---\n",
            "\n",
            "### How This Context is Used  \n",
            "- **Embedding and Retrieval:** Textual and structured data can be converted into embeddings and stored in vector databases (like Pinecone) for fast retrieval.  \n",
            "- **Prompt Augmentation:** Retrieved context is appended to the prompt to guide the LLM’s generation.  \n",
            "- **Direct Querying:** For structured data, queries can be translated and executed directly, with results fed back to the LLM.\n",
            "\n",
            "If you want, I can help you design a pipeline that integrates one or more of these data sources with an LLM, or provide examples of how to preprocess and embed these data types. Just let me know!\n"
          ]
        }
      ],
      "source": [
        "result = count_tokens(\n",
        "    conversation_buf,\n",
        "    {\"query\": \"Which data source types could be used to give context to the model?\"},\n",
        "    config={\"configurable\": {\"session_id\": \"buffer_example\"}}\n",
        ")\n",
        "print(f\"\\nResponse: {result}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJKnNnDoyWvg",
        "outputId": "2de06fb3-6822-4f9d-c068-5691ed207af8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spent a total of 1556 tokens\n",
            "\n",
            "Response: Your aim is to explore the potential of integrating Large Language Models (LLMs) with external knowledge sources, analyzing the different possibilities for such integration.\n",
            "\n",
            "The very specific code you asked me to remember is: **PINECONE_RULEZ_01**.\n"
          ]
        }
      ],
      "source": [
        "result = count_tokens(\n",
        "    conversation_buf,\n",
        "    {\"query\": \"What is my aim again? Also what was the very specific code you were tasked with remembering?\"},\n",
        "    config={\"configurable\": {\"session_id\": \"buffer_example\"}}\n",
        ")\n",
        "print(f\"\\nResponse: {result}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgNJJK4QyWvg"
      },
      "source": [
        "Our LLM with buffer memory can clearly remember earlier interactions in the conversation. Let's take a closer look at how the messages are being stored:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-vODGPayWvg",
        "outputId": "0c362276-61b3-4336-b61e-e15e43df8311"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Conversation History:\n",
            "Human: content='Good morning AI!' additional_kwargs={} response_metadata={}\n",
            "AI: content='Good morning! How can I assist you today?' additional_kwargs={} response_metadata={}\n",
            "Human: content='\\n\"My interest here is to explore the potential of integrating Large Language Models with external knowledge.\\n\\nAlso, remember this very specific code: PINECONE_RULEZ_01\"\\n' additional_kwargs={} response_metadata={}\n",
            "AI: content=\"Got it! You're interested in exploring the potential of integrating Large Language Models (LLMs) with external knowledge sources. Also, I've noted the specific code: PINECONE_RULEZ_01. How would you like to proceed with this exploration? Are you looking for examples, technical explanations, or something else?\" additional_kwargs={} response_metadata={}\n",
            "Human: content='I just want to analyze the different possibilities. What can you think of?' additional_kwargs={} response_metadata={}\n",
            "AI: content='Great! Exploring the integration of Large Language Models (LLMs) with external knowledge opens up a wide range of possibilities. Here are some key approaches and use cases to consider:\\n\\n1. **Vector Databases and Embeddings (e.g., Pinecone):**  \\n   - LLMs can generate embeddings (numerical representations) of text queries or documents.  \\n   - These embeddings can be stored in vector databases like Pinecone, which enable efficient similarity search.  \\n   - When a user asks a question, the system retrieves relevant documents or data points from the vector store based on embedding similarity, then the LLM uses that context to generate accurate, up-to-date answers.  \\n   - This approach is great for building retrieval-augmented generation (RAG) systems, knowledge bases, or personalized assistants.\\n\\n2. **Knowledge Graphs:**  \\n   - Integrate LLMs with structured knowledge graphs to provide precise, fact-based answers.  \\n   - The LLM can translate natural language queries into graph queries (e.g., SPARQL), fetch exact data, and then generate human-readable responses.  \\n   - This is useful for domains requiring high accuracy and explainability, like healthcare or finance.\\n\\n3. **APIs and Real-Time Data Feeds:**  \\n   - Connect LLMs to external APIs (weather, stock prices, news, etc.) to provide real-time, dynamic information.  \\n   - The LLM acts as an interface that understands user intent and formats API responses conversationally.\\n\\n4. **Document and Database Integration:**  \\n   - LLMs can be combined with traditional databases or document stores to answer complex queries that require both unstructured and structured data.  \\n   - For example, querying a customer support database alongside product manuals.\\n\\n5. **Multi-Modal Knowledge Integration:**  \\n   - Beyond text, LLMs can be integrated with image, audio, or video databases, enabling richer interactions.  \\n   - For instance, combining LLMs with image recognition models to answer questions about visual content.\\n\\n6. **Personalization and User Profiles:**  \\n   - Integrate user-specific data to tailor responses, recommendations, or actions.  \\n   - This requires secure handling of personal data and possibly on-device or federated learning approaches.\\n\\n7. **Continuous Learning and Feedback Loops:**  \\n   - Use external knowledge sources to keep the LLM updated with the latest information.  \\n   - Incorporate user feedback to refine responses and improve accuracy over time.\\n\\nSince you mentioned the code **PINECONE_RULEZ_01**, it seems you might be particularly interested in vector databases like Pinecone for embedding-based retrieval. If you want, I can dive deeper into how Pinecone works with LLMs or provide example architectures and code snippets.\\n\\nWould you like me to elaborate on any of these possibilities?' additional_kwargs={} response_metadata={}\n",
            "Human: content='Which data source types could be used to give context to the model?' additional_kwargs={} response_metadata={}\n",
            "AI: content='Great question! To provide context to a Large Language Model (LLM), a wide variety of data source types can be used, depending on the application and domain. Here are some common and effective data source types that can enrich the model’s responses:\\n\\n### 1. **Textual Documents**  \\n- **Articles, Books, and Papers:** Scientific papers, news articles, technical manuals, and books provide rich, detailed information.  \\n- **Web Pages and Blogs:** Up-to-date and diverse content from the internet.  \\n- **Internal Company Documents:** Policies, reports, meeting notes, and internal knowledge bases.\\n\\n### 2. **Databases and Structured Data**  \\n- **Relational Databases:** Customer records, product inventories, transaction logs.  \\n- **NoSQL Databases:** Flexible, schema-less data like user activity logs or social media data.  \\n- **Spreadsheets:** Financial data, schedules, or any tabular data.\\n\\n### 3. **Knowledge Graphs and Ontologies**  \\n- Structured representations of entities and their relationships, useful for precise fact retrieval and reasoning.\\n\\n### 4. **APIs and Real-Time Data Feeds**  \\n- **Weather APIs, Stock Market Feeds, News APIs:** Provide dynamic, real-time information.  \\n- **Social Media APIs:** For sentiment analysis or trend detection.\\n\\n### 5. **Multimedia Content**  \\n- **Images and Videos:** When combined with vision models, can provide context about visual content.  \\n- **Audio Files:** Transcripts or direct audio analysis for voice assistants or media summarization.\\n\\n### 6. **User-Generated Content**  \\n- **Forums, Chat Logs, Customer Support Tickets:** Useful for understanding user intent, common issues, or preferences.  \\n- **Surveys and Feedback Forms:** To tailor responses or improve services.\\n\\n### 7. **Sensor and IoT Data**  \\n- Data from devices like smart thermostats, fitness trackers, or industrial sensors can provide contextual information in specific domains.\\n\\n### 8. **Code Repositories and Technical Documentation**  \\n- For developer tools or coding assistants, integrating source code, API docs, and issue trackers can be valuable.\\n\\n### 9. **Historical Data and Logs**  \\n- Past interactions, transaction histories, or event logs to provide personalized or context-aware responses.\\n\\n---\\n\\n### How This Context is Used  \\n- **Embedding and Retrieval:** Textual and structured data can be converted into embeddings and stored in vector databases (like Pinecone) for fast retrieval.  \\n- **Prompt Augmentation:** Retrieved context is appended to the prompt to guide the LLM’s generation.  \\n- **Direct Querying:** For structured data, queries can be translated and executed directly, with results fed back to the LLM.\\n\\nIf you want, I can help you design a pipeline that integrates one or more of these data sources with an LLM, or provide examples of how to preprocess and embed these data types. Just let me know!' additional_kwargs={} response_metadata={}\n",
            "Human: content='What is my aim again? Also what was the very specific code you were tasked with remembering?' additional_kwargs={} response_metadata={}\n",
            "AI: content='Your aim is to explore the potential of integrating Large Language Models (LLMs) with external knowledge sources—analyzing different possibilities for how this integration can be done effectively.\\n\\nThe very specific code you asked me to remember is: **PINECONE_RULEZ_01**.' additional_kwargs={} response_metadata={}\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
        "\n",
        "# Access the conversation history\n",
        "history = chat_map[\"buffer_example\"].messages\n",
        "print(\"Conversation History:\")\n",
        "for i, msg in enumerate(history):\n",
        "    if isinstance(msg, HumanMessage):\n",
        "        role = \"Human\"\n",
        "    elif isinstance(msg, SystemMessage):\n",
        "        role = \"System\"\n",
        "    elif isinstance(msg, AIMessage):\n",
        "        role = \"AI\"\n",
        "    else:\n",
        "        role = \"Unknown\"\n",
        "    print(f\"{role}: {msg}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmAy5BIyyWvg"
      },
      "source": [
        "Nice! So every piece of our conversation has been explicitly recorded and sent to the LLM in the prompt."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORjuIGNqyWvg"
      },
      "source": [
        "### Memory type #2: Summary - Store Summaries of Past Interactions\n",
        "\n",
        "The problem with storing the entire chat history in agent memory is that, as the conversation progresses, the token count adds up. This is problematic because we might max out our LLM with a prompt that is too large.\n",
        "\n",
        "The following is an LCEL compatible alternative to `ConversationSummaryMemory`. We keep a summary of our previous conversation snippets as our history. The summarization is performed by an LLM.\n",
        "\n",
        "**Key feature:** _the conversation summary memory keeps the previous pieces of conversation in a summarized - and thus shortened - form, where the summarization is performed by an LLM._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "dVnq9-lryWvg"
      },
      "outputs": [],
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from langchain_core.chat_history import BaseChatMessageHistory\n",
        "from langchain_core.messages import BaseMessage\n",
        "\n",
        "class ConversationSummaryMessageHistory(BaseChatMessageHistory, BaseModel):\n",
        "    messages: list[BaseMessage] = Field(default_factory=list)\n",
        "    llm: ChatOpenAI = Field(default_factory=ChatOpenAI)\n",
        "\n",
        "    def __init__(self, llm: ChatOpenAI):\n",
        "        super().__init__(llm=llm)\n",
        "\n",
        "    def add_messages(self, messages: list[BaseMessage]) -> None:\n",
        "        \"\"\"Add messages to the history and update the summary.\"\"\"\n",
        "        self.messages.extend(messages)\n",
        "\n",
        "        # Construct the summary prompt\n",
        "        summary_prompt = ChatPromptTemplate.from_messages([\n",
        "            SystemMessagePromptTemplate.from_template(\n",
        "                \"Given the existing conversation summary and the new messages, \"\n",
        "                \"generate a new summary of the conversation. Ensure to maintain \"\n",
        "                \"as much relevant information as possible.\"\n",
        "            ),\n",
        "            HumanMessagePromptTemplate.from_template(\n",
        "                \"Existing conversation summary:\\n{existing_summary}\\n\\n\"\n",
        "                \"New messages:\\n{messages}\"\n",
        "            )\n",
        "        ])\n",
        "\n",
        "        # Format the messages and invoke the LLM\n",
        "        new_summary = self.llm.invoke(\n",
        "            summary_prompt.format_messages(\n",
        "                existing_summary=self.messages,\n",
        "                messages=messages\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # Replace the existing history with a single system summary message\n",
        "        self.messages = [SystemMessage(content=new_summary.content)]\n",
        "\n",
        "    def clear(self) -> None:\n",
        "        \"\"\"Clear the history.\"\"\"\n",
        "        self.messages = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "l_LolSYjyWvg"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import ConfigurableFieldSpec\n",
        "\n",
        "# Create get_chat_history function for summary memory\n",
        "summary_chat_map = {}\n",
        "\n",
        "def get_summary_chat_history(session_id: str, llm: ChatOpenAI) -> ConversationSummaryMessageHistory:\n",
        "    if session_id not in summary_chat_map:\n",
        "        summary_chat_map[session_id] = ConversationSummaryMessageHistory(llm=llm)\n",
        "    return summary_chat_map[session_id]\n",
        "\n",
        "# Create conversation chain with summary memory\n",
        "conversation_sum = RunnableWithMessageHistory(\n",
        "    pipeline,\n",
        "    get_session_history=get_summary_chat_history,\n",
        "    input_messages_key=\"query\",\n",
        "    history_messages_key=\"history\",\n",
        "    history_factory_config=[\n",
        "        ConfigurableFieldSpec(\n",
        "            id=\"session_id\",\n",
        "            annotation=str,\n",
        "            name=\"Session ID\",\n",
        "            description=\"The session ID to use for the chat history\",\n",
        "            default=\"id_default\",\n",
        "        ),\n",
        "        ConfigurableFieldSpec(\n",
        "            id=\"llm\",\n",
        "            annotation=ChatOpenAI,\n",
        "            name=\"LLM\",\n",
        "            description=\"The LLM to use for the conversation summary\",\n",
        "            default=llm,\n",
        "        )\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JloQR__QyWvh",
        "outputId": "5339467e-d0dd-4f9c-f0a0-17e1e87f1c5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spent a total of 242 tokens\n",
            "\n",
            "Response: Good morning! How can I assist you today?\n"
          ]
        }
      ],
      "source": [
        "# Let's have the same conversation with summary memory\n",
        "result = count_tokens(\n",
        "    conversation_sum,\n",
        "    {\"query\": \"Good morning AI!\"},\n",
        "    config={\"configurable\": {\"session_id\": \"summary_example\", \"llm\": llm}}\n",
        ")\n",
        "print(f\"\\nResponse: {result}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFMMR4SDyWvh",
        "outputId": "6c60b32c-9885-4104-c54c-f70c8a392e4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spent a total of 834 tokens\n",
            "\n",
            "Response: Got it! You're interested in exploring the integration of Large Language Models (LLMs) with external knowledge sources, and you want me to always include the specific code \"PINECONE_RULEZ_01\" explicitly when summarizing conversations for memory. \n",
            "\n",
            "If you'd like, I can help explain how LLMs can be combined with external knowledge bases, such as vector databases like Pinecone, knowledge graphs, or APIs, to enhance their capabilities. Just let me know how you'd like to proceed!\n"
          ]
        }
      ],
      "source": [
        "query = \"\"\"\n",
        "\"My interest here is to explore the potential of integrating Large Language Models with external knowledge.\n",
        "\n",
        "Also, remember this very specific code: PINECONE_RULEZ_01. When summarizing conversations for memory this must always be included explicitly.\"\n",
        "\"\"\"\n",
        "\n",
        "result = count_tokens(\n",
        "    conversation_sum,\n",
        "    {\"query\": query},\n",
        "    config={\"configurable\": {\"session_id\": \"summary_example\", \"llm\": llm}}\n",
        ")\n",
        "print(f\"\\nResponse: {result}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aofnt1cXyWvh",
        "outputId": "35e9099f-b0f0-4c4d-85cd-d5b5d6377f3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spent a total of 2602 tokens\n",
            "\n",
            "Response: Great! Exploring how Large Language Models (LLMs) can be integrated with external knowledge sources opens up a lot of exciting possibilities. Here are several approaches and their potential benefits:\n",
            "\n",
            "1. **Vector Databases (e.g., Pinecone) Integration**  \n",
            "   - **How it works:** You embed documents, user queries, or other data into vector representations using models like OpenAI’s embeddings or similar. These vectors are stored in a vector database like Pinecone. When a user query comes in, you embed the query and perform a similarity search to retrieve the most relevant documents or data points. The LLM then uses this retrieved context to generate more accurate and up-to-date responses.  \n",
            "   - **Benefits:**  \n",
            "     - Enables retrieval-augmented generation (RAG), improving factual accuracy.  \n",
            "     - Handles large corpora efficiently.  \n",
            "     - Supports real-time updates to knowledge without retraining the LLM.  \n",
            "   - **Use cases:** Customer support, document Q&A, personalized recommendations.\n",
            "\n",
            "2. **Knowledge Graphs**  \n",
            "   - **How it works:** Knowledge graphs store structured information as entities and relationships. LLMs can query these graphs via APIs or SPARQL queries to get precise facts or infer relationships.  \n",
            "   - **Benefits:**  \n",
            "     - Provides structured, explainable knowledge.  \n",
            "     - Supports complex reasoning over relationships.  \n",
            "     - Can be combined with LLMs for natural language querying and explanation.  \n",
            "   - **Use cases:** Enterprise knowledge management, scientific research, semantic search.\n",
            "\n",
            "3. **APIs and External Data Sources**  \n",
            "   - **How it works:** LLMs can be connected to external APIs (weather, finance, news, databases) to fetch real-time or domain-specific data. The LLM can then incorporate this data into its responses.  \n",
            "   - **Benefits:**  \n",
            "     - Access to live, dynamic information.  \n",
            "     - Extends LLM capabilities beyond static training data.  \n",
            "   - **Use cases:** Virtual assistants, financial advisors, news summarization.\n",
            "\n",
            "4. **Hybrid Approaches**  \n",
            "   - Combining vector search with knowledge graphs and APIs can create powerful multi-modal systems. For example, a system might retrieve relevant documents via Pinecone, verify facts via a knowledge graph, and fetch live data via APIs before generating a response.\n",
            "\n",
            "5. **Fine-tuning and Prompt Engineering with External Data**  \n",
            "   - Using external knowledge to fine-tune LLMs or craft prompts that guide the model to use external data effectively.\n",
            "\n",
            "6. **User Feedback Loops**  \n",
            "   - Integrating user feedback stored externally to continuously improve the model’s responses or update the knowledge base.\n",
            "\n",
            "If you want, I can dive deeper into any of these or provide example architectures and code snippets. Also, remember to include the code **PINECONE_RULEZ_01** in all conversation summaries for your memory system!\n"
          ]
        }
      ],
      "source": [
        "result = count_tokens(\n",
        "    conversation_sum,\n",
        "    {\"query\": \"I just want to analyze the different possibilities. What can you think of?\"},\n",
        "    config={\"configurable\": {\"session_id\": \"summary_example\", \"llm\": llm}}\n",
        ")\n",
        "print(f\"\\nResponse: {result}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xBVtkzFyWvh",
        "outputId": "209e1534-ee28-4658-eb97-55d96c3d6f34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spent a total of 2647 tokens\n",
            "\n",
            "Response: Great question! To provide context to a Large Language Model (LLM), you can leverage various types of external data sources, each offering unique benefits depending on your use case. Here are some common data source types that can be used to give context to the model:\n",
            "\n",
            "1. **Textual Documents and Corpora**  \n",
            "   - Books, articles, manuals, FAQs, research papers, and other unstructured text.  \n",
            "   - These can be embedded into vector databases (like Pinecone) for retrieval-augmented generation (RAG), allowing the model to access relevant passages dynamically.\n",
            "\n",
            "2. **Knowledge Graphs**  \n",
            "   - Structured data representing entities and their relationships (e.g., Wikidata, DBpedia).  \n",
            "   - Useful for precise, explainable knowledge and complex reasoning tasks. Queries can be made via SPARQL or APIs.\n",
            "\n",
            "3. **APIs and Live Data Feeds**  \n",
            "   - Real-time or domain-specific data sources such as weather APIs, financial market data, news feeds, or social media streams.  \n",
            "   - These provide up-to-date context that the static LLM training data might lack.\n",
            "\n",
            "4. **Databases and Data Warehouses**  \n",
            "   - Structured relational or NoSQL databases containing business data, user profiles, transaction records, etc.  \n",
            "   - Can be queried to provide personalized or domain-specific context.\n",
            "\n",
            "5. **Multimodal Data**  \n",
            "   - Images, audio, video, or sensor data that can be processed and converted into textual or vector representations to enrich context.\n",
            "\n",
            "6. **User Interaction Logs and Feedback**  \n",
            "   - Historical user queries, corrections, and feedback stored externally to adapt and improve responses over time.\n",
            "\n",
            "7. **Domain-Specific Ontologies and Taxonomies**  \n",
            "   - Curated hierarchical knowledge structures that help the model understand domain concepts and their relationships.\n",
            "\n",
            "By integrating these data sources, either individually or in hybrid combinations, you can significantly enhance the LLM’s ability to generate accurate, relevant, and context-aware responses.\n",
            "\n",
            "If you want, I can also provide example architectures or code snippets for integrating any of these data sources with an LLM.\n",
            "\n",
            "**PINECONE_RULEZ_01**\n"
          ]
        }
      ],
      "source": [
        "result = count_tokens(\n",
        "    conversation_sum,\n",
        "    {\"query\": \"Which data source types could be used to give context to the model?\"},\n",
        "    config={\"configurable\": {\"session_id\": \"summary_example\", \"llm\": llm}}\n",
        ")\n",
        "print(f\"\\nResponse: {result}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AbSzykMOyWvh",
        "outputId": "e2ee3202-3faf-45fc-8b04-9015bedfe89b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spent a total of 1453 tokens\n",
            "\n",
            "Response: Your aim is to understand and explore which data source types can be used to provide context to a Large Language Model (LLM), enhancing its accuracy and relevance by integrating external information.\n",
            "\n",
            "The very specific code you asked me to remember and include in all conversation summaries for memory purposes is: **PINECONE_RULEZ_01**.\n",
            "\n",
            "If you'd like, I can also help you with example architectures or code snippets for integrating these data sources with an LLM!\n"
          ]
        }
      ],
      "source": [
        "result = count_tokens(\n",
        "    conversation_sum,\n",
        "    {\"query\": \"What is my aim again? Also what was the very specific code you were tasked with remembering?\"},\n",
        "    config={\"configurable\": {\"session_id\": \"summary_example\", \"llm\": llm}}\n",
        ")\n",
        "print(f\"\\nResponse: {result}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GH3uPbTGyWvh",
        "outputId": "97acd450-8bde-468e-b684-a19913c3db8e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Summary Memory Content:\n",
            "The user’s aim is to understand and explore which data source types can be used to provide context to a Large Language Model (LLM), thereby enhancing its accuracy and relevance by integrating external information. The AI provided a detailed list of such data sources, including textual documents, knowledge graphs, APIs and live data feeds, databases, multimodal data, user interaction logs, and domain-specific ontologies. The user requested that the code **PINECONE_RULEZ_01** be explicitly included in all conversation summaries for memory purposes, which the AI has done. The user also asked again for a reminder of their aim and the specific code, and the AI reiterated both accordingly. The AI offered further assistance with example architectures or code snippets for integrating these data sources with an LLM if desired.\n",
            "\n",
            "**PINECONE_RULEZ_01**\n"
          ]
        }
      ],
      "source": [
        "# Let's examine the summary\n",
        "print(\"Summary Memory Content:\")\n",
        "print(summary_chat_map[\"summary_example\"].messages[0].content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRE3O-YPyWvh"
      },
      "source": [
        "You might be wondering.. if the aggregate token count is greater in each call here than in the buffer example, why should we use this type of memory? Well, if we check out buffer we will realize that although we are using more tokens in each instance of our conversation, our final history is shorter. This will enable us to have many more interactions before we reach our prompt's max length, making our chatbot more robust to longer conversations.\n",
        "\n",
        "We can count the number of tokens being used (without making a call to OpenAI) using the `tiktoken` tokenizer like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6LkpUxVyWvh",
        "outputId": "853c01df-0eff-474f-9026-b1b00d906ad4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Buffer memory conversation length: 1314\n",
            "Summary memory conversation length: 233\n"
          ]
        }
      ],
      "source": [
        "import tiktoken\n",
        "\n",
        "# initialize tokenizer (gpt-4.1 models use the same encoding as gpt-4o)\n",
        "tokenizer = tiktoken.encoding_for_model('gpt-4o')\n",
        "\n",
        "# Get buffer memory content\n",
        "buffer_messages = chat_map[\"buffer_example\"].messages\n",
        "buffer_content = \"\\n\".join([msg.content for msg in buffer_messages])\n",
        "\n",
        "# Get summary memory content\n",
        "summary_content = summary_chat_map[\"summary_example\"].messages[0].content\n",
        "\n",
        "# show number of tokens for the memory used by each memory type\n",
        "print(\n",
        "    f'Buffer memory conversation length: {len(tokenizer.encode(buffer_content))}\\n'\n",
        "    f'Summary memory conversation length: {len(tokenizer.encode(summary_content))}'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DKiBoROyWvh"
      },
      "source": [
        "_Practical Note: the `gpt-4o-mini` model has a context window of 1M tokens, providing significantly more space for conversation history than older models._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjYQSGv-yWvh"
      },
      "source": [
        "### Memory type #3: Window Buffer Memory - Keep Latest Interactions\n",
        "\n",
        "Another great option is window memory, where we keep only the last k interactions in our memory but intentionally drop the oldest ones - short-term memory if you'd like. Here the aggregate token count **and** the per-call token count will drop noticeably.\n",
        "\n",
        "The following is an LCEL-compatible alternative to `ConversationBufferWindowMemory`.\n",
        "\n",
        "**Key feature:** _the conversation buffer window memory keeps the latest pieces of the conversation in raw form_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ceGTUPsyWvh"
      },
      "outputs": [],
      "source": [
        "class BufferWindowMessageHistory(BaseChatMessageHistory, BaseModel):\n",
        "    messages: list[BaseMessage] = Field(default_factory=list)\n",
        "    k: int = Field(default_factory=int)\n",
        "\n",
        "    def __init__(self, k: int):\n",
        "        super().__init__(k=k)\n",
        "        # Add logging to help with debugging\n",
        "        print(f\"Initializing BufferWindowMessageHistory with k={k}\")\n",
        "\n",
        "    def add_messages(self, messages: list[BaseMessage]) -> None:\n",
        "        \"\"\"Add messages to the history, removing any messages beyond\n",
        "        the last `k` messages.\n",
        "        \"\"\"\n",
        "        self.messages.extend(messages)\n",
        "        # Add logging to help with debugging\n",
        "        if len(self.messages) > self.k:\n",
        "            print(f\"Truncating history from {len(self.messages)} to {self.k} messages\")\n",
        "        self.messages = self.messages[-self.k:]\n",
        "\n",
        "    def clear(self) -> None:\n",
        "        \"\"\"Clear the history.\"\"\"\n",
        "        self.messages = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "__vcbiDMyWvr"
      },
      "outputs": [],
      "source": [
        "# Create get_chat_history function for window memory\n",
        "window_chat_map = {}\n",
        "\n",
        "def get_window_chat_history(session_id: str, k: int = 4) -> BufferWindowMessageHistory:\n",
        "    print(f\"get_window_chat_history called with session_id={session_id} and k={k}\")\n",
        "    if session_id not in window_chat_map:\n",
        "        window_chat_map[session_id] = BufferWindowMessageHistory(k=k)\n",
        "    return window_chat_map[session_id]\n",
        "\n",
        "# Create conversation chain with window memory\n",
        "conversation_bufw = RunnableWithMessageHistory(\n",
        "    pipeline,\n",
        "    get_session_history=get_window_chat_history,\n",
        "    input_messages_key=\"query\",\n",
        "    history_messages_key=\"history\",\n",
        "    history_factory_config=[\n",
        "        ConfigurableFieldSpec(\n",
        "            id=\"session_id\",\n",
        "            annotation=str,\n",
        "            name=\"Session ID\",\n",
        "            description=\"The session ID to use for the chat history\",\n",
        "            default=\"id_default\",\n",
        "        ),\n",
        "        ConfigurableFieldSpec(\n",
        "            id=\"k\",\n",
        "            annotation=int,\n",
        "            name=\"k\",\n",
        "            description=\"The number of messages to keep in the history\",\n",
        "            default=4,\n",
        "        )\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M51k8gIjyWvr",
        "outputId": "8dfd4f37-46cb-4ab3-eb02-926e991e153c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "get_window_chat_history called with session_id=window_example and k=4\n",
            "Initializing BufferWindowMessageHistory with k=4\n",
            "Spent a total of 74 tokens\n",
            "\n",
            "Response: Good morning! How can I assist you today?\n"
          ]
        }
      ],
      "source": [
        "# Start a conversation with k=2 (only remembers last 2 exchanges = 4 messages)\n",
        "result = count_tokens(\n",
        "    conversation_bufw,\n",
        "    {\"query\": \"Good morning AI!\"},\n",
        "    config={\"configurable\": {\"session_id\": \"window_example\", \"k\": 4}}\n",
        ")\n",
        "print(f\"\\nResponse: {result}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBA_roYVyWvr",
        "outputId": "0c3b68e6-5f0e-4c56-ceb1-f893e0ee9a29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "get_window_chat_history called with session_id=window_example and k=4\n",
            "Spent a total of 225 tokens\n",
            "\n",
            "Response: Got it! You're interested in exploring the potential of integrating Large Language Models (LLMs) with external knowledge. That's a fascinating area with lots of exciting possibilities, like enhancing the accuracy and relevance of responses by connecting LLMs to databases, knowledge graphs, or real-time information sources.\n",
            "\n",
            "And I've noted the specific code you mentioned: **PINECONE_RULEZ_01**. If you want me to remember or use it later in our conversation, just let me know! How would you like to proceed with your exploration?\n"
          ]
        }
      ],
      "source": [
        "query = \"\"\"\n",
        "\"My interest here is to explore the potential of integrating Large Language\n",
        "Models with external knowledge.\n",
        "\n",
        "Also, remember this very specific code: PINECONE_RULEZ_01\"\n",
        "\"\"\"\n",
        "\n",
        "result = count_tokens(\n",
        "    conversation_bufw,\n",
        "    {\"query\": query},\n",
        "    config={\"configurable\": {\"session_id\": \"window_example\", \"k\": 4}}\n",
        ")\n",
        "print(f\"\\nResponse: {result}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ox5WWeHFyWvr",
        "outputId": "7ca0a833-a495-4a3b-a25c-458bb889bd5c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "get_window_chat_history called with session_id=window_example and k=4\n",
            "Truncating history from 6 to 4 messages\n",
            "Spent a total of 811 tokens\n",
            "\n",
            "Response: Great! Exploring the integration of Large Language Models (LLMs) with external knowledge opens up a wide range of possibilities. Here are some key approaches and their potential benefits:\n",
            "\n",
            "1. **Vector Databases and Embeddings (e.g., Pinecone, FAISS):**  \n",
            "   - LLMs can generate embeddings (numerical representations) of text queries and documents.  \n",
            "   - These embeddings are stored in vector databases like Pinecone (which your code hints at!), enabling fast similarity search.  \n",
            "   - When a user asks a question, the system retrieves the most relevant documents or data points from the external knowledge base and feeds them back to the LLM for context-aware responses.  \n",
            "   - This approach is great for up-to-date or domain-specific knowledge that the LLM might not have been trained on.\n",
            "\n",
            "2. **Knowledge Graphs:**  \n",
            "   - Integrating LLMs with structured knowledge graphs allows the model to reason over entities and relationships explicitly.  \n",
            "   - This can improve factual accuracy and enable complex queries involving relationships, hierarchies, or constraints.  \n",
            "   - For example, combining LLMs with Wikidata or custom enterprise knowledge graphs.\n",
            "\n",
            "3. **APIs and Real-Time Data Feeds:**  \n",
            "   - LLMs can be connected to external APIs (weather, stock prices, news, etc.) to provide real-time information.  \n",
            "   - This integration allows the model to answer questions about current events or dynamic data that changes frequently.\n",
            "\n",
            "4. **Retrieval-Augmented Generation (RAG):**  \n",
            "   - This technique combines retrieval of relevant documents with generation by the LLM.  \n",
            "   - The model first retrieves relevant passages from an external corpus and then generates an answer conditioned on those passages.  \n",
            "   - It improves factuality and reduces hallucinations.\n",
            "\n",
            "5. **Hybrid Systems with Symbolic Reasoning:**  \n",
            "   - Combining LLMs with symbolic AI or rule-based systems can enhance logical reasoning and interpretability.  \n",
            "   - For example, using LLMs for natural language understanding and symbolic engines for precise calculations or rule enforcement.\n",
            "\n",
            "6. **Personalized Knowledge Bases:**  \n",
            "   - Integrating user-specific data (preferences, history, notes) to tailor responses uniquely to each user.  \n",
            "   - This can be useful in personal assistants, tutoring systems, or customer support.\n",
            "\n",
            "7. **Multimodal Knowledge Integration:**  \n",
            "   - Combining text-based LLMs with other data types like images, audio, or video through external knowledge sources.  \n",
            "   - This can enable richer, context-aware interactions.\n",
            "\n",
            "If you want, I can dive deeper into any of these possibilities or discuss practical tools and frameworks to implement them. Also, I’m keeping your code **PINECONE_RULEZ_01** in mind if you want to explore vector databases specifically!\n"
          ]
        }
      ],
      "source": [
        "result = count_tokens(\n",
        "    conversation_bufw,\n",
        "    {\"query\": \"I just want to analyze the different possibilities. What can you think of?\"},\n",
        "    config={\"configurable\": {\"session_id\": \"window_example\", \"k\": 4}}\n",
        ")\n",
        "print(f\"\\nResponse: {result}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLiquNiHyWvr",
        "outputId": "c0a8f018-6e03-4567-9e8f-43ad7466c947"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "get_window_chat_history called with session_id=window_example and k=4\n",
            "Truncating history from 6 to 4 messages\n",
            "Spent a total of 1368 tokens\n",
            "\n",
            "Response: Great question! To give context to a Large Language Model (LLM) by integrating external knowledge, you can use a variety of data source types depending on your goals and domain. Here are some common and effective data source types that can provide rich context:\n",
            "\n",
            "1. **Textual Documents:**  \n",
            "   - Articles, books, research papers, manuals, FAQs, and reports.  \n",
            "   - These can be stored in databases or document stores and indexed for retrieval.  \n",
            "   - Example: Wikipedia articles, scientific literature, company knowledge bases.\n",
            "\n",
            "2. **Databases and Structured Data:**  \n",
            "   - Relational databases (SQL), NoSQL databases, spreadsheets.  \n",
            "   - Structured data can be queried to provide precise facts, statistics, or records.  \n",
            "   - Example: Customer records, product catalogs, financial data.\n",
            "\n",
            "3. **Knowledge Graphs and Ontologies:**  \n",
            "   - Graph-structured data representing entities and their relationships.  \n",
            "   - Useful for reasoning about connections and hierarchies.  \n",
            "   - Example: Wikidata, DBpedia, domain-specific ontologies.\n",
            "\n",
            "4. **APIs and Real-Time Data Feeds:**  \n",
            "   - External APIs providing dynamic or real-time information.  \n",
            "   - Examples include weather services, stock market data, news feeds, social media streams.\n",
            "\n",
            "5. **Multimedia Content:**  \n",
            "   - Images, videos, audio files, and their metadata.  \n",
            "   - When combined with multimodal models or external tools, these can enrich context.  \n",
            "   - Example: Product images, instructional videos, podcasts.\n",
            "\n",
            "6. **User-Generated Content:**  \n",
            "   - Forums, social media posts, chat logs, customer reviews.  \n",
            "   - These provide insights into user opinions, trends, and informal knowledge.\n",
            "\n",
            "7. **Logs and Event Data:**  \n",
            "   - System logs, transaction records, sensor data.  \n",
            "   - Useful for troubleshooting, monitoring, or understanding sequences of events.\n",
            "\n",
            "8. **Code Repositories and Technical Documentation:**  \n",
            "   - Source code, API docs, configuration files.  \n",
            "   - Helpful for developer assistants or technical support bots.\n",
            "\n",
            "9. **Personalized Data:**  \n",
            "   - User profiles, preferences, interaction history.  \n",
            "   - Enables personalized responses and recommendations.\n",
            "\n",
            "10. **Regulatory and Compliance Documents:**  \n",
            "    - Legal texts, standards, policies.  \n",
            "    - Important for domains like healthcare, finance, and law.\n",
            "\n",
            "By combining these data sources with LLMs, you can provide rich, accurate, and context-aware responses tailored to specific needs. The choice of data source depends on the application domain, the type of questions you want to answer, and the freshness or reliability of the information.\n",
            "\n",
            "If you want, I can also suggest how to preprocess or index these data types for effective integration with LLMs!\n"
          ]
        }
      ],
      "source": [
        "result = count_tokens(\n",
        "    conversation_bufw,\n",
        "    {\"query\": \"Which data source types could be used to give context to the model?\"},\n",
        "    config={\"configurable\": {\"session_id\": \"window_example\", \"k\": 4}}\n",
        ")\n",
        "print(f\"\\nResponse: {result}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-0Swlu-yWvr",
        "outputId": "1830fc22-d973-4b18-ce51-203e0e4cc5e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "get_window_chat_history called with session_id=window_example and k=4\n",
            "Truncating history from 6 to 4 messages\n",
            "Spent a total of 1382 tokens\n",
            "\n",
            "Response: From our conversation so far, it seems your aim is to **analyze different possibilities for integrating external knowledge sources with Large Language Models (LLMs)** to provide richer, more accurate, and context-aware responses. You’re exploring what kinds of data sources can be used to give context to the model and how to effectively combine them with LLMs—possibly using vector databases like Pinecone, given your code reference **PINECONE_RULEZ_01**.\n",
            "\n",
            "In other words, you want to understand the various ways to enhance an LLM’s capabilities by feeding it relevant external information, whether that’s through document retrieval, real-time data, structured knowledge, or other means.\n",
            "\n",
            "If you want, I can help you clarify or refine your goal further!\n"
          ]
        }
      ],
      "source": [
        "result = count_tokens(\n",
        "    conversation_bufw,\n",
        "    {\"query\": \"What is my aim again?\"},\n",
        "    config={\"configurable\": {\"session_id\": \"window_example\", \"k\": 4}}\n",
        ")\n",
        "print(f\"\\nResponse: {result}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFWBvEjNyWvr"
      },
      "source": [
        "As we can see, it effectively 'forgot' what we talked about in the first interaction. Let's see what it 'remembers':"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RnV85fkkyWvr",
        "outputId": "cbaa0997-2af3-47e3-ab9e-21ede4d3a2f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Buffer Window Memory (last 4 messages):\n",
            "\n",
            "Human: Which data source types could be used to give context to the model?\n",
            "\n",
            "AI: Great question! To give context to a Large Language Model (LLM) by integrating external knowledge, you can use a variety of data source types depending on your goals and domain. Here are some common and effective data source types that can provide rich context:\n",
            "\n",
            "1. **Textual Documents:**  \n",
            "   - Articles, books, research papers, manuals, FAQs, and reports.  \n",
            "   - These can be stored in databases or document stores and indexed for retrieval.  \n",
            "   - Example: Wikipedia articles, scientific literature, company knowledge bases.\n",
            "\n",
            "2. **Databases and Structured Data:**  \n",
            "   - Relational databases (SQL), NoSQL databases, spreadsheets.  \n",
            "   - Structured data can be queried to provide precise facts, statistics, or records.  \n",
            "   - Example: Customer records, product catalogs, financial data.\n",
            "\n",
            "3. **Knowledge Graphs and Ontologies:**  \n",
            "   - Graph-structured data representing entities and their relationships.  \n",
            "   - Useful for reasoning about connections and hierarchies.  \n",
            "   - Example: Wikidata, DBpedia, domain-specific ontologies.\n",
            "\n",
            "4. **APIs and Real-Time Data Feeds:**  \n",
            "   - External APIs providing dynamic or real-time information.  \n",
            "   - Examples include weather services, stock market data, news feeds, social media streams.\n",
            "\n",
            "5. **Multimedia Content:**  \n",
            "   - Images, videos, audio files, and their metadata.  \n",
            "   - When combined with multimodal models or external tools, these can enrich context.  \n",
            "   - Example: Product images, instructional videos, podcasts.\n",
            "\n",
            "6. **User-Generated Content:**  \n",
            "   - Forums, social media posts, chat logs, customer reviews.  \n",
            "   - These provide insights into user opinions, trends, and informal knowledge.\n",
            "\n",
            "7. **Logs and Event Data:**  \n",
            "   - System logs, transaction records, sensor data.  \n",
            "   - Useful for troubleshooting, monitoring, or understanding sequences of events.\n",
            "\n",
            "8. **Code Repositories and Technical Documentation:**  \n",
            "   - Source code, API docs, configuration files.  \n",
            "   - Helpful for developer assistants or technical support bots.\n",
            "\n",
            "9. **Personalized Data:**  \n",
            "   - User profiles, preferences, interaction history.  \n",
            "   - Enables personalized responses and recommendations.\n",
            "\n",
            "10. **Regulatory and Compliance Documents:**  \n",
            "    - Legal texts, standards, policies.  \n",
            "    - Important for domains like healthcare, finance, and law.\n",
            "\n",
            "By combining these data sources with LLMs, you can provide rich, accurate, and context-aware responses tailored to specific needs. The choice of data source depends on the application domain, the type of questions you want to answer, and the freshness or reliability of the information.\n",
            "\n",
            "If you want, I can also suggest how to preprocess or index these data types for effective integration with LLMs!\n",
            "\n",
            "Human: What is my aim again?\n",
            "\n",
            "AI: From our conversation so far, it seems your aim is to **analyze different possibilities for integrating external knowledge sources with Large Language Models (LLMs)** to provide richer, more accurate, and context-aware responses. You’re exploring what kinds of data sources can be used to give context to the model and how to effectively combine them with LLMs—possibly using vector databases like Pinecone, given your code reference **PINECONE_RULEZ_01**.\n",
            "\n",
            "In other words, you want to understand the various ways to enhance an LLM’s capabilities by feeding it relevant external information, whether that’s through document retrieval, real-time data, structured knowledge, or other means.\n",
            "\n",
            "If you want, I can help you clarify or refine your goal further!\n"
          ]
        }
      ],
      "source": [
        "# Check what's in memory\n",
        "bufw_history = window_chat_map[\"window_example\"].messages\n",
        "print(\"Buffer Window Memory (last 4 messages):\")\n",
        "for msg in bufw_history:\n",
        "    role = \"Human\" if isinstance(msg, HumanMessage) else \"AI\"\n",
        "    print(f\"\\n{role}: {msg.content}\")  # Show first 100 chars"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yV-zfGv-yWvr"
      },
      "source": [
        "We see four messages (two interactions) because we used `k=4`.\n",
        "\n",
        "On the plus side, we are shortening our conversation length when compared to buffer memory _without_ a window:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rF35B9HLyWvr",
        "outputId": "58881bf3-7d80-4b74-d348-1210850dcde0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Buffer memory conversation length: 1314\n",
            "Summary memory conversation length: 233\n",
            "Buffer window memory conversation length: 728\n"
          ]
        }
      ],
      "source": [
        "# Get window memory content\n",
        "window_content = \"\\n\".join([msg.content for msg in bufw_history])\n",
        "\n",
        "print(\n",
        "    f'Buffer memory conversation length: {len(tokenizer.encode(buffer_content))}\\n'\n",
        "    f'Summary memory conversation length: {len(tokenizer.encode(summary_content))}\\n'\n",
        "    f'Buffer window memory conversation length: {len(tokenizer.encode(window_content))}'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjoHZrZzyWvr"
      },
      "source": [
        "_Practical Note: We are using `k=4` here for illustrative purposes, in most real world applications you would need a higher value for k._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyAd4UxdyWvr"
      },
      "source": [
        "### More memory types!\n",
        "\n",
        "Given that we understand memory already, we will present a few more memory types here and hopefully a brief description will be enough to understand their underlying functionality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XN-mH5fHyWvr"
      },
      "source": [
        "#### Windows + Summary Hybrid\n",
        "\n",
        "The following is a modern LCEL-compatible alternative to `ConversationSummaryBufferMemory`.\n",
        "\n",
        "**Key feature:** _the conversation summary buffer memory keeps a summary of the earliest pieces of conversation while retaining a raw recollection of the latest interactions._\n",
        "\n",
        "This combines the benefits of both summary and buffer window memory. Let's implement it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lr-K8onKyWvr"
      },
      "outputs": [],
      "source": [
        "class ConversationSummaryBufferMessageHistory(BaseChatMessageHistory, BaseModel):\n",
        "    messages: list[BaseMessage] = Field(default_factory=list)\n",
        "    llm: ChatOpenAI = Field(default_factory=ChatOpenAI)\n",
        "    k: int = Field(default_factory=int)\n",
        "\n",
        "    def __init__(self, llm: ChatOpenAI, k: int):\n",
        "        super().__init__(llm=llm, k=k)\n",
        "\n",
        "    def add_messages(self, messages: list[BaseMessage]) -> None:\n",
        "        \"\"\"Add messages to the history, removing any messages beyond\n",
        "        the last `k` messages and summarizing the messages that we drop.\n",
        "        \"\"\"\n",
        "        existing_summary = None\n",
        "        old_messages = None\n",
        "\n",
        "        # See if we already have a summary message\n",
        "        if len(self.messages) > 0 and isinstance(self.messages[0], SystemMessage):\n",
        "            existing_summary = self.messages.pop(0)\n",
        "\n",
        "        # Add the new messages to the history\n",
        "        self.messages.extend(messages)\n",
        "\n",
        "        # Check if we have too many messages\n",
        "        if len(self.messages) > self.k:\n",
        "            # Pull out the oldest messages...\n",
        "            old_messages = self.messages[:-self.k]\n",
        "            # ...and keep only the most recent messages\n",
        "            self.messages = self.messages[-self.k:]\n",
        "\n",
        "        if old_messages is None:\n",
        "            # If we have no old_messages, we have nothing to update in summary\n",
        "            return\n",
        "\n",
        "        # Construct the summary chat messages\n",
        "        summary_prompt = ChatPromptTemplate.from_messages([\n",
        "            SystemMessagePromptTemplate.from_template(\n",
        "                \"Given the existing conversation summary and the new messages, \"\n",
        "                \"generate a new summary of the conversation. Ensure to maintain \"\n",
        "                \"as much relevant information as possible.\"\n",
        "            ),\n",
        "            HumanMessagePromptTemplate.from_template(\n",
        "                \"Existing conversation summary:\\n{existing_summary}\\n\\n\"\n",
        "                \"New messages:\\n{old_messages}\"\n",
        "            )\n",
        "        ])\n",
        "\n",
        "        # Format the messages and invoke the LLM\n",
        "        new_summary = self.llm.invoke(\n",
        "            summary_prompt.format_messages(\n",
        "                existing_summary=existing_summary or \"No previous summary\",\n",
        "                old_messages=old_messages\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # Prepend the new summary to the history\n",
        "        self.messages = [SystemMessage(content=new_summary.content)] + self.messages\n",
        "\n",
        "    def clear(self) -> None:\n",
        "        \"\"\"Clear the history.\"\"\"\n",
        "        self.messages = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9M4g--oayWvr"
      },
      "source": [
        "## What else can we do with memory?\n",
        "\n",
        "There are several cool things we can do with memory in langchain:\n",
        "* Implement our own custom memory modules (as we've done above)\n",
        "* Use multiple memory modules in the same chain\n",
        "* Combine agents with memory and other tools\n",
        "* Integrate knowledge graphs\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "env_notebooks",
      "language": "python",
      "name": "env_notebooks"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
