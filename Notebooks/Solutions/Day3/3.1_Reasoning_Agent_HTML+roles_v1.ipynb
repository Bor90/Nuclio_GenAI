{"cells":[{"cell_type":"markdown","metadata":{"id":"header"},"source":["# Reasoning Agent: HTML Code Generator with Self-Improvement\n","\n","This notebook demonstrates a **reasoning agent** that:\n","1. Generates HTML code based on a user prompt\n","2. Evaluates and improves the code iteratively\n","\n","We'll implement two versions:\n","- **Version 1**: LLM-as-Judge (the LLM evaluates its own output)\n","- **Version 2**: Reflection with External Feedback (using HTML validation)\n","\n","We'll use **Hugging Face's free Inference API** with open-source models."]},{"cell_type":"markdown","metadata":{"id":"theory"},"source":["---\n","\n","## üìö Theory: Understanding Agentic AI\n","\n","Before diving into the implementation, let's understand the theoretical foundations of agentic AI and reasoning systems."]},{"cell_type":"markdown","metadata":{"id":"theory_what"},"source":["### What is Agentic AI?\n","\n","**Agentic AI** refers to AI systems that can **plan, act, evaluate, and improve** autonomously in pursuit of specific goals. Unlike traditional AI that follows fixed instructions or responds to patterns, agentic systems use **reasoning loops** to make context-aware decisions in real time.\n","\n","At its core, an agentic system combines:\n","- A **Large Language Model (LLM)** as the reasoning engine\n","- **External tools** that extend capabilities (search, code execution, validation)\n","- **Feedback loops** that enable learning and self-improvement\n","\n","This combination allows AI to handle open-ended, multifaceted problems that require adaptive workflows and context-aware decisions."]},{"cell_type":"markdown","metadata":{"id":"theory_loop"},"source":["### The Reasoning Loop: Think-Act-Observe\n","\n","Agentic systems operate through a continuous cycle that mirrors human problem-solving:\n","\n","```\n","‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n","‚îÇ   THINK     ‚îÇ  1. Task Decomposition: Break down the goal\n","‚îÇ  (Reason)   ‚îÇ  2. Planning: Decide on approach\n","‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n","       ‚îÇ\n","       ‚ñº\n","‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n","‚îÇ    ACT      ‚îÇ  3. Delegation: Assign to tools/agents\n","‚îÇ  (Execute)  ‚îÇ  4. Action: Generate output or call tools\n","‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n","       ‚îÇ\n","       ‚ñº\n","‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n","‚îÇ  OBSERVE    ‚îÇ  5. Evaluation: Review results\n","‚îÇ (Reflect)   ‚îÇ  6. Adaptation: Refine approach\n","‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n","       ‚îÇ\n","       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ Loop back to THINK\n","```\n","\n","This iterative process enables:\n","- **Decision-making** based on context\n","- **Learning** from results\n","- **Long-term planning** across multiple steps"]},{"cell_type":"markdown","metadata":{"id":"theory_patterns"},"source":["### Key Agentic Design Patterns\n","\n","#### 1. **Reflection** üîÑ\n","\n","Creating feedback loops where LLMs review and improve their outputs.\n","\n","**Two Types**:\n","- **Self-Reflection**: LLM critiques its own output (LLM-as-Judge)\n","- **External Feedback**: Using tools to provide objective validation\n","\n","**Research Evidence**:\n","- **Self-Refine** (Madaan et al., 2023): ~20% improvement across diverse tasks\n","- **Reflexion** (Shinn et al., 2023): 91% accuracy on HumanEval (vs GPT-4's 80%)\n","- **CRITIC** (Gou et al., 2024): 10-30% improvement using external tools\n","\n","**When to Apply Reflection**:\n","- ‚úÖ Validating request feasibility\n","- ‚úÖ Checking initial plans\n","- ‚úÖ After each execution step\n","- ‚úÖ Verifying final outputs\n","\n","**Trade-offs**:\n","- ‚ûï Improved accuracy and quality\n","- ‚ûñ Increased latency (multiple LLM calls)\n","- ‚ûñ Higher costs\n","\n","#### 2. **Tool Use** üõ†Ô∏è\n","\n","Extending LLM capabilities with external tools:\n","- Web search for real-time information\n","- Code execution for calculations\n","- Database queries for data access\n","- Validators for correctness checking\n","\n","#### 3. **Planning** üìã\n","\n","Breaking complex goals into actionable steps:\n","- Multi-step reasoning\n","- Conditional logic and branching\n","- Dynamic replanning based on results\n","\n","#### 4. **Multi-Agent Collaboration** üë•\n","\n","Multiple specialized agents working together:\n","- Division of labor by expertise\n","- Parallel processing of subtasks\n","- Coordination and synthesis"]},{"cell_type":"markdown","metadata":{"id":"theory_react"},"source":["### The ReAct Framework\n","\n","**ReAct** (Reasoning + Acting) by Yao et al. (2022) is a foundational framework that combines:\n","\n","- **Reasoning**: Explicit thought traces (reflection + planning)\n","- **Acting**: Task-relevant actions in the environment\n","\n","The framework creates a loop where:\n","1. Reasoning guides action selection\n","2. Actions produce observations\n","3. Observations inform further reasoning\n","\n","**Popular Implementations**:\n","- **DSPy** (Databricks): `ReAct` class\n","- **LangGraph**: `create_react_agent` function\n","- **smolagents** (HuggingFace): ReAct-based code agents"]},{"cell_type":"markdown","metadata":{"id":"theory_comparison"},"source":["### Traditional AI vs Agentic AI\n","\n","| Aspect | Traditional AI | Agentic AI |\n","|--------|---------------|------------|\n","| **Behavior** | Fixed instructions | Dynamic decision-making |\n","| **Feedback** | One-shot response | Iterative refinement |\n","| **Tools** | Limited/none | Extensive tool use |\n","| **Planning** | Pre-programmed | Adaptive planning |\n","| **Learning** | Static | Self-improvement |\n","| **Context** | Pattern matching | Context-aware reasoning |"]},{"cell_type":"markdown","metadata":{"id":"theory_our_approach"},"source":["### Our Implementation Approach\n","\n","In this notebook, we'll implement **two versions** of a reasoning agent:\n","\n","#### **Version 1: LLM-as-Judge (Self-Reflection)**\n","```\n","User Prompt ‚Üí Generate HTML ‚Üí Self-Evaluate ‚Üí Improve ‚Üí Repeat\n","```\n","- The LLM generates code\n","- The same LLM judges its own output\n","- Iteratively improves based on self-critique\n","- **Pros**: Simple, no external dependencies\n","- **Cons**: May have blind spots in self-evaluation\n","\n","#### **Version 2: Reflection with External Feedback**\n","```\n","User Prompt ‚Üí Generate HTML ‚Üí External Validator ‚Üí Reflect on Errors ‚Üí Fix ‚Üí Repeat\n","```\n","- The LLM generates code\n","- External HTML parser validates syntax\n","- LLM reflects on objective validation errors\n","- Iteratively fixes issues\n","- **Pros**: Objective validation, catches concrete errors\n","- **Cons**: Requires external tools, more complex\n","\n","Both approaches demonstrate the power of **reflection** in improving AI output quality through iterative refinement."]},{"cell_type":"markdown","metadata":{"id":"theory_references"},"source":["### Key Research Papers\n","\n","1. **Yao et al. (2022)**: [\"ReAct: Synergizing Reasoning and Acting in Language Models\"](https://arxiv.org/abs/2210.03629)\n","2. **Madaan et al. (2023)**: [\"Self-Refine: Iterative Refinement with Self-Feedback\"](https://arxiv.org/abs/2303.17651)\n","3. **Shinn et al. (2023)**: [\"Reflexion: Language Agents with Verbal Reinforcement Learning\"](https://arxiv.org/abs/2303.11366)\n","4. **Gou et al. (2024)**: [\"CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing\"](https://arxiv.org/abs/2305.11738)\n","\n","Now let's see these concepts in action! üöÄ"]},{"cell_type":"markdown","metadata":{"id":"setup"},"source":["## Setup\n","\n","First, install the required packages and set up authentication."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"install"},"outputs":[],"source":["!pip install -q huggingface_hub"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"imports"},"outputs":[],"source":["import os\n","from huggingface_hub import InferenceClient\n","from typing import Dict, List, Tuple\n","import json\n","from html.parser import HTMLParser\n","import re"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"token"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import os\n","from dotenv import load_dotenv\n","\n","env_path = \"/content/drive/MyDrive/.env\"\n","load_dotenv(env_path)\n","\n","HF_TOKEN = os.getenv('HF_TOKEN')"],"metadata":{"id":"pUeaY3ebYmmK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"model_setup"},"source":["### Model Configuration\n","\n","We'll use **Qwen/Qwen2.5-72B-Instruct** - a powerful open-source model available via Hugging Face Inference API.\n","\n","Alternative models you can try:\n","- `meta-llama/Llama-3.1-70B-Instruct`\n","- `mistralai/Mixtral-8x7B-Instruct-v0.1`\n","- `microsoft/Phi-3-medium-4k-instruct`"]},{"cell_type":"code","source":["# Initialize the Hugging Face Inference Client\n","MODEL_NAME = \"Qwen/Qwen2.5-72B-Instruct\"\n","client = InferenceClient(token=HF_TOKEN)"],"metadata":{"id":"7fCERzHKY9Br"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def call_llm(messages: List[Dict[str, str]], max_tokens: int = 2000, temperature: float = 0.7) -> str:\n","    \"\"\"\n","    Call the LLM with a list of messages.\n","\n","    Args:\n","        messages: List of message dicts with 'role' and 'content'\n","        max_tokens: Maximum tokens to generate\n","        temperature: Sampling temperature (0.0 to 1.0)\n","\n","    Returns:\n","        Generated text response\n","    \"\"\"\n","    try:\n","        response = client.chat_completion(\n","            __,\n","            model=MODEL_NAME,\n","            __,\n","            __\n","        )\n","        return response.choices[0].message.content\n","    except Exception as e:\n","        return f\"Error calling LLM: {str(e)}\""],"metadata":{"id":"AVzXpY-EYxdf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Test the connection\n","test_response = call_llm([{\"role\": \"user\", \"content\": __}], max_tokens=50)\n","print(\"Model test:\", test_response)"],"metadata":{"id":"s7dRkabVZckb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"llm_messages_intro"},"source":["---\n","\n","## üí¨ Understanding LLM Messages: Roles and Content\n","\n","Before we dive into building our reasoning agent, let's understand how we communicate with Large Language Models (LLMs)."]},{"cell_type":"markdown","metadata":{"id":"llm_messages_explain"},"source":["### How Do We Talk to LLMs?\n","\n","When you interact with an LLM (like ChatGPT, Claude, or open-source models), you're not just sending plain text. Instead, you send **structured messages** that help the LLM understand the context and respond appropriately.\n","\n","Each message has two key components:\n","\n","#### 1. **Role** - Who is speaking?\n","\n","There are three main roles:\n","\n","| Role | Description | Purpose |\n","|------|-------------|----------|\n","| **`system`** | Sets the behavior and context | \"You are a helpful assistant\", \"You are an expert coder\" |\n","| **`user`** | The human asking questions | Your prompts and requests |\n","| **`assistant`** | The LLM's responses | Previous answers from the AI |\n","\n","#### 2. **Content** - What is being said?\n","\n","The actual text of the message - the instructions, questions, or responses.\n","\n","### Message Structure\n","\n","Messages are formatted as a list of dictionaries:\n","\n","```python\n","messages = [\n","    {\n","        \"role\": \"system\",\n","        \"content\": \"You are a helpful coding assistant.\"\n","    },\n","    {\n","        \"role\": \"user\",\n","        \"content\": \"Write a Python function to calculate factorial.\"\n","    }\n","]\n","```\n","\n","### Why This Matters\n","\n","Understanding message roles is crucial because:\n","\n","1. **System messages** set the \"personality\" and instructions for the LLM\n","2. **Conversation history** is maintained through user/assistant message pairs\n","3. **Context** from previous messages influences future responses\n","4. **Agentic systems** use this structure to create feedback loops\n","\n","Let's see this in action! üëá"]},{"cell_type":"markdown","metadata":{"id":"llm_messages_demo"},"source":["### üéØ Interactive Demo: Message Roles in Action"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"demo_basic_message"},"outputs":[],"source":["# Example 1: Simple message with system role\n","print(\"Example 1: Basic Message Structure\\n\")\n","print(\"=\"*60)\n","\n","messages_example1 = [\n","    {\n","        \"role\": \"__\",\n","        \"content\": \"You are a friendly teacher explaining concepts simply.\"\n","    },\n","    {\n","        \"role\": \"__\",\n","        \"content\": \"What is Python?\"\n","    }\n","]\n","\n","print(\"Messages sent to LLM:\")\n","for msg in messages_example1:\n","    print(f\"\\n[{msg['role'].upper()}]\")\n","    print(f\"{msg['content']}\")\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"Calling LLM...\\n\")\n","\n","response1 = __(__, max_tokens=200)\n","print(\"[ASSISTANT]\")\n","print(response1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"demo_conversation"},"outputs":[],"source":["# Example 2: Multi-turn conversation\n","print(\"Example 2: Multi-Turn Conversation\\n\")\n","print(\"=\"*60)\n","\n","messages_example2 = [\n","    {\n","        \"role\": \"__\",\n","        \"content\": \"You are a concise coding assistant.\"\n","    },\n","    {\n","        \"role\": \"__\",\n","        \"content\": \"Write a function to add two numbers.\"\n","    },\n","    {\n","        \"role\": \"__\",\n","        \"content\": \"def add(a, b):\\n    return a + b\"\n","    },\n","    {\n","        \"role\": \"__\",\n","        \"content\": \"Now add type hints to it.\"\n","    }\n","]\n","\n","print(\"Conversation history:\")\n","for i, msg in enumerate(messages_example2, 1):\n","    print(f\"\\n{i}. [{msg['role'].upper()}]\")\n","    print(f\"   {msg['content'][:100]}...\" if len(msg['content']) > 100 else f\"   {msg['content']}\")\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"Calling LLM with conversation history...\\n\")\n","\n","response2 = __(__, max_tokens=150)\n","print(\"[ASSISTANT]\")\n","print(response2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"demo_system_role"},"outputs":[],"source":["# Example 3: Impact of different system messages\n","print(\"Example 3: How System Messages Change Behavior\\n\")\n","print(\"=\"*60)\n","\n","user_question = \"Explain what a variable is in programming.\"\n","\n","# Friendly teacher\n","messages_friendly = [\n","    {\"role\": \"__\", \"content\": \"You are a friendly teacher for 10-year-olds. Use simple words and fun examples.\"},\n","    {\"role\": \"__\", \"content\": __}\n","]\n","\n","# Technical expert\n","messages_technical = [\n","    {\"role\": \"__\", \"content\": \"You are a computer science professor. Be precise and technical.\"},\n","    {\"role\": \"__\", \"content\": __}\n","]\n","\n","print(\"Same question, different system messages:\\n\")\n","print(f\"Question: {user_question}\\n\")\n","\n","print(\"\\n\" + \"-\"*60)\n","print(\"Response 1: Friendly Teacher\")\n","print(\"-\"*60)\n","response_friendly = call_llm(__, max_tokens=150)\n","print(response_friendly)\n","\n","print(\"\\n\" + \"-\"*60)\n","print(\"Response 2: Technical Expert\")\n","print(\"-\"*60)\n","response_technical = call_llm(__, max_tokens=150)\n","print(response_technical)\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"Notice how the SAME question gets DIFFERENT answers!\")\n","print(\"This is the power of the system message.\")"]},{"cell_type":"markdown","metadata":{"id":"version1"},"source":["---\n","\n","## Version 1: LLM-as-Judge\n","\n","In this version, the LLM generates HTML code, then acts as a judge to evaluate its own output and suggest improvements. The agent iterates through multiple rounds of generation and self-critique."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v1_generator"},"outputs":[],"source":["def generate_html(prompt: str) -> str:\n","    \"\"\"\n","    Generate HTML code based on a prompt.\n","\n","    Args:\n","        prompt: Description of the HTML to generate\n","\n","    Returns:\n","        Generated HTML code\n","    \"\"\"\n","    messages = [\n","        {\n","            \"role\": \"__\", ## What role?\n","            \"content\": \"You are an expert HTML developer. Generate clean, semantic, and well-structured HTML code based on user requirements. Return ONLY the HTML code without explanations.\"\n","        },\n","        {\n","            \"role\": \"__\", ## What role?\n","            \"content\": f\"Generate HTML code for: {__}\" ## What input?\n","        }\n","    ]\n","\n","    response = call_llm(__, max_tokens=2000, temperature=0.7) ## What context?\n","\n","    # Extract HTML code from response (remove markdown code blocks if present)\n","    html_code = response.strip()\n","    if \"```html\" in html_code:\n","        html_code = html_code.split(\"```html\")[1].split(\"```\")[0].strip()\n","    elif \"```\" in html_code:\n","        html_code = html_code.split(\"```\")[1].split(\"```\")[0].strip()\n","\n","    return html_code"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v1_judge"},"outputs":[],"source":["def judge_html(html_code: str, original_prompt: str) -> Tuple[float, str]:\n","    \"\"\"\n","    Use LLM to judge the quality of generated HTML.\n","\n","    Args:\n","        html_code: The HTML code to evaluate\n","        original_prompt: The original user prompt\n","\n","    Returns:\n","        Tuple of (score, feedback)\n","        - score: Quality score from 0.0 to 10.0\n","        - feedback: Detailed feedback and improvement suggestions\n","    \"\"\"\n","    messages = [\n","        {\n","            \"role\": \"__\", ## What role?\n","            \"content\": \"\"\"You are an expert HTML code reviewer. Evaluate the provided HTML code based on:\n","1. Correctness: Does it match the requirements?\n","2. Code quality: Is it semantic, accessible, and well-structured?\n","3. Best practices: Does it follow HTML5 standards?\n","\n","Provide:\n","- A score from 0 to 10\n","- Specific feedback on what's good and what needs improvement\n","\n","Format your response as:\n","SCORE: [number]\n","FEEDBACK: [your detailed feedback]\"\"\"\n","        },\n","        {\n","            \"role\": \"__\", ## What role?\n","            \"content\": f\"\"\"Original requirement: {__}\n","\n","HTML code to evaluate:\n","```html\n","{__}\n","```\n","\n","Please evaluate this code.\"\"\" ## What inputs?\n","        }\n","    ]\n","\n","    response = __(__, max_tokens=1000, temperature=0.3)\n","\n","    # Parse score and feedback\n","    score = 5.0  # Default score\n","    feedback = response\n","\n","    if \"SCORE:\" in response:\n","        try:\n","            score_text = response.split(\"SCORE:\")[1].split(\"\\n\")[0].strip()\n","            score = float(re.findall(r'\\d+\\.?\\d*', score_text)[0])\n","        except:\n","            pass\n","\n","    if \"FEEDBACK:\" in response:\n","        feedback = response.split(\"FEEDBACK:\")[1].strip()\n","\n","    return score, feedback"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v1_improve"},"outputs":[],"source":["def improve_html(html_code: str, feedback: str, original_prompt: str) -> str:\n","    \"\"\"\n","    Improve HTML code based on feedback.\n","\n","    Args:\n","        html_code: Current HTML code\n","        feedback: Feedback from the judge\n","        original_prompt: Original user prompt\n","\n","    Returns:\n","        Improved HTML code\n","    \"\"\"\n","    messages = [\n","        {\n","            \"role\": \"__\", ## What role?\n","            \"content\": \"You are an expert HTML developer. Improve the provided HTML code based on the feedback. Return ONLY the improved HTML code without explanations.\"\n","        },\n","        {\n","            \"role\": \"__\", ## What role?\n","            \"content\": f\"\"\"Original requirement: {__}\n","\n","Current HTML code:\n","```html\n","{__}\n","```\n","\n","Feedback for improvement:\n","{__}\n","\n","Please provide the improved HTML code.\"\"\" ## What inputs?\n","        }\n","    ]\n","\n","    response = call_llm(__, max_tokens=2000, temperature=0.7)\n","\n","    # Extract HTML code\n","    html_code = response.strip()\n","    if \"```html\" in html_code:\n","        html_code = html_code.split(\"```html\")[1].split(\"```\")[0].strip()\n","    elif \"```\" in html_code:\n","        html_code = html_code.split(\"```\")[1].split(\"```\")[0].strip()\n","\n","    return html_code"]},{"cell_type":"markdown","source":["Let's tie it all together:"],"metadata":{"id":"PnukZoW_D0Zp"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"v1_agent"},"outputs":[],"source":["def reasoning_agent_v1(prompt: str, max_iterations: int = 3, target_score: float = 8.0) -> Dict:\n","    \"\"\"\n","    Reasoning agent that generates and improves HTML using LLM-as-Judge.\n","\n","    Args:\n","        prompt: Description of the HTML to generate\n","        max_iterations: Maximum number of improvement iterations\n","        target_score: Target quality score to achieve\n","\n","    Returns:\n","        Dictionary with final HTML, score, and iteration history\n","    \"\"\"\n","    print(f\"ü§ñ Reasoning Agent V1: LLM-as-Judge\")\n","    print(f\"üìù Task: {prompt}\\n\")\n","\n","    history = []\n","\n","    # Initial generation\n","    print(\"[Iteration 1] Generating initial HTML...\")\n","    html_code = __(__) ## Which function?\n","\n","    # Evaluate\n","    print(\"[Iteration 1] Evaluating quality...\")\n","    score, feedback = __(__, __) ## Which function?\n","    print(f\"[Iteration 1] Score: {score}/10\")\n","    print(f\"[Iteration 1] Feedback: {feedback[:200]}...\\n\")\n","\n","    history.append({\n","        \"iteration\": 1,\n","        \"html\": html_code,\n","        \"score\": score,\n","        \"feedback\": feedback\n","    })\n","\n","    # Iterative improvement\n","    for i in range(2, max_iterations + 1):\n","        if score >= target_score:\n","            print(f\"‚úÖ Target score achieved! Stopping at iteration {i-1}\\n\")\n","            break\n","\n","        print(f\"[Iteration {i}] Improving HTML based on feedback...\")\n","        html_code = __(__, __, __) ## Which function?\n","\n","        print(f\"[Iteration {i}] Evaluating improved version...\")\n","        score, feedback = __(__, __) ## Which function?\n","        print(f\"[Iteration {i}] Score: {score}/10\")\n","        print(f\"[Iteration {i}] Feedback: {feedback[:200]}...\\n\")\n","\n","        history.append({\n","            \"iteration\": i,\n","            \"html\": html_code,\n","            \"score\": score,\n","            \"feedback\": feedback\n","        })\n","\n","    print(f\"üéØ Final Score: {score}/10\\n\")\n","\n","    return {\n","        \"final_html\": html_code,\n","        \"final_score\": score,\n","        \"history\": history\n","    }"]},{"cell_type":"markdown","metadata":{"id":"v1_demo"},"source":["### Demo: Version 1 (LLM-as-Judge)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v1_run"},"outputs":[],"source":["# Run the reasoning agent\n","result_v1 = __( ## Which function?\n","    prompt=\"A modern landing page for a coffee shop with a hero section, menu preview, and contact form\",\n","    max_iterations=3,\n","    target_score=8.0\n",")\n","\n","# Display final HTML\n","print(\"=\"*80)\n","print(\"FINAL HTML CODE:\")\n","print(\"=\"*80)\n","print(result_v1[\"final_html\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v1_visualize"},"outputs":[],"source":["# Visualize the HTML in Colab\n","from IPython.display import HTML, display\n","\n","display(HTML(result_v1[\"final_html\"]))"]},{"cell_type":"markdown","metadata":{"id":"version2"},"source":["---\n","\n","## Version 2: Reflection with External Feedback\n","\n","In this version, we add **external validation** using:\n","1. HTML syntax validation (checking for parsing errors)\n","2. Structure validation (checking for required elements)\n","3. LLM reflection based on external feedback\n","\n","This demonstrates how external tools can provide objective feedback to guide the reasoning process."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v2_validator"},"outputs":[],"source":["class HTMLValidator(HTMLParser):\n","    \"\"\"\n","    Custom HTML parser to validate HTML structure and collect errors.\n","    \"\"\"\n","    def __init__(self):\n","        super().__init__()\n","        self.errors = []\n","        self.tags = []\n","        self.tag_stack = []\n","\n","    def handle_starttag(self, tag, attrs):\n","        self.tags.append(tag)\n","        if tag not in ['img', 'br', 'hr', 'input', 'meta', 'link']:\n","            self.tag_stack.append(tag)\n","\n","    def handle_endtag(self, tag):\n","        if tag in ['img', 'br', 'hr', 'input', 'meta', 'link']:\n","            return\n","        if not self.tag_stack:\n","            self.errors.append(f\"Unexpected closing tag: </{tag}>\")\n","        elif self.tag_stack[-1] != tag:\n","            self.errors.append(f\"Mismatched tags: expected </{self.tag_stack[-1]}>, got </{tag}>\")\n","        else:\n","            self.tag_stack.pop()\n","\n","    def error(self, message):\n","        self.errors.append(f\"Parse error: {message}\")\n","\n","def validate_html(html_code: str, required_elements: List[str] = None) -> Tuple[bool, List[str]]:\n","    \"\"\"\n","    Validate HTML code for syntax errors and required elements.\n","\n","    Args:\n","        html_code: HTML code to validate\n","        required_elements: List of required HTML tags (e.g., ['html', 'body', 'head'])\n","\n","    Returns:\n","        Tuple of (is_valid, list_of_issues)\n","    \"\"\"\n","    validator = HTMLValidator()\n","    issues = []\n","\n","    try:\n","        validator.feed(html_code)\n","    except Exception as e:\n","        issues.append(f\"Critical parsing error: {str(e)}\")\n","        return False, issues\n","\n","    # Check for parsing errors\n","    if validator.errors:\n","        issues.extend(validator.errors)\n","\n","    # Check for unclosed tags\n","    if validator.tag_stack:\n","        issues.append(f\"Unclosed tags: {', '.join(validator.tag_stack)}\")\n","\n","    # Check for required elements\n","    if required_elements:\n","        missing = [elem for elem in required_elements if elem not in validator.tags]\n","        if missing:\n","            issues.append(f\"Missing required elements: {', '.join(missing)}\")\n","\n","    # Basic structure checks\n","    if 'html' in validator.tags:\n","        if 'head' not in validator.tags:\n","            issues.append(\"Missing <head> element\")\n","        if 'body' not in validator.tags:\n","            issues.append(\"Missing <body> element\")\n","\n","    is_valid = len(issues) == 0\n","    return is_valid, issues"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v2_reflect"},"outputs":[],"source":["def reflect_and_improve(html_code: str, validation_issues: List[str], original_prompt: str) -> str:\n","    \"\"\"\n","    Use LLM to reflect on external validation feedback and improve code.\n","\n","    Args:\n","        html_code: Current HTML code\n","        validation_issues: List of issues from external validator\n","        original_prompt: Original user prompt\n","\n","    Returns:\n","        Improved HTML code\n","    \"\"\"\n","    issues_text = \"\\n- \" + \"\\n- \".join(validation_issues)\n","\n","    messages = [\n","        {\n","            \"role\": \"__\",\n","            \"content\": \"\"\"You are an expert HTML developer. You will receive HTML code with validation errors from an external validator.\n","Reflect on these issues and generate corrected HTML code that addresses all problems.\n","Return ONLY the corrected HTML code without explanations.\"\"\"\n","        },\n","        {\n","            \"role\": \"__\",\n","            \"content\": f\"\"\"Original requirement: {__}\n","\n","Current HTML code:\n","```html\n","{__}\n","```\n","\n","External validation found these issues:\n","{__}\n","\n","Please fix all issues and provide the corrected HTML code.\"\"\"\n","        }\n","    ]\n","\n","    response = call_llm(messages, max_tokens=2000, temperature=0.7)\n","\n","    # Extract HTML code\n","    html_code = response.strip()\n","    if \"```html\" in html_code:\n","        html_code = html_code.split(\"```html\")[1].split(\"```\")[0].strip()\n","    elif \"```\" in html_code:\n","        html_code = html_code.split(\"```\")[1].split(\"```\")[0].strip()\n","\n","    return html_code"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v2_agent"},"outputs":[],"source":["def reasoning_agent_v2(prompt: str, max_iterations: int = 3, required_elements: List[str] = None) -> Dict:\n","    \"\"\"\n","    Reasoning agent with reflection based on external validation feedback.\n","\n","    Args:\n","        prompt: Description of the HTML to generate\n","        max_iterations: Maximum number of improvement iterations\n","        required_elements: List of required HTML elements\n","\n","    Returns:\n","        Dictionary with final HTML, validation status, and iteration history\n","    \"\"\"\n","    print(f\"ü§ñ Reasoning Agent V2: Reflection with External Feedback\")\n","    print(f\"üìù Task: {prompt}\\n\")\n","\n","    history = []\n","\n","    # Initial generation\n","    print(\"[Iteration 1] Generating initial HTML...\")\n","    html_code = generate_html(prompt)\n","\n","    # External validation\n","    print(\"[Iteration 1] Running external validation...\")\n","    is_valid, issues = validate_html(html_code, required_elements)\n","\n","    if is_valid:\n","        print(\"[Iteration 1] ‚úÖ Validation passed!\")\n","    else:\n","        print(f\"[Iteration 1] ‚ùå Validation failed with {len(issues)} issue(s)\")\n","        for issue in issues:\n","            print(f\"  - {issue}\")\n","    print()\n","\n","    # Also get LLM judge score\n","    score, feedback = judge_html(html_code, prompt)\n","    print(f\"[Iteration 1] LLM Judge Score: {score}/10\\n\")\n","\n","    history.append({\n","        \"iteration\": 1,\n","        \"html\": html_code,\n","        \"is_valid\": is_valid,\n","        \"issues\": issues,\n","        \"score\": score,\n","        \"feedback\": feedback\n","    })\n","\n","    # Iterative improvement based on external feedback\n","    for i in range(2, max_iterations + 1):\n","        if is_valid and score >= 8.0:\n","            print(f\"‚úÖ Code is valid and high quality! Stopping at iteration {i-1}\\n\")\n","            break\n","\n","        if not is_valid:\n","            # Fix validation issues first\n","            print(f\"[Iteration {i}] Reflecting on validation issues and improving...\")\n","            html_code = reflect_and_improve(html_code, issues, prompt)\n","        else:\n","            # Improve based on LLM feedback\n","            print(f\"[Iteration {i}] Improving based on LLM feedback...\")\n","            html_code = improve_html(html_code, feedback, prompt)\n","\n","        # Validate again\n","        print(f\"[Iteration {i}] Running external validation...\")\n","        is_valid, issues = validate_html(html_code, required_elements)\n","\n","        if is_valid:\n","            print(f\"[Iteration {i}] ‚úÖ Validation passed!\")\n","        else:\n","            print(f\"[Iteration {i}] ‚ùå Validation failed with {len(issues)} issue(s)\")\n","            for issue in issues:\n","                print(f\"  - {issue}\")\n","        print()\n","\n","        # Get LLM score\n","        score, feedback = judge_html(html_code, prompt)\n","        print(f\"[Iteration {i}] LLM Judge Score: {score}/10\\n\")\n","\n","        history.append({\n","            \"iteration\": i,\n","            \"html\": html_code,\n","            \"is_valid\": is_valid,\n","            \"issues\": issues,\n","            \"score\": score,\n","            \"feedback\": feedback\n","        })\n","\n","    print(f\"üéØ Final Status: {'‚úÖ Valid' if is_valid else '‚ùå Invalid'}, Score: {score}/10\\n\")\n","\n","    return {\n","        \"final_html\": html_code,\n","        \"is_valid\": is_valid,\n","        \"final_score\": score,\n","        \"history\": history\n","    }"]},{"cell_type":"markdown","metadata":{"id":"v2_demo"},"source":["### Demo: Version 2 (Reflection with External Feedback)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v2_run"},"outputs":[],"source":["# Run the reasoning agent with external validation\n","result_v2 = reasoning_agent_v2(\n","    prompt=\"A modern landing page for a coffee shop with a hero section, menu preview, and contact form\",\n","    max_iterations=3,\n","    required_elements=['html', 'head', 'body', 'title']\n",")\n","\n","# Display final HTML\n","print(\"=\"*80)\n","print(\"FINAL HTML CODE:\")\n","print(\"=\"*80)\n","print(result_v2[\"final_html\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v2_visualize"},"outputs":[],"source":["# Visualize the HTML in Colab\n","from IPython.display import HTML, display\n","\n","display(HTML(result_v2[\"final_html\"]))"]},{"cell_type":"markdown","metadata":{"id":"save_view"},"source":["---\n","\n","## üíæ Saving and Viewing HTML Files\n","\n","Let's save the generated HTML files and learn different ways to view them."]},{"cell_type":"markdown","metadata":{"id":"save_instructions"},"source":["### Method 1: Save to Files and Download\n","\n","Save the HTML files to the Colab filesystem and download them to view in your browser."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"save_html_files"},"outputs":[],"source":["import os\n","from google.colab import files\n","\n","def save_html_file(html_content: str, filename: str) -> str:\n","    \"\"\"\n","    Save HTML content to a file.\n","\n","    Args:\n","        html_content: The HTML code to save\n","        filename: Name of the file (e.g., 'output_v1.html')\n","\n","    Returns:\n","        Full path to the saved file\n","    \"\"\"\n","    filepath = f'/content/{filename}'\n","\n","    with open(filepath, 'w', encoding='utf-8') as f:\n","        f.write(html_content)\n","\n","    print(f\"‚úÖ Saved: {filepath}\")\n","    return filepath\n","\n","# Save both HTML files\n","print(\"Saving HTML files...\\n\")\n","\n","v1_filepath = save_html_file(result_v1['final_html'], 'coffee_shop_v1_llm_judge.html')\n","v2_filepath = save_html_file(result_v2['final_html'], 'coffee_shop_v2_external_feedback.html')\n","\n","print(\"\\nüìÅ Files saved successfully!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"download_files"},"outputs":[],"source":["# Download files to your local machine\n","print(\"Downloading files...\\n\")\n","\n","files.download(v1_filepath)\n","files.download(v2_filepath)\n","\n","print(\"\\n‚úÖ Files downloaded! Open them in your browser to view.\")"]},{"cell_type":"markdown","metadata":{"id":"view_inline"},"source":["### Method 2: View Inline in Colab\n","\n","Display the HTML directly in the notebook using IPython's HTML display."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"view_v1_inline"},"outputs":[],"source":["from IPython.display import HTML, display\n","\n","print(\"=\"*80)\n","print(\"VERSION 1: LLM-as-Judge\")\n","print(\"=\"*80)\n","display(HTML(result_v1['final_html']))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"view_v2_inline"},"outputs":[],"source":["print(\"=\"*80)\n","print(\"VERSION 2: Reflection with External Feedback\")\n","print(\"=\"*80)\n","display(HTML(result_v2['final_html']))"]},{"cell_type":"markdown","metadata":{"id":"view_iframe"},"source":["### Method 3: View in IFrame\n","\n","Display the saved HTML files in an iframe for a more isolated view."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"view_iframe_code"},"outputs":[],"source":["from IPython.display import IFrame\n","\n","print(\"Viewing Version 1 in IFrame:\\n\")\n","display(IFrame(src=v1_filepath, width=800, height=600))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"view_iframe_v2"},"outputs":[],"source":["print(\"Viewing Version 2 in IFrame:\\n\")\n","display(IFrame(src=v2_filepath, width=800, height=600))"]},{"cell_type":"markdown","metadata":{"id":"view_comparison"},"source":["### Method 4: Side-by-Side Comparison\n","\n","View both versions side by side to compare the results."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"side_by_side"},"outputs":[],"source":["from IPython.display import HTML, display\n","\n","comparison_html = f\"\"\"\n","<div style=\"display: flex; gap: 20px;\">\n","    <div style=\"flex: 1; border: 2px solid #4CAF50; padding: 10px;\">\n","        <h3 style=\"color: #4CAF50; text-align: center;\">Version 1: LLM-as-Judge</h3>\n","        <p style=\"text-align: center;\"><strong>Score:</strong> {result_v1['final_score']}/10</p>\n","        <iframe srcdoc='{result_v1['final_html'].replace(\"'\", \"&apos;\")}'\n","                width=\"100%\" height=\"500\" style=\"border: 1px solid #ddd;\"></iframe>\n","    </div>\n","    <div style=\"flex: 1; border: 2px solid #2196F3; padding: 10px;\">\n","        <h3 style=\"color: #2196F3; text-align: center;\">Version 2: External Feedback</h3>\n","        <p style=\"text-align: center;\">\n","            <strong>Score:</strong> {result_v2['final_score']}/10 |\n","            <strong>Valid:</strong> {'‚úÖ' if result_v2['is_valid'] else '‚ùå'}\n","        </p>\n","        <iframe srcdoc='{result_v2['final_html'].replace(\"'\", \"&apos;\")}'\n","                width=\"100%\" height=\"500\" style=\"border: 1px solid #ddd;\"></iframe>\n","    </div>\n","</div>\n","\"\"\"\n","\n","display(HTML(comparison_html))"]},{"cell_type":"markdown","metadata":{"id":"export_tips"},"source":["### üí° Tips for Viewing HTML Files\n","\n","**In Google Colab**:\n","- Use `display(HTML(...))` for inline viewing\n","- Use `IFrame(...)` for isolated viewing\n","- Files are saved to `/content/` directory\n","\n","**On Your Local Machine**:\n","1. Download the files using `files.download()`\n","2. Open them in any web browser (Chrome, Firefox, Safari, etc.)\n","3. Right-click ‚Üí Open With ‚Üí Your Browser\n","\n","**Sharing Your HTML**:\n","- Upload to GitHub Pages for free hosting\n","- Use services like CodePen, JSFiddle, or Netlify\n","- Share the HTML code directly\n","\n","**Editing the HTML**:\n","- Open in any text editor (VS Code, Sublime, Notepad++)\n","- Use browser DevTools to inspect and modify\n","- Feed back to the agent for further improvements"]},{"cell_type":"markdown","metadata":{"id":"comparison"},"source":["---\n","\n","## Comparison: V1 vs V2\n","\n","Let's compare the two approaches:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"compare"},"outputs":[],"source":["import pandas as pd\n","\n","comparison_data = {\n","    \"Aspect\": [\n","        \"Feedback Source\",\n","        \"Objectivity\",\n","        \"Error Detection\",\n","        \"Improvement Focus\",\n","        \"Reliability\"\n","    ],\n","    \"V1: LLM-as-Judge\": [\n","        \"LLM self-evaluation\",\n","        \"Subjective\",\n","        \"May miss syntax errors\",\n","        \"Overall quality & style\",\n","        \"Depends on LLM capability\"\n","    ],\n","    \"V2: External Feedback\": [\n","        \"External validator + LLM\",\n","        \"Objective validation\",\n","        \"Catches syntax errors\",\n","        \"Correctness first, then quality\",\n","        \"More reliable for correctness\"\n","    ]\n","}\n","\n","df_comparison = pd.DataFrame(comparison_data)\n","print(df_comparison.to_string(index=False))"]},{"cell_type":"markdown","metadata":{"id":"custom"},"source":["---\n","\n","## Try Your Own Prompts!\n","\n","Experiment with both agents using your own prompts:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"custom_run"},"outputs":[],"source":["# Customize your prompt here\n","custom_prompt = \"A portfolio page for a photographer with an image gallery and about section\"\n","\n","print(\"\\n\" + \"=\"*80)\n","print(\"TESTING VERSION 1: LLM-as-Judge\")\n","print(\"=\"*80 + \"\\n\")\n","\n","result_custom_v1 = reasoning_agent_v1(\n","    prompt=custom_prompt,\n","    max_iterations=2,\n","    target_score=8.0\n",")\n","\n","print(\"\\n\" + \"=\"*80)\n","print(\"TESTING VERSION 2: Reflection with External Feedback\")\n","print(\"=\"*80 + \"\\n\")\n","\n","result_custom_v2 = reasoning_agent_v2(\n","    prompt=custom_prompt,\n","    max_iterations=2,\n","    required_elements=['html', 'head', 'body', 'title']\n",")\n","\n","# Display both results\n","print(\"\\n\" + \"=\"*80)\n","print(\"RESULTS COMPARISON\")\n","print(\"=\"*80)\n","print(f\"V1 Final Score: {result_custom_v1['final_score']}/10\")\n","print(f\"V2 Final Score: {result_custom_v2['final_score']}/10\")\n","print(f\"V2 Valid HTML: {result_custom_v2['is_valid']}\")"]},{"cell_type":"markdown","metadata":{"id":"interactive_section"},"source":["---\n","\n","## üéÆ Interactive Exercises for Students\n","\n","Time to get hands-on! These exercises will help you understand reasoning agents better."]},{"cell_type":"markdown","metadata":{"id":"exercise1"},"source":["### üèÜ Exercise 1: Design Your Own System Message\n","\n","**Challenge**: Create a system message that makes the LLM generate HTML in a specific style.\n","\n","Try these personas:\n","- A minimalist designer (clean, simple HTML)\n","- A creative artist (colorful, animated)\n","- An accessibility expert (ARIA labels, semantic HTML)\n","- A 90s web designer (tables, marquee tags!)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"exercise1_code"},"outputs":[],"source":["# TODO: Students fill this in!\n","# Create your own system message and generate HTML\n","\n","my_system_message = \"\"\"\n","# YOUR SYSTEM MESSAGE HERE\n","# Example: You are a minimalist web designer who believes less is more...\n","\"\"\"\n","\n","my_prompt = \"Create a simple contact form\"  # Change this too!\n","\n","messages = [\n","    {\"role\": \"system\", \"content\": my_system_message},\n","    {\"role\": \"user\", \"content\": f\"Generate HTML for: {my_prompt}\"}\n","]\n","\n","my_html = call_llm(messages, max_tokens=2000)\n","\n","# Extract and display\n","if \"```html\" in my_html:\n","    my_html = my_html.split(\"```html\")[1].split(\"```\")[0].strip()\n","elif \"```\" in my_html:\n","    my_html = my_html.split(\"```\")[1].split(\"```\")[0].strip()\n","\n","from IPython.display import HTML, display\n","print(\"Your Generated HTML:\\n\")\n","print(my_html)\n","print(\"\\n\" + \"=\"*60 + \"\\n\")\n","print(\"Preview:\")\n","display(HTML(my_html))"]},{"cell_type":"markdown","metadata":{"id":"exercise2"},"source":["### üèÜ Exercise 2: Build a Simple Feedback Loop\n","\n","**Challenge**: Implement a mini reasoning agent that:\n","1. Generates a joke\n","2. Rates the joke (1-10)\n","3. If score < 7, generates a better joke\n","\n","This teaches the core concept of reflection!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"exercise2_code"},"outputs":[],"source":["# TODO: Students complete this!\n","\n","def generate_joke(topic: str) -> str:\n","    \"\"\"Generate a joke about the given topic.\"\"\"\n","    messages = [\n","        {\"role\": \"system\", \"content\": \"You are a comedian. Generate a short, funny joke.\"},\n","        {\"role\": \"user\", \"content\": f\"Tell me a joke about {topic}\"}\n","    ]\n","    return call_llm(messages, max_tokens=100)\n","\n","def rate_joke(joke: str) -> float:\n","    \"\"\"Rate the joke from 1-10.\"\"\"\n","    messages = [\n","        {\"role\": \"system\", \"content\": \"You are a comedy critic. Rate jokes from 1-10. Respond with just: SCORE: [number]\"},\n","        {\"role\": \"user\", \"content\": f\"Rate this joke: {joke}\"}\n","    ]\n","    response = call_llm(messages, max_tokens=50)\n","\n","    # Extract score\n","    try:\n","        import re\n","        score = float(re.findall(r'\\d+\\.?\\d*', response)[0])\n","        return score\n","    except:\n","        return 5.0\n","\n","def improve_joke(joke: str, score: float) -> str:\n","    \"\"\"Improve the joke based on the score.\"\"\"\n","    # TODO: Students implement this!\n","    # Hint: Ask the LLM to improve the joke\n","    messages = [\n","        # YOUR CODE HERE\n","    ]\n","    return call_llm(messages, max_tokens=100)\n","\n","# Run the joke improvement loop\n","topic = \"programming\"  # Change this!\n","max_iterations = 3\n","\n","print(f\"üé≠ Joke Improvement Agent: Topic = '{topic}'\\n\")\n","print(\"=\"*60)\n","\n","joke = generate_joke(topic)\n","print(f\"\\n[Iteration 1]\")\n","print(f\"Joke: {joke}\")\n","score = rate_joke(joke)\n","print(f\"Score: {score}/10\")\n","\n","for i in range(2, max_iterations + 1):\n","    if score >= 7.0:\n","        print(f\"\\n‚úÖ Great joke! Stopping at iteration {i-1}\")\n","        break\n","\n","    print(f\"\\n[Iteration {i}] Improving...\")\n","    joke = improve_joke(joke, score)\n","    print(f\"Joke: {joke}\")\n","    score = rate_joke(joke)\n","    print(f\"Score: {score}/10\")\n","\n","print(\"\\n\" + \"=\"*60)\n","print(f\"Final joke (Score: {score}/10):\")\n","print(joke)"]},{"cell_type":"markdown","metadata":{"id":"exercise3"},"source":["### üèÜ Exercise 3: Experiment with Reflection Patterns\n","\n","**Challenge**: Modify the HTML generator to add a new type of feedback.\n","\n","Ideas:\n","- Check for accessibility (alt tags, ARIA labels)\n","- Count the number of HTML elements\n","- Validate CSS if inline styles are used\n","- Check for mobile responsiveness"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"exercise3_code"},"outputs":[],"source":["# TODO: Students implement a new validator!\n","\n","def check_accessibility(html_code: str) -> tuple[bool, list[str]]:\n","    \"\"\"\n","    Check HTML for accessibility issues.\n","\n","    Returns:\n","        (is_accessible, list_of_issues)\n","    \"\"\"\n","    issues = []\n","\n","    # TODO: Implement accessibility checks\n","    # Hints:\n","    # - Check for <img> tags without alt attributes\n","    # - Check for form inputs without labels\n","    # - Check for proper heading hierarchy (h1, h2, h3...)\n","\n","    # Example check:\n","    if '<img' in html_code and 'alt=' not in html_code:\n","        issues.append(\"Images missing alt attributes\")\n","\n","    # Add more checks here!\n","\n","    is_accessible = len(issues) == 0\n","    return is_accessible, issues\n","\n","# Test your validator\n","test_html = \"\"\"\n","<html>\n","<body>\n","    <h1>Welcome</h1>\n","    <img src=\"logo.png\">\n","    <input type=\"text\">\n","</body>\n","</html>\n","\"\"\"\n","\n","is_accessible, issues = check_accessibility(test_html)\n","print(\"Accessibility Check Results:\")\n","print(f\"Accessible: {is_accessible}\")\n","if issues:\n","    print(\"\\nIssues found:\")\n","    for issue in issues:\n","        print(f\"  - {issue}\")"]},{"cell_type":"markdown","metadata":{"id":"exercise4"},"source":["### üèÜ Exercise 4: Create Your Own Reasoning Agent\n","\n","**Challenge**: Build a reasoning agent for a different task!\n","\n","Ideas:\n","- **Email writer**: Generate professional emails and improve tone\n","- **Recipe creator**: Generate recipes and check for missing ingredients\n","- **Story writer**: Generate stories and improve plot consistency\n","- **Math solver**: Solve problems and verify answers\n","- **Code debugger**: Generate code and check for syntax errors"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"exercise4_code"},"outputs":[],"source":["# TODO: Students build their own reasoning agent!\n","\n","def my_reasoning_agent(task: str, max_iterations: int = 3):\n","    \"\"\"\n","    Your custom reasoning agent.\n","\n","    Steps:\n","    1. Generate initial output\n","    2. Evaluate output (self-reflection or external validation)\n","    3. Improve based on feedback\n","    4. Repeat until satisfied or max iterations reached\n","    \"\"\"\n","    print(f\"ü§ñ My Reasoning Agent\")\n","    print(f\"Task: {task}\\n\")\n","    print(\"=\"*60)\n","\n","    # TODO: Implement your agent here!\n","    # Use the patterns from Version 1 or Version 2\n","\n","    pass\n","\n","# Test your agent\n","# my_reasoning_agent(\"Write a professional email asking for a meeting\")"]},{"cell_type":"markdown","metadata":{"id":"discussion_questions"},"source":["### üí≠ Discussion Questions for Class\n","\n","1. **When is self-reflection (LLM-as-Judge) better than external validation?**\n","   - Think about subjective vs. objective tasks\n","\n","2. **What are the trade-offs of adding more reflection iterations?**\n","   - Consider: quality, cost, latency, diminishing returns\n","\n","3. **How would you combine multiple types of feedback?**\n","   - Example: Syntax validation + style checking + user preferences\n","\n","4. **What tasks are NOT suitable for reasoning agents?**\n","   - When is a simple one-shot generation better?\n","\n","5. **How can we prevent infinite loops in reasoning agents?**\n","   - What stopping criteria make sense?\n","\n","6. **Real-world applications**: Where have you seen reasoning agents in action?\n","   - GitHub Copilot, ChatGPT Code Interpreter, etc."]},{"cell_type":"markdown","metadata":{"id":"bonus_challenges"},"source":["### üåü Bonus Challenges\n","\n","For advanced students:\n","\n","1. **Multi-Agent System**: Create two agents that critique each other's work\n","2. **Adaptive Iterations**: Dynamically decide how many iterations to run based on improvement rate\n","3. **Feedback Fusion**: Combine LLM-as-Judge with multiple external validators\n","4. **Memory System**: Make the agent remember past mistakes and avoid them\n","5. **Human-in-the-Loop**: Add a step where the agent asks for human feedback\n","6. **Parallel Generation**: Generate multiple candidates and pick the best one\n","7. **Cost Optimization**: Minimize API calls while maintaining quality"]},{"cell_type":"markdown","metadata":{"id":"conclusion"},"source":["---\n","\n","## Key Takeaways\n","\n","This notebook demonstrated two approaches to building reasoning agents:\n","\n","### **Version 1: LLM-as-Judge**\n","- The LLM evaluates its own output\n","- Useful for subjective quality assessment\n","- Simpler implementation\n","- May have blind spots in self-evaluation\n","\n","### **Version 2: Reflection with External Feedback**\n","- Combines LLM reasoning with objective external tools\n","- More reliable for catching concrete errors\n","- Demonstrates how to integrate external validation\n","- Better separation of concerns (correctness vs. quality)\n","\n","### **General Principles**\n","1. **Iterative refinement**: Both agents improve through multiple iterations\n","2. **Feedback loops**: Critical for self-improvement\n","3. **External validation**: Adds objectivity and reliability\n","4. **Open-source models**: Powerful reasoning is possible without proprietary APIs\n","\n","### **Extensions You Can Try**\n","- Add CSS validation\n","- Include accessibility checks (WCAG compliance)\n","- Add performance metrics (page size, load time)\n","- Implement multi-agent collaboration (one generates, another reviews)\n","- Add user feedback as another external signal"]}],"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}