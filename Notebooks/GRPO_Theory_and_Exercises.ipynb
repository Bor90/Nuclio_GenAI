{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab2bcdab",
   "metadata": {},
   "source": [
    "# Example: Solving a Task Without RL\n",
    "\n",
    "Let's consider a simple task: balancing a pole on a cart (the classic CartPole problem).\n",
    "\n",
    "**Without RL:**\n",
    "- We use a fixed, hand-crafted policy (e.g., always move the cart in the direction the pole is falling).\n",
    "- This approach does not learn from experience and cannot adapt to new situations.\n",
    "\n",
    "```python\n",
    "# Hand-crafted (non-RL) policy for CartPole\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "state = env.reset()\n",
    "done = False\n",
    "steps = 0\n",
    "while not done:\n",
    "    # Simple rule: move right if pole angle > 0, else left\n",
    "    action = 1 if state[2] > 0 else 0\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    steps += 1\n",
    "print(f'Hand-crafted policy survived for {steps} steps.')\n",
    "```\n",
    "\n",
    "*This policy is simple but not optimal. It cannot improve with experience. Later, we'll compare this to a learned RL policy!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1614b89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization for Hand-Crafted Policy (No RL)\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "def run_handcrafted_policy(render=True):\n",
    "    env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "    state = env.reset()\n",
    "    frames = []\n",
    "    done = False\n",
    "    steps = 0\n",
    "    while not done:\n",
    "        if render:\n",
    "            frames.append(env.render())\n",
    "        action = 1 if state[2] > 0 else 0\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        steps += 1\n",
    "    env.close()\n",
    "    print(f'Pole fell after {steps} steps (hand-crafted policy).')\n",
    "    return frames\n",
    "\n",
    "def display_frames_as_gif(frames):\n",
    "    fig = plt.figure(figsize=(frames[0].shape[1]/72, frames[0].shape[0]/72))\n",
    "    plt.axis('off')\n",
    "    ims = [[plt.imshow(frame, animated=True)] for frame in frames]\n",
    "    ani = animation.ArtistAnimation(fig, ims, interval=50, blit=True, repeat_delay=1000)\n",
    "    plt.close(fig)\n",
    "    return HTML(ani.to_jshtml())\n",
    "\n",
    "frames = run_handcrafted_policy()\n",
    "display_frames_as_gif(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba97894",
   "metadata": {},
   "source": [
    "# Group Relative Policy Optimization (GRPO): Theory and Exercises\n",
    "\n",
    "This Colab-ready notebook covers the theory and practical exercises for Group Relative Policy Optimization (GRPO), based on the DeepSeek R1 and related RL resources. You will learn the motivation, theory, and implementation of GRPO, and get hands-on experience with code exercises.\n",
    "\n",
    "---\n",
    "\n",
    "**Outline:**\n",
    "1. Import Required Libraries\n",
    "2. Theory: GRPO Overview\n",
    "3. Theory: Policy Gradient Methods\n",
    "4. Theory: The GRPO Algorithm\n",
    "5. Exercise: Implementing the GRPO Update Rule\n",
    "6. Exercise: Policy Network Architecture\n",
    "7. Exercise: Training Loop for GRPO\n",
    "8. Exercise: Visualizing Training Progress\n",
    "9. Exercise: Hyperparameter Tuning\n",
    "\n",
    "---\n",
    "\n",
    "*Let's get started!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebc7953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Import Required Libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List\n",
    "\n",
    "# If running in Colab, install extra packages if needed\n",
    "try:\n",
    "    import datasets\n",
    "    import trl\n",
    "except ImportError:\n",
    "    !pip install -qqq datasets==3.2.0 transformers==4.47.1 trl==0.14.0 peft==0.14.0 accelerate==1.2.1 bitsandbytes==0.45.2 wandb==0.19.7 --progress-bar off\n",
    "    import datasets\n",
    "    import trl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216345b2",
   "metadata": {},
   "source": [
    "# 2. Theory: GRPO Overview\n",
    "\n",
    "Group Relative Policy Optimization (GRPO) is a reinforcement learning algorithm designed to efficiently align large language models (LLMs) with human preferences or verifiable objectives. GRPO is especially useful for tasks where multiple candidate responses can be generated and compared within a group.\n",
    "\n",
    "**Key ideas:**\n",
    "- Instead of comparing just two responses (as in PPO or DPO), GRPO generates a group of responses for each prompt and evaluates them together.\n",
    "- Rewards can come from any function: a reward model, rule-based checks, or even human feedback.\n",
    "- GRPO normalizes rewards within each group, making learning more stable and robust.\n",
    "- The algorithm updates the policy to favor responses that are better than the group average, while penalizing those that are worse.\n",
    "\n",
    "**Why GRPO?**\n",
    "- More stable and efficient than pairwise methods.\n",
    "- Flexible: works with any reward function.\n",
    "- Used in state-of-the-art LLMs like DeepSeek R1 for reasoning and alignment.\n",
    "\n",
    "*Next, let's review the basics of policy gradient methods that GRPO builds upon.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac7e81f",
   "metadata": {},
   "source": [
    "# 3. Theory: Policy Gradient Methods\n",
    "\n",
    "Policy gradient methods are a family of reinforcement learning algorithms that directly optimize the parameters of a policy (the agent's decision-making function) to maximize expected reward.\n",
    "\n",
    "**REINFORCE Algorithm:**\n",
    "- The classic policy gradient method.\n",
    "- The agent samples actions according to its policy, collects rewards, and updates the policy to increase the probability of actions that led to higher rewards.\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "\n",
    "The policy gradient objective is:\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\mathbb{E}_{\\pi_\\theta}[R]\n",
    "$$\n",
    "\n",
    "The gradient is:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\pi_\\theta} [\\nabla_\\theta \\log \\pi_\\theta(a|s) \\cdot (R - b)]\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\pi_\\theta(a|s)$: Policy probability of action $a$ in state $s$\n",
    "- $R$: Reward\n",
    "- $b$: Baseline (e.g., average reward) to reduce variance\n",
    "\n",
    "*GRPO generalizes this idea by comparing multiple actions (responses) within a group, rather than just one at a time.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12357d4d",
   "metadata": {},
   "source": [
    "# 4. Theory: The GRPO Algorithm\n",
    "\n",
    "GRPO extends policy gradient methods by evaluating and optimizing over groups of responses.\n",
    "\n",
    "**Algorithm Steps:**\n",
    "1. For each prompt, generate a group of $G$ responses.\n",
    "2. Compute a reward for each response using a reward function (can be rule-based, model-based, or human feedback).\n",
    "3. Normalize rewards within the group:\n",
    "\n",
    "$$\n",
    "A_i = \\frac{r_i - \\text{mean}(\\{r_1, ..., r_G\\})}{\\text{std}(\\{r_1, ..., r_G\\})}\n",
    "$$\n",
    "\n",
    "where $A_i$ is the advantage for response $i$.\n",
    "4. Update the policy to increase the probability of responses with $A_i > 0$ and decrease for $A_i < 0$.\n",
    "5. Use a clipped objective and KL penalty for stability:\n",
    "\n",
    "$$\n",
    "J_{GRPO}(\\theta) = \\mathbb{E}\\left[\\min\\left(ratio \\cdot A, \\text{clip}(ratio, 1-\\epsilon, 1+\\epsilon) \\cdot A\\right) - \\beta D_{KL}(\\pi_\\theta || \\pi_{ref})\\right]\n",
    "$$\n",
    "\n",
    "- $ratio = \\frac{\\pi_\\theta(o_i|q)}{\\pi_{\\theta_{old}}(o_i|q)}$\n",
    "- $\\epsilon$: Clipping parameter\n",
    "- $\\beta$: KL penalty coefficient\n",
    "\n",
    "**Summary:**\n",
    "- GRPO is more stable than pairwise methods (like PPO/DPO)\n",
    "- Works with any reward function\n",
    "- Used for aligning LLMs with human preferences or verifiable objectives\n",
    "\n",
    "*Next, let's implement the GRPO update rule!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab5ac94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Exercise: Implementing the GRPO Update Rule\n",
    "\n",
    "# Given: rewards (list of floats), old_log_probs (tensor), new_log_probs (tensor), epsilon (float), beta (float), kl_div (tensor)\n",
    "# TODO: Complete the GRPO update rule below\n",
    "\n",
    "def grpo_advantages(rewards: List[float]):\n",
    "    rewards = np.array(rewards)\n",
    "    mean = rewards.mean()\n",
    "    std = rewards.std() + 1e-8\n",
    "    return (rewards - mean) / std\n",
    "\n",
    "# Example usage:\n",
    "rewards = [1, 0, 0, 1]  # 2 correct, 2 incorrect\n",
    "advantages = grpo_advantages(rewards)\n",
    "print('Advantages:', advantages)\n",
    "\n",
    "# Now, fill in the GRPO loss calculation (PyTorch)\n",
    "def grpo_loss(new_log_probs, old_log_probs, advantages, epsilon=0.2, beta=0.04, kl_div=None):\n",
    "    # new_log_probs, old_log_probs: shape (batch,)\n",
    "    # advantages: shape (batch,)\n",
    "    prob_ratio = torch.exp(new_log_probs - old_log_probs)\n",
    "    clipped_ratio = torch.clamp(prob_ratio, 1 - epsilon, 1 + epsilon)\n",
    "    loss1 = -prob_ratio * advantages\n",
    "    loss2 = -clipped_ratio * advantages\n",
    "    policy_loss = torch.max(loss1, loss2).mean()\n",
    "    kl_penalty = beta * kl_div.mean() if kl_div is not None else 0.0\n",
    "    return policy_loss + kl_penalty\n",
    "\n",
    "# EXERCISE: Try changing the rewards and see how the advantages and loss change!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcb2f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Exercise: Policy Network Architecture\n",
    "\n",
    "# Here is a simple policy network for discrete action spaces (e.g., CartPole, simple RL tasks)\n",
    "# EXERCISE: Try changing the hidden layer size or adding more layers!\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Example usage:\n",
    "# state_dim = 4  # e.g., for CartPole\n",
    "# action_dim = 2\n",
    "# policy = PolicyNetwork(state_dim, action_dim)\n",
    "# print(policy)\n",
    "\n",
    "# EXERCISE: Try different architectures and see how it affects learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1645e2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Exercise: Training Loop for GRPO\n",
    "\n",
    "# This is a simplified training loop for a toy RL environment (e.g., CartPole)\n",
    "# EXERCISE: Run the loop and try modifying the number of episodes, group size, or reward function!\n",
    "\n",
    "import gym\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "policy = PolicyNetwork(state_dim, action_dim)\n",
    "optimizer = optim.Adam(policy.parameters(), lr=1e-2)\n",
    "\n",
    "group_size = 4  # Number of responses per prompt (episode)\n",
    "num_episodes = 100\n",
    "all_rewards = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    states, actions, rewards, log_probs = [], [], [], []\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    ep_reward = 0\n",
    "    while not done:\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "        logits = policy(state_tensor)\n",
    "        action_dist = torch.distributions.Categorical(logits=logits)\n",
    "        action = action_dist.sample()\n",
    "        log_prob = action_dist.log_prob(action)\n",
    "        next_state, reward, done, _ = env.step(action.item())\n",
    "        states.append(state)\n",
    "        actions.append(action.item())\n",
    "        rewards.append(reward)\n",
    "        log_probs.append(log_prob)\n",
    "        state = next_state\n",
    "        ep_reward += reward\n",
    "    all_rewards.append(ep_reward)\n",
    "    # GRPO: group rewards and log_probs (simulate group by batching episodes)\n",
    "    if (episode + 1) % group_size == 0:\n",
    "        group_rewards = all_rewards[-group_size:]\n",
    "        group_log_probs = torch.stack(log_probs[-group_size:])\n",
    "        advantages = torch.FloatTensor(grpo_advantages(group_rewards))\n",
    "        # For simplicity, use old_log_probs = new_log_probs\n",
    "        loss = grpo_loss(group_log_probs, group_log_probs, advantages)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f'Episode {episode+1}, Group Reward: {np.mean(group_rewards):.2f}, Loss: {loss.item():.3f}')\n",
    "\n",
    "# EXERCISE: Try changing group_size, num_episodes, or the reward function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003a0dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Exercise: Visualizing Training Progress\n",
    "\n",
    "# Plot rewards over episodes to see how the agent is learning\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(all_rewards)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('GRPO Training Progress')\n",
    "plt.show()\n",
    "\n",
    "# EXERCISE: What do you observe about the learning curve? Try running with different group sizes or learning rates and see how the plot changes!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc984187",
   "metadata": {},
   "source": [
    "# 9. Exercise: Hyperparameter Tuning\n",
    "\n",
    "Try changing the following hyperparameters and observe their effect on GRPO performance:\n",
    "- Learning rate (in the optimizer)\n",
    "- Group size (number of responses per prompt)\n",
    "- Hidden layer size in the policy network\n",
    "- Number of training episodes\n",
    "\n",
    "**Questions:**\n",
    "- How does increasing the group size affect stability and learning speed?\n",
    "- What happens if the learning rate is too high or too low?\n",
    "- How does the policy network architecture impact performance?\n",
    "\n",
    "*Modify the code above and re-run the training loop and plots to answer these questions!*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5de493",
   "metadata": {},
   "source": [
    "# Example: Solving the Same Task With RL (GRPO)\n",
    "\n",
    "Now let's revisit the CartPole task, but this time using a policy learned with RL (GRPO).\n",
    "\n",
    "- The RL agent starts with no knowledge and learns from experience.\n",
    "- Over time, it discovers a much better policy than the hand-crafted rule.\n",
    "\n",
    "Run the training loop above, then try this code to see how long the learned policy can balance the pole:\n",
    "\n",
    "```python\n",
    "# Evaluate the trained RL policy\n",
    "state = env.reset()\n",
    "done = False\n",
    "steps = 0\n",
    "while not done:\n",
    "    state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "    logits = policy(state_tensor)\n",
    "    action = torch.argmax(logits, dim=1).item()\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    steps += 1\n",
    "print(f'RL-trained policy survived for {steps} steps.')\n",
    "```\n",
    "\n",
    "*Compare the number of steps survived by the hand-crafted policy and the RL-trained policy. What do you observe?*\n",
    "\n",
    "**This demonstrates the power of RL: the agent learns to outperform fixed rules by learning from experience!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d91567a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization for RL-Trained Policy (GRPO)\n",
    "# Make sure to run the training loop first to train the policy network!\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "def run_rl_policy(policy, render=True):\n",
    "    env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "    state = env.reset()\n",
    "    frames = []\n",
    "    done = False\n",
    "    steps = 0\n",
    "    while not done:\n",
    "        if render:\n",
    "            frames.append(env.render())\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "        logits = policy(state_tensor)\n",
    "        action = torch.argmax(logits, dim=1).item()\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        steps += 1\n",
    "    env.close()\n",
    "    print(f'Pole fell after {steps} steps (RL-trained policy).')\n",
    "    return frames\n",
    "\n",
    "frames = run_rl_policy(policy)\n",
    "display_frames_as_gif(frames)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
