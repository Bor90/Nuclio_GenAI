{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b59e8730",
   "metadata": {},
   "source": [
    "# Practical Exercise: Fine-tune a Model with GRPO\n",
    "\n",
    "In this Colab-ready notebook, you'll fine-tune a language model using Group Relative Policy Optimization (GRPO) as described in the DeepSeek R1 and Hugging Face TRL documentation. This exercise is adapted from the original by [@mlabonne](https://huggingface.co/mlabonne).\n",
    "\n",
    "---\n",
    "\n",
    "**Outline:**\n",
    "1. Install dependencies\n",
    "2. Import libraries\n",
    "3. Log in to Weights & Biases (optional)\n",
    "4. Load dataset\n",
    "5. Load model and LoRA\n",
    "6. Define reward function\n",
    "7. Define training arguments\n",
    "8. Train with GRPO\n",
    "9. Interpret results\n",
    "10. Save and publish model\n",
    "11. Generate text\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af5fdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Install dependencies\n",
    "!pip install -qqq datasets==3.2.0 transformers==4.47.1 trl==0.14.0 peft==0.14.0 accelerate==1.2.1 bitsandbytes==0.45.2 wandb==0.19.7 --progress-bar off\n",
    "!pip install -qqq flash-attn --no-build-isolation --progress-bar off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e6d8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Import libraries\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from trl import GRPOConfig, GRPOTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbeaf9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Log in to Weights & Biases (optional)\n",
    "import wandb\n",
    "wandb.login()  # You can skip this cell if you don't want to log runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ebee4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Load dataset\n",
    "dataset = load_dataset(\"mlabonne/smoltldr\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20da1721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Load model and LoRA\n",
    "model_id = \"HuggingFaceTB/SmolLM-135M-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Load LoRA\n",
    "lora_config = LoraConfig(\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=\"all-linear\",\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "print(model.print_trainable_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cc6a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Define the reward function\n",
    "ideal_length = 50\n",
    "\n",
    "def reward_len(completions, **kwargs):\n",
    "    return [-abs(ideal_length - len(completion)) for completion in completions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794a63fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Define the training arguments\n",
    "training_args = GRPOConfig(\n",
    "    output_dir=\"GRPO\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=2,\n",
    "    max_prompt_length=512,\n",
    "    max_completion_length=96,\n",
    "    num_generations=8,\n",
    "    optim=\"adamw_8bit\",\n",
    "    num_train_epochs=1,\n",
    "    bf16=True,\n",
    "    report_to=[\"wandb\"],\n",
    "    remove_unused_columns=False,\n",
    "    logging_steps=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f6d96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Train with GRPO\n",
    "trainer = GRPOTrainer(\n",
    "    model=model,\n",
    "    reward_funcs=[reward_len],\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    ")\n",
    "\n",
    "import wandb\n",
    "wandb.init(project=\"GRPO\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3126db",
   "metadata": {},
   "source": [
    "# 9. Interpret training results\n",
    "\n",
    "`GRPOTrainer` logs the reward from your reward function, the loss, and a range of other metrics.\n",
    "\n",
    "- The reward should move closer to 0 as the model learns to generate text of the correct length.\n",
    "- The loss may increase as the model diverges from its initial policy, which is expected in GRPO.\n",
    "\n",
    "You can view the training logs in your Weights & Biases dashboard if you logged in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c5126e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Save and publish the model\n",
    "# (Optional) Push the model to the Hugging Face Hub\n",
    "# merged_model = trainer.model.merge_and_unload()\n",
    "# merged_model.push_to_hub(\n",
    "#     \"SmolGRPO-135M\", private=False, tags=[\"GRPO\", \"Reasoning-Course\"]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e62663f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Generate text with the fine-tuned model\n",
    "from transformers import pipeline\n",
    "\n",
    "prompt = \"\"\"\n",
    "# A long document about the Cat\n",
    "\n",
    "The cat (Felis catus), also referred to as the domestic cat or house cat, is a small \n",
    "domesticated carnivorous mammal. It is the only domesticated species of the family Felidae.\n",
    "Advances in archaeology and genetics have shown that the domestication of the cat occurred\n",
    "in the Near East around 7500 BC. It is commonly kept as a pet and farm cat, but also ranges\n",
    "freely as a feral cat avoiding human contact. It is valued by humans for companionship and\n",
    "its ability to kill vermin. Its retractable claws are adapted to killing small prey species\n",
    "such as mice and rats. It has a strong, flexible body, quick reflexes, and sharp teeth,\n",
    "and its night vision and sense of smell are well developed. It is a social species,\n",
    "but a solitary hunter and a crepuscular predator. Cat communication includes\n",
    "vocalizations—including meowing, purring, trilling, hissing, growling, and grunting—as\n",
    "well as body language. It can hear sounds too faint or too high in frequency for human ears,\n",
    "such as those made by small mammals. It secretes and perceives pheromones.\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "]\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=\"SmolGRPO-135M\")\n",
    "# Or use the model and tokenizer you defined earlier:\n",
    "# generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "generate_kwargs = {\n",
    "    \"max_new_tokens\": 256,\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 0.5,\n",
    "    \"min_p\": 0.1,\n",
    "}\n",
    "\n",
    "generated_text = generator(messages, generate_kwargs=generate_kwargs)\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
